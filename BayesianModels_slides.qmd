---  
format:  # ← Specifies output formats. You can define multiple formats like revealjs (slides), html, pdf, etc.
  revealjs:  # ← This block defines settings specifically for Reveal.js slide output.
    include-plotly: true
    include-in-header:
      - text: |
          <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    mermaid:
      enabled: true
    code-fold: true  # ← Adds toggle buttons to code blocks so viewers can hide/show the code.
    incremental: true  # ← Makes slide bullet points appear one-by-one, rather than all at once.
    slide-number: c/t  # ← Shows current slide number and total slide count (e.g., “3/10”) in the presentation.
    execute:
      enabled: true  # ← Ensures code cells in the document are actually executed during rendering.
    navigation-mode: vertical  # ← Enables 2D slide navigation: horizontal for sections, vertical for subsections.
    chalkboard:
      buttons: true  # ← Adds buttons to use a chalkboard or pointer tool while presenting.
    preview-links: auto  # ← Automatically previews external web links (e.g., YouTube, PDFs) inside slides.
    css: BayesianModels_styles.css  # ← Applies custom styles using a user-defined CSS file.
    footer:   |  # ← Adds a custom HTML footer across all slides. The pipe (|) means the following lines are literal block text.
        <a href="https://tinyurl.com/bayeBinderRepo" target="_blank">
        tinyurl.com/bayesBinderRepo </a>&nbsp;&nbsp;&nbsp;  
        <a href="https://tinyurl.com/bayesGithub" target="_blank">
        tinyurl.com/bayesGithub </a>&nbsp;&nbsp;</a>      
  html:  # ← Defines settings for basic HTML output (not slides). This is optional, but useful if rendering to HTML format too.
    execute:
      enabled: true  # ← Ensures that code cells are executed in HTML output as well.
jupyter: python3  # ← Tells Quarto to use the Python Jupyter kernel, not the default R kernel.
--- 
<!-- #region Title slide -->
--- 
 

<div style="display: flex; flex-direction: column; align-items: center; height: 100vh; padding-top: 60px; text-align: center;">
  <div style="font-size:1.6em; font-weight:bold; margin-bottom:10px; margin-top:10px;">
    Bayesian Models of Brains, Minds, & Behaviors 
  </div>

  <div style="font-size:1.1em; margin-bottom:15px;">
    DRCMR · Copenhagen · May 2025 
  </div>

  <div style="font-size:0.9em; max-width: 80%; margin: 0 auto;">
    Ollie Hulme · David Meder · Janine Bühler · Melissa Larsen
        Amin Kangavari · Simon Steinkamp · Naiara Demnitz
  </div>

  <img src="images/DRCMR_regionH_KU_logo.png" alt="DRCMR logo" style="height:80px; margin-top:15px;">
</div>

---
<!-- #endregion -->

<!-- #region Lecture#1 Preamble -->
## Lecture 1: Preamble — Roadmap  
- What this course is about 
- Materials: GitHub, Binder, Book  
- Schedule, social, and how the week works  
- Group project: model minds & brains  
- Our vibe & what we expect from you

## How the week will go
- A mix of lectures, exercises, and group work  
- You’ll build and present a research project with your group  
- Lectures lay the conceptual groundwork  
- Most hands-on work happens in Jupyter notebooks  
- We’re here to learn by doing & discussing

## Materials  
- GitHub: slides, code, schedule, everything...  
- Binder: run notebooks in browser, no install  
- Book: key reference, especially early in the week  
- All links in the GitHub `README.md`

## Schedule  
- Up to date on GitHub  
- Locations may change — check daily  
- Each day blends concept and coding  
- Friday = presentations 
- Be curious, flexible, and collaborative

## Social  
- Join the WhatsApp group (link on GitHub)  
- Ask questions, comment, share thoughts, coordinate  
- Friday = informal bar to celebrate

## Course overview  
- **Basic modelling** → Get started (Mon-Tues) 
- **Intermediate modelling** → Go deeper  (Wed)
- **Neural data integration** → Link models to brain (Thurs) 
- **Group project** → Model, analyze, present (Fri) 
- ↓  
- lectures · exercises · case studies · group work

## Course schematic  
- ![](images/course_schema.jpg){width=100%}  
- Your group project will follow this workflow

## Group project  
- Form a small group  
- Pick a cognitive question  
- Design an experiment (behavioral + neural)  
- Build models to test your hypotheses  
- Present your work on Friday (~15 min)

## Supervision & Support  
- Janine: talk to her if you are shy to ask in class  
- Ollie & David: logistics, organisation, schedule  
- Simon: anything technical, Binder, GitHub, Python  
- Group work: you will have many supervisors depending on the day

## Binder  
- Launch notebooks in your browser  
- No installation needed  
- Be patient — it takes time to load  
- Keep your Binder tab open all day
- It can time out so best to use continuously   
  <img src="/images/binder.png" width="300px">

## GitHub  
- Main hub for slides, code, notebooks, and schedule  
- Just browse or clone the repo, link in footer  
  <img src="/images/github.png" width="300px">

## Book  
- Core reference for models & theory
- This course roughly goes through it in order  
- Especially useful Mon–Wed  
  <img src="/images/book.jpeg" width="300px">

## WhatsApp  
- Ask questions, coordinate, share  
- Link is on GitHub `README.md`  
  <img src="/images/whatsapp.png" width="300px">

## Overarching aim ofcourse 
- Use probability theory to explain minds and behavior  
- Upgrade your scientific reasoning  
- Simplicity: intuitive, powerful tools  
- Universality: same ideas apply across science  
- From description → explanation

## Specific aims  
- Build and test cognitive models  
- Link cognitive models to neural data  
- Be hands-on, interactive, exploratory  
- Get messy — you’ll learn by doing  

## Our expectations of you  
- Ask questions — don’t nod and fake it  
- Disrupt — curiosity is good  
- You deserve to understand this  
- Doubt is natural, confusion is common  
- Let’s work it out together

## Things we love to hear  
- “I might have missed this, but…”  
- “Can I ask a stupid question?”  
- “Do you have an intuition for why…”  
- “I’m confused” 🤔  

## Things not to do to yourself  
- Don’t pretend to get it  
- Don’t assume you’re the only one confused  
- Don’t sit in silence out of self-doubt  
- Speak up — you’re helping the group

## Upcoming lectures  
- Basics of Bayesian thinking  
- Modeling cognition as a binary process  
- Modeling mixtures of processes  
- Selecting models  
- Bayes factors and posterior odds  


<!-- #endregion-->

<!-- #region Lecture#2 Basics of Bayesian analysis-->

# Lecture 2: Basics of Bayesian Analysis
- <br>
- **Roadmap**
- The spirit of Bayesian thinking
- What is Bayesian modeling?
- Principles of Bayesian inference
- Observable vs. latent variables
- Beliefs and evidence
- Estimation methods
- Why Bayesian methods?

## The spirit of Bayesian thinking 
- <br>
- *"Probability theory is nothing but common sense reduced to calculation."* — Laplace (1814)  
- <br>
- *"The rules of probability are the rules of consistent reasoning."* — Jaynes (2003)  
- <br>
- *"Bayesian methods are not a special brand of inference; they are the only logically consistent rules for inference that are known."* — Jaynes (2003)

::: {.footer}
Laplace (1814); Jaynes (2003)
:::

## Bayesianism as the calculus of common sense
- <br><br>
- *Bayesianism is just probability theory applied to inference.* — Jaynes (2003)

::: {.footer}
Jaynes (2003)
:::

## Probability as rational consistency 
- Bayesian modeling follows the rules of probability.
- Probability is logic.
- Logic is consistency.
- Consistency is rationality.
- And rationality is just thinking clearly.

## So what is "Bayesian Modeling of Minds, Brains, and Behavior"?
- This course is about **thinking clearly about minds, brains, and behavior**.
- It's about testing theories rationally, using the evidence provided by data.
- Bayesian modeling offers a principled, rational way to update beliefs based on evidence.
- Ta-da!

## Bayesian Updating in a Nutshell
- Prior belief → Evidence → Posterior belief

## Example of a cognitive task 
- e.g., go/no-go
- <img src="/images/gonogo.png" width="800px">

## Cognitive task example
- 10 binary trials of equal difficulty
- Estimate ability $\theta$ from behavior
- $\theta$ is *latent*; 
- data are *observed*
- e.g., correct responses $k = 8$ out of $n = 10$

## Latent vs. observed
- <img src="/images/latent_observable.png" width="1000px">

## Why latent variables?
- We want to explain, not just describe
- *Descriptive*: e.g. "What did the subject score?"
- *Explanatory*: e.g. "What ability caused that score?"

## Why Science Cares About the Latent
- Scientific questions are causal:
  - Do parkinsons patients differ in *risk taking* on and off medication? 
  - Does serotonin change *empathy*? 
  - Does alpha waves cause *memory consolidation*? 
  - Does ozempic improve *cognitive flexibility*?
- We observe data, but we infer about latent causes
- Bayesian modeling connects observables to latent processes

## Back to the cogntive task
- **Observed**: number correct ($k/n$)
- **Latent**: ability ($\theta$)
- Same $\theta$ can result in different $k/n$
- Different $\theta$ can result in the same $k/n$ 
- Bayesian inference accounts for this uncertainty

## Beliefs as Distributions
- Probability distributions encode **beliefs**
- The **center** = most likely value
- The **spread** = uncertainty
- *To play with this open the notebook on Binder*
- `notebooks/probability_distributions.ipynb`
- see "Beliefs as distributions"

## Probability mass functions for discrete variables
- <img src="/images/probability_mass.jpg" width="350px" style="display:inline"/> 
- Total mass sums to 1: $\sum_x p(x) = 1$  
- Range sums: $p(2 \leq x \leq 4) = 0.4$  
- Odds: $\frac{p(5)}{p(7)} = 7$

## Probaility density functions for continuous variables
- <img src="/images/probability_density.jpg" width="350px">
- all probability density integrates to 1
- area under the curve is 1
- densities can exceed 1
- ratios make sense: 5.5 is 5 times less likely than 0.7

## Probability Density Functions for continuous variables
- <img src="/images/probability_density.jpg" width="350px" style="display:inline"/>
- Total area under curve: $\int p(x)\,dx = 1$  
- Densities can > 1, but only area matters.  
- Likelihood ratios make sense: e.g., $p(5.5)/p(0.7) = 1/5$

## Interpreting probability distributions  
- It's important to read and reason with probabilty distributions.
- `notebooks/probability_distributions.ipynb` 
- see "Interpreting probability distributions"

## Bayes’ rule
- <br> 
- $p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}$
- <br>
- Posterior = $\frac{\color{red}{\text{Likelihood}}  \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}$
- <br>
- $\theta$ is a parameter, here "ability"
- $D$ is data, here it is the correct performance, $k$ successes out of $n$ trials
- <br>
- This tells us how our beliefs about ability are updated by the evidence provided by the data.

## Prior
- $p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}$
- <br>
- **$\color{blue}{\text{Prior}}$** is what we believe about $\theta$ before seeing the data. 
- It reflects our assumptions or prior knowledge.

## Likelihood
- $p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}$
- **$\color{red}{\text{Likelihood}}$** is the probability of data $D$ given a value of $\theta$.
- It shows how well each $\theta$ explains the data.
- Higher likelihood → stronger belief in $\theta$ 
- Lower likelihood → weaker belief.

## Marginal likelihood
- $p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}$
- <br>
- **$\color{green}{\text{Marginal likelihood}}$** is the total probability of the data under all possible values of $\theta$.
- It represents how good the model is at predicting the data. 
- In doing so it normalizes the result so the posterior is a valid probability distribution.
- Note that it is independent of $\theta$

## Posterior
- $p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}$
- <br>
- **Posterior** is what we believe about $\theta$ *after* seeing the data. 
- It’s the prior that has been updated by the evidence provided by the data.

## Updating beliefs with data
- We start with a **prior** belief $p(\theta)$
- Observe **data**: e.g. \( k = 9 \) correct out of \( n = 10 \)
- Bayes updates the belief:
- $p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}$

## Intuition behind belief updating  
- The more likely the data is for a given $\theta$  
- The more we believe in that value of $\theta$ after seeing the data.
- The values of $\theta$ that are better supported by the data, are more believed in after experiencing the data
- We have updated our beliefs according to the data

## Proportional form of Bayes rule
- Since:  
  $p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}$
- <br>
- The **$\color{green}{\text{Marginal likelihood}}$** $\color{green}{p(D)}$ doesn’t depend on $\theta$
- <br>
- So we can rewrite as:  
- $p(\theta \mid D) \propto \color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}$
- $\text{Posterior} \propto \color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}$
- "*Posterior is proportional to likelihood times prior*""

## Prior beliefs for theta
- `notebooks/probability distributions.ipynb`
- see "Prior beliefs for theta"

## Multiplying prior and likelihood
- `notebooks/probability distributions.ipynb`
-  see "Multiplying prior and likelihood" 

## Is the likelihood a probability distribution?
- `notebooks/probability distributions.ipynb`
-  see "Is the likelihood a probability distribution?"  

## Bayesian credibility intervals
- `notebooks/probability distributions.ipynb`
-  see "Bayesian credibility intervals"  

## Summarising the posterior
- `notebooks/probability distributions.ipynb`
-  see "Summarising the posterior" 

## Compute the posterior for the beta
- Prior: $p(\theta) \sim \text{Beta}(1,1)$
- Posterior: $p(\theta \mid D) \sim \text{Beta}(1 + k, 1 + n - k)$
- $k$ = correct, $n$ = total trials
- Simple, tractable update rule for binary outcomes
- `notebooks/probability distributions.ipynb`
- see "Computing posterior for the beta"

## Sequential updating of posterior
- Bayesian inference is consistent across steps
- One-step:
  - Prior → Combined data → Posterior
- Two-step:
  - Prior → Data1 → Intermediate Posterior → Data2 → Final Posterior
- Final result is identical
-  `notebooks/probability distributions.ipynb`
- see "Sequential updating"

## Why sequential updating of posterior matters
- You can peak at your data, it's ok! 
- Enables inference as data rolls in.
- *Optional stopping*: stop early, or extend data collection 
- *Efficient*: time, money, resources
- *Ethical*: minimises animals, humans, unnecessary treatment etc.

## Frequentist methods don’t allow this
- Frequentist inference assumes a fixed sample size and plan
- Stopping early or collecting more data invalidates p-values
- Counterfactual policies: what you would have done, if the data had turned out different impacts on your p-values. 
- Not widely understood. 

## Conjugate priors
- If Prior and posterior from the same distribution family → *conjugate*  
- <br>
- For example:
- $p(\theta): \text{Beta}(\alpha, \beta)$ 
- $p(\theta|data): \text{Beta}(\alpha + k, \beta + n - k)$
- <br>
- The posterior can be computed by plugging data directly into an equation
- Conjugacy allows for *analytic updates* 

## When conjugacy isnt possible: Sampling
- Conjugacy is relatively rare in real world cases
- In cases where conjugacy is not available sampling solutions are possible
- Commonly MCMC - Markov Chain Monte Carlo
- Works even when no closed-form solution
- Approximates the posterior via sampling

## Analytic vs. Sampling
- **Analytic**: Exact, when conjugate, rare
- **MCMC**: Approximate, more flexible, common

## MCMC in Practice
- **Red pill**: Learn how MCMC really works 🤓  
  - [Intuitive guide to MCMC internals](https://chat.openai.com/share/887b4e2d-2689-4e2d-b9ae-246b71d5782e)  
  - [Metropolis-Hastings explained simply](https://chat.openai.com/share/55cb6a00-1718-4327-8d51-3902a2064a11)  
  - These methods will keep evolving — expect newer algorithms, faster sampling, and better approximations.

- **Blue pill**: Trust the method and use it (but know how to spot when it's broken) 😄  
  - We'll demonstrate and visualise MCMC in practice.

## MCMC demo
- Go to "MCMC"
- `notebooks/probability distributions.ipynb`

## Why Bayesian Methods?
- **Principled reasoning**: Probabilistic logic = consistent thinking  
- **Uncertainty-aware**: Fully models uncertainty  
- **Latent variables**: Model hidden causes, not just outcomes  
- **Transparent updating**: Prior → Evidence → Posterior
- **Sequential updating**: Updating is the same whether data is sequential or all-in-one-go.  
- **Richer explanations**: From describing data to explaining via theory  
- **Flexible models**: Hierarchical, generative, extensible
- **Simple**: Same principle always. Learn it once. Apply it forever. 

<!-- #endregion-->

<!-- #region Lecture#3 Modelling a binary process-->

# Modeling a Binary Process
- Modeling binary outcomes in cognition  
- From observed data to hidden probabilities  
- Graphical models and their notation  
- Bayesian inference with JAGS  
- Convergence diagnostics and posterior checks  

## Modeling a Binary Process
- Start simple: focus on binary outcomes  
  e.g., *Success/Failure*, *Correct/Incorrect*, *Yes/No*
- Common examples: 
  *coin flips, true/false questions, detection tasks, motor responses*
- In our go/no-go example, each of the $n$ trials results in either $k$ successes  
- Binary processes are foundational for modeling cognition

## Getting Started
- Our goal is to infer ability in a go-no-go task
- We estimate a rate — the hidden probability $\theta$ that a response is correct
- We represent our uncertainty about $\theta$ as a probability distribution
- Many cognitive tasks can be modeled this way

## Binary Tasks in Cognitive Science
- Go/no-go, stop-signal, 2AFC, task switching  
- Recognition memory, Stroop, Flanker, oddball detection  
- Visual search, discrimination tasks, and so on. 

## From Binary Outcomes to a Hidden Rate
- Observe $k$ successes in $n$ trials → compute $\frac{k}{n}$
- But our interest is in the *underlying* success rate $\theta$
- Model: $k \sim \text{Binomial}(\theta, n)$
- $p(k \mid \theta, n) = \binom{n}{k} \theta^k (1 - \theta)^{n - k}$
- Assumes independent, identically distributed (i.i.d.) trials — no history effects

## Try it Yourself
- `notebooks/probability distributions.ipynb`
- see lecture 3 > "Binomial distribution"

## Graphical Models
- Graphical models represent probabilistic structure visually
- Nodes represent variables; edges represent dependencies
- Child nodes are conditionally dependent on parent nodes
- This is our JAGS model:
- ![](images/graphical%20model.png){width=30%}

## Graphical Notation

- **Circular** nodes: continuous variables  
- **Square** nodes: discrete variables  

- **Shaded** nodes: observed  
- **Unshaded** nodes: hidden  

- **Single border**: stochastic variable  
- **Double border**: deterministic relationship

## Graphical Notation Reference
- ![](images/graphical%20lexicon.jpeg){width=70%}

## Graphical Model Quiz
- ![](images/graphical%20model%20test.jpeg){width=30%}
- Upper node: What type of variable is this?  
- **Answer:** Continuous, stochastic, and observed
- Lower node: What type of variable is this?  
- **Answer:** Discrete, deterministic, and unobserved

## Sampling via JAGS
```{.python code-line-numbers="0|1|2|3|4|5"}
model {              # Define the model
  theta ~ dbeta(1,1) # Prior: theta follows a uniform beta distribution
  k ~ dbin(theta,n)  # Likelihood: k follows a binomial distribution
                     # with parameters theta and n
}                    # End of model
```
- ![](images/graphical%20model.png){width=30%}

## Interpreting the graphical model for the code
- ![](images/graphical%20model.png){width=30%}
- Theta is latent, and continuous
- n is observed and discrete
- k is observed and discrete
- Both feed n and k feed in to the likelihood to generate k

## R-hat as a convergence check
- It’s important to check that the sampling has converged to the stationary distribution.
- One heuristic is the R-hat statistic:  
  $\hat{R} = \frac{\text{var}(\text{within-chain})}{\text{var}(\text{across-chain})}$
- Rule of thumb: $\hat{R}$ should be between 1.00 and 1.01 for convergence.

## Inspecting the chains 
- the chains of samples should look like *hairy catapillars*
- like this:
- ![](images/hairy_caterpillar.png){width=60%}

## Try it yourself
 - go to "MCMC convergence checks**
 -  📂 `notebooks/probability distributions.ipynb`

## Difference between two rates
- Suppose we observe two processes, each producing successes out of trials:
- Process 1: $k_1$ successes out of $n_1$ trials  
- Process 2: $k_2$ successes out of $n_2$ trials
- We assume each is governed by a different underlying rate: $\theta_1$ and $\theta_2$.

## Estimating the difference
- We want to model each rate with a posterior Beta distribution, and we are interested in the difference:
- $\delta = \theta_1 - \theta_2$
- This tells us how much more (or less) likely success is in one group compared to the other.

## Examples and intuition
- Examples:
- 📈 Effect of a drug on performance ($\theta_1$ = treated, $\theta_2$ = control)
- 👶 Performance difference between age groups
- 🧪 Comparison of two algorithms on success rate
- A positive $\delta$ means group 1 is better; a negative $\delta$ means group 2 is better.

## Graphical model for inferring differences
- ![](images/fig3.3.jpg){width=100%}
- Why is delta double boundary?
- Because it is completely determined by the two thetas 

## JAGS code for infferring differences in rates
```{.python code-line-numbers="0|1|2|3|4|5|6"}
model{ 
  k1 ~ dbin(theta1,n1)
  k2 ~ dbin(theta2,n2)
  theta1 ~ dbeta(1,1)
  theta2 ~ dbeta(1,1)
  delta <-theta1-theta2
}                   # End of model
```
## Try it yourself
 - go to "Inferring the difference between two rates"  
 - `notebooks/probability distributions.ipynb`

## Interpret the posterior
- What is the approximate probability that the difference in rates  ($\delta$) is below 0?
- ![](images/posterior_delta.png){width=45%}

## Inferring a common rate 
- In some cases we want to infer a rate for 2 different processes
- e.g. *same subject & task, two different sessions*
- e.g. *same group, different subjects*
- e.g. *same subject, different tasks*
- Here we would model a single $\theta$
- ![](/images/common_rate_model.jpg) 

##  Same model with plate notation
- ![](/images/common_rate_model_plate.jpg)
- note only one theta, but multiple processes indexed by i 

## JAGS code for inferring a common rate 
```{.python code-line-numbers="0|1|2|3|4"}
 model{ 
  k1 ~ dbin(theta,n1)
  k2 ~ dbin(theta,n2)
  theta ~ dbeta(1,1)
}
```
- ![](/images/common_rate_model.jpg){width=100%} 
- Only one $\theta$ is modelling the two sets of data $k1$ and $k2$

## Try it out
 - go to "Inferring a common rate"  
 - `notebooks/probability distributions.ipynb`

## Predictions
- In Bayesian modeling, everything is about prediction.
- There are two fundamental axes:
  1. Are we predicting **parameters** or **data**?
  2. Are we predicting **before** or **after** observing data?

## Diiferent types of prediction
|                       | **Before data**                                         | **After data**                                                             |
|-----------------------|---------------------------------------------------------|----------------------------------------------------------------------------|
| **Parameters**        | Prior distribution: $p(\theta)$                         | Posterior distribution: $p(\theta \mid d_{\text{obs}})$                    | 
| **Data**              | Prior predictive distribution: $p(d_{\text{new}})$      | Posterior predictive distribution: $p(d_{\text{new}} \mid d_{\text{obs}})$ |

## Predicting parameters
- The **prior distribution** $p(\theta)$ is our prediction about the parameter before seeing data.
- The **posterior distribution** $p(\theta \mid d_\text{obs})$ is our updated prediction after observing data.

## Predicting data.
- The **prior predictive distribution** $p(d_{\text{new}})$ tells us what data we expect based on our prior belief about $\theta$.
- The **posterior predictive distribution** $p(d_{\text{new}} \mid d_{\text{obs}})$ predicts new data based on our updated belief.

## Todays posterior is tomorrows prior
- This measn that any posterior can always become a prior for a future prediction, and so on.  

## Prior and posterior prediction
```{.python code-line-numbers="1-3|5-6|8-10|12-13"}
model {
  # Prior distribution
  thetaprior ~ dbeta(1,1) 
  
  # Prior predictive distribtion
  priorpredk ~ dbin(thetapior,n) 
  
  # Posterior distribution 
  theta ~ dbeta(1,1) # theta becomes the posterior distribution of   
  k ~ dbin(theta,n) #likelihood for updating prior to posterior
  
  # Posterior predictive distribution
  postpredk ~ dbin(theta,n) # posterior predictive distibution
}
```
## Samples of the four distributions
- ![](/images/prior_post_predictions.jpg){width=60%}
- Prior and posterior distributions are in the space of the parameter
- Prior and posterior predictive distributions are in space of the data k out of n trials. 
 
## Comparing data to the posterior predictive distribution 
- - ![](/images/common_rate_model.jpg){width=100%} 
- If we estimate the model along with its predictive distributions
- We can see how this compares to the actual data

## Descriptive adequacy
- ...means how well does the model describe the data
-  Posterior for this data: k1 = 0, n1 =10 & k2=10, n2 = 10
- ![](images/posterior_common.jpg){width=50%}
- Looks ok, but does it have descriptive adequacy?

## Check posterior predictive distribution against data
- ![](images/posterior_predictive_common.jpg){width=40%}
- The model has poor *descriptive adequacy*. Why?
- Its a common rate model, so it predicts the same rate, but the data clearly is better modelled with different rates.  
 
## Prediction backwards and forward in time 
- Prediction normally about the future.
- Predictions can also apply to the past e.g. when information is missing.
- Data can help us infer hidden cognitive variables, which in turn may predict past behavior where information is incomplete.
- We predict what might have been known in the past if more information had been available. 
- Inference allows prediction both forward and backward in time.
 
## Prediction forward and backward in time
- Prediction is usually about the future — but it can also apply to the past.
- When data are missing, we use observed evidence to infer what *might have happened*.
- Cognitive models often predict hidden variables that explain past behavior.
- Inference lets us predict both what **might** happen and what **might have** happened.
<!-- #endregion -->

<!-- #region Lecture#4 Latent Mixture models      -->

# Latent mixture models
- What are they?
- How to use them to model mixtures of cognitive processes or traits
- How to use them to model compare models 

## Latent mixture models
- ...allow you to model data as coming from a *mixture* of *latent* processes. 
- This could be a mixture of cognitive processes, e.g. *guessing and trying, attending and not attending, remembering and forgetting.*
- Or mixture of states or traits, e.g. *depresssed vs. healthy, parkinsons vs. healthy, sleepy vs. awake*  
- An indicator variable estimates which mixture of processes generated the data.
- You can also use these mixture models to compare different models 

## Example: Latent mixture model of cognitive test
- *“Tryers”* have an ability that determines their rate of correct responses.
- *“Guessers”* score at chance level (e.g. 50%).
- Each participant belongs to one of the two groups.
- An *indicator* variable $z_i$ models *guesser* or *tryer*.

## Latent mixture model vs. simpler model
- ![](images/lmm.jpg){width=50%}
- ![](images/%20binomial%20rate%20model.jpg){width=50%}
- Its the same model, $z$ just allows a mix of two different processes that can set the rate $\theta$.
- Simples. 

## Latent mixture graphical model 
- ![](images/lmm.jpg){width=50%}
- $z_i$ is an indicator variable for participant $i$.
- $z_i = 0$ → guesser; $z_i = 1$ → tryer.
- $\psi = 0.5$ is the known chance performance level.
- $\phi$ is the tryer’s ability (same as $\theta$).
- Posterior of $z$ tells us the probability of each subject being either a guessers vs. a tryer.

## JAGS code for latent mixture
```{.python code-line-numbers="1-3|4-5|6-8"}
model {
  for (i in 1:p) {
    z[i] ~ dbern(0.5)
  }

  psi <- 0.5
  phi ~ dbeta(1, 1) I(0.5, 1)

  for (i in 1:p) {
    theta[i] <- equals(z[i], 0) * psi + equals(z[i], 1) * phi
    k[i] ~ dbin(theta[i], n)
  }
}
```
## Interactive demo
- `notebooks/probability distributions.ipynb`
-  Let's simulate data first
-  go to "Simulate guessers and tryers"  
- <br>
- Then we can run inference on each subject
-  go to "Infer guessers and tryers" 
 
<!-- #endregion                             -->

<!-- #region Lecture#5 Model selection            -->

# Model selection
- Why we need to compare models
- Simplicity vs. complexity
- Marginal likelihood as a comparison tool
- Predictive performance and prior bets
- Bayesian model comparison intuition

## Why compare models?
- Exept for the last section, we’ve mainly focused on single models
- Science advances by comparing competing explanations
- “This theory is good” → compared to what?
- We want to know which theory explains the data *better*
- This requires comparing models

## Simplicity vs. complexity
- “Explain phenomena by the simplest hypothesis that works” — Ptolemy
- “Avoid unnecessary plurality” — Occam’s razor
- “Complexity must pay for itself” — Hinton
- “Minimize free energy” — Friston
- These ideas all reflect the same principle: balance fit and complexity

## The Bayesian solution
- Many methods try to balance fit and complexity
- Bayesian methods do it naturally 
- Bayes gives us a single number for model quality: marginal likelihood
- It rewards accuracy...
- ...and penalizes wasted complexity

## Conditioning on a model
- Bayes' rule always assumes *a model*  
- But we can easily imagine different models, with different priors or structures
- Science is filled with competing explanations and models after all  
- We now ask: which model better predicts the data?  
- This leads us to the *marginal likelihood*  
- It’s the probability of the data, *given the model*

## Marginal likelihood
- $p(D \mid M)$ = average predictive performance of model $M$
- It’s a single number: 
- *How well the model predicted the data, on average*
- It accounts for *all* parameter values, weighted by their prior

## Analogy to betting
- Think of it like your model is betting on which parameter values best predict the data
- The better your bets, the higher your model’s score
- The prior is the placing of the bets, and the marginal likelihood is how good those bets paid off. 

## Marginal likelihood in words
- How probable was the data under this model $M$?
- Did the model concentrate its predictions where the data actually were?
- Priors spread out predictive mass
- Bad priors waste predictions on wrong areas
- Good priors focus predictions where the data land

## Octopus example
- There are two ~~octopi~~ octopusses.
- Both claim to be paranormal.
- It's 1970, so they are both working for the CIA
- ![](images/octopuses.jpg){width=50%}

## Octopus predictions 
- Where is the Russian sub?
- Alice: northern hemisphere
- Bob: Baltic Sea
- Data: Off the coast of Stockholm
- Alice was vaguely right 
- Bob was more precisely right → higher marginal likelihood

## How is marginal likelihood calculated?
- It’s the expected likelihood under the predictions of the prior:
- $p(D \mid M) = \int p(D \mid \theta, M)\, p(\theta \mid M)\, d\theta$
- For discrete parameters:
- $p(D \mid M) = \sum p(D \mid \xi_i)\, p(\xi_i)$
- It's a weighted average across all parameter settings

## Try it out  
- Lets compare alice and bob
-  `notebooks/probability distributions.ipynb`
- "Compare octopusses via marginal likelihood"

## Worked example
- Suppose a model has three parameter values: $\theta = \{0, 0.5, 1\}$
- Prior probabilities: $p(\theta_1) = \color{blue}{0.6}$, $p(\theta_2) = \color{blue}{0.3}$, $p(\theta_3) = \color{blue}{0.1}$
- Likelihoods: $p(D \mid \theta_1) = \color{red}{0.1}$, $p(D \mid \theta_2) = \color{red}{0.4}$, $p(D \mid \theta_3) = \color{red}{0.6}$
- Marginal likelihood:  
  $p(D) = \color{blue}{0.6} \cdot \color{red}{0.1} + \color{blue}{0.3} \cdot \color{red}{0.4} + \color{blue}{0.1} \cdot \color{red}{0.6}$
- Step by step:  
  $= \color{gray}{0.06} + \color{gray}{0.12} + \color{gray}{0.06} = \boxed{0.24}$

## Complexity and spread
- More complex models spread their predictions widely
- This lowers the average likelihood
- Even if they include the truth, they may assign low probability to it
- Marginal likelihood punishes this
- Broad priors = wasted predictions = lower score = lower marginal likelihood

## Complexity is not just parameter count
- A model with *many* narrow priors can be simple
- A model with *one* vague prior can be complex
- Complexity = how broadly a model spreads its predictions
- Narrow predictive distributions = simpler models
- Wide, uncertain predictions = complex models

## Misconception - sidenote
- <span style="color:gray">AIC, BIC penalize complexity by counting parameters</span>  
- <span style="color:gray">But parameter count ≠ true complexity</span>  
- <span style="color:gray">Complexity = how broadly a model spreads its predictions</span>
- <span style="color:gray">Bayesian marginal likelihood captures this automatically</span>

## Example: prior vagueness
- $\theta \sim \text{Uniform}(0,1)$ → vague → complex
- $\theta \sim \text{Uniform}(0.5,1)$ → tighter → simpler
- Both models have one parameter
- But they differ in how much of the prediction space they cover
- Complexity depends on how much ground a model tries to cover

## Example: prior vagueness 
- ![](images/complexity.png){width=100%}

## Why marginal likelihood matters
- It’s the most important quantity in Bayesian model comparison
- It unifies inference in brain, behavior, science
- Maximizing it means best average predictive performance
- It is the quantity that arguably everything else is trying to approximate: variational Bayes, free energy minimisation, ELBO, predictive coding, predictive processing

## Why marginal likelihood matters
- <span style="color:gray">Can even argue it is a *unique universal maximandum* for all physical and adaptive systems</span>
- <span style="color:gray">Woah man, that's like, deep.</span>
- 😵‍💫

<!-- #endregion                             -->

<!-- #region Lecture#6 Bayes factors              -->

# The Bayes factor
- Compared to what?  
- From marginal likelihood to relative evidence  
- Interpreting Bayes factors  
- Worked example: guessing vs. non-guessing  
- Pitfalls and philosophical notes

## Why compare models?
- A model with high marginal likelihood is good — but only *relative* to alternatives  
- Absolute goodness is rarely meaningful on its own  
- The key question is: compared to what?  
- We want relative evidence — which model explains the data better  
- The Bayes factor gives us that answer

## Compared to what?
- ![](images/oompa.jpg){width=100%}

## Set a compared-to-what alarm in your brain
- Listen out for one-sided superiority / inferiority claims
- *"this theory explains..."* 
- *"this model predicts the data poorly..."*
- *"this data is unlikely under the null hypothesis..."*
- **Ding!** → Compared to what?

## Bayes factor definition
- Marginal likelihood: *average predictive performance of a model*  
- Bayes factor compares this between two models  
- Defined as the ratio of marginal likelihoods:  
  $BF_{\color{blue}{1} \color{red}{2}} = \frac{p(D \mid \color{blue}{M_1})}{p(D \mid \color{red}{M_2})}$  
- Quantifies how much more likely the data is under $\color{blue}{M_1}$ than $\color{red}{M_2}$

## We already plotted Bayes factors
- Sneakly i didnt tell you they were bayes factors. 
- ![](images/ratio_marginal_likelihoods.png)

## Interpreting the Bayes factor
- $BF_{12} > 1$ → data favors $M_1$  
- $BF_{12} < 1$ → data favors $M_2$  
- $BF_{12} = 5$ → data is 5× more likely under $M_1$  
- $BF_{12} = \frac{1}{5}$ → data is 5× more likely under $M_2$  
- Strength of evidence depends on how far from 1 the ratio is

## Jeffreys' scale
| $BF_{12}$       | Interpretation                 |
|----------------|-------------------------------|
| >100           | Extreme evidence for $M_1$     |
| 30–100         | Very strong evidence for $M_1$ |
| 10–30          | Strong evidence for $M_1$      |
| 3–10           | Moderate evidence for $M_1$    |
| 1–3            | Anecdotal evidence for $M_1$   |
| 1              | No preference                  |
| 1/3–1          | Anecdotal evidence for $M_2$   |
| 1/10–1/3       | Moderate evidence for $M_2$    |
| 1/30–1/10      | Strong evidence for $M_2$      |
| 1/100–1/30     | Very strong evidence for $M_2$ |
| <1/100         | Extreme evidence for $M_2$     |

## Try it out
- see "Bayes factor scale interpretation"
- `notebooks/probability distributions.ipynb`

## Example: guessing vs. non-guessing
- 9 out of 10 trials correct → $k = 9$, $n = 10$  
- $M_1$: unknown ability → $\theta \sim \text{Uniform}(0,1)$  
- $M_2$: guessing → $\theta = 0.5$  
- Compute marginal likelihood under each model  
- Compare with a Bayes factor

## Bayes factor calculation
- For $M_1$: $p(D \mid M_1) = \frac{1}{1+n} = \frac{1}{11} \approx 0.0909$  
- For $M_2$: $p(D \mid M_2) = \binom{10}{9} (0.5)^{10} = 10 \cdot 0.000976 = 0.0098$  
- Bayes factor:  
  $BF_{12} = \frac{0.0909}{0.0098} \approx 9.3$  
- Data is about 9× more likely under $M_1$ than $M_2$

## Try it out
- see "Bayes factor calculation step by step"
- `notebooks/probability distributions.ipynb`

## Flipping the BF
- If $BF_{12} < 1$...
- ...take the reciprocal: $BF_{21} = \frac{1}{BF_{12}}$  
- Keeps interpretation intuitive: how many times more likely is the data?  
- Example: $BF_{12} = 0.2$ → $BF_{21} = 5$  
- Now we say: data is 5 × more likely under $M_2$  
- Same info, more digestible

## Bayes vs. Fisher
- Bayes compares models  
- Fisherian methods tests a single null hypothesis  
- p-values ask: “how unlikely is this data under $H_0$?”  
- Bayes factors ask: “which model better explains the data?”  
- Evidence is always comparative: $p(\text{data} \mid \text{A})$ vs. $p(\text{data} \mid \text{B})$

## Critiques

- **Criticism:** *Bayes factors are sensitive to prior choice*  
- **Answer:** This is a **feature** — priors are part of the model.

- **Criticism:** *But I don’t want my conclusions to depend so much on the prior*  
- **Answer:** Then use **sensitivity analysis** to check robustness.

- **Criticism:** *BFs be high if one bad model is much worse than another*  
- **Answer:** True. Check **descriptive adequacy** — look at **posterior predictive distributions**, simulate data, or compare out-of-sample predictions.

## The arc of civilisation  
- Oppposable thumbs, fire, the wheel, writing, zero, the printing press, Newtonian physics, germ theory, the steam engine, the combustion engine, the Moon landing, In Rainbows, the internet, CRISPR, Bayes factors

- <sub><span style="color:gray"><em>I’m being satirical (kinda).</em></span></sub>

<!-- #endregion                             -->

<!-- #region Lecture#7 Posterior odds -->

# Posterior odds
- From Bayes factors to posterior model plausibility  
- Combining likelihood and prior beliefs  
- Posterior odds: what we believe after seeing the data  
- Why priors matter  
- Common critiques and computational issues

## From Bayes factor to posterior odds
- Bayes factors compare predictive performance  
- But model plausibility also depends on prior belief  
- Posterior odds = updated belief in models after data  
- Combines evidence (likelihood) and belief (prior)  
- This gives a more complete model comparison

## Posterior odds equation
- Relative plausibility:  
  $\frac{p(M_1 \mid D)}{p(M_2 \mid D)} = \frac{p(D \mid M_1)}{p(D \mid M_2)} \cdot \frac{p(M_1)}{p(M_2)}$  
- In words:  
  posterior odds = Bayes factor × prior odds  
- Clear separation of belief and data  
- Makes assumptions explicit  
- Encourages transparency in modeling

## Posterior odds example
- ![](images/posterior_odds_rain.png)

## Posterior odds schematic
- ![](images/posterior_odds_eq.png)

## Try it out
- This demo ties everything together!
- `notebooks/probability distributions.ipynb`
- Go to "posterior odds calculation"

## Transforming prior into posterior
- Bayes factor converts prior odds into posterior odds  
- If prior odds = 1, posterior odds = Bayes factor  
- Prior beliefs influence conclusions  
- But can be updated rationally with data  
- That’s the power of Bayes

## Why Bayes factors are powerful
- Automatically penalize overly complex models  
- Allow direct evidence for the null  
- Can handle multiple models, not just null vs. alt  
- Support sequential data collection  
- Intuitive: how much more likely is the data?

## Extraordinary claims, extraordinary evidence
- Strong priors require stronger data  
- $p(M \mid D) = BF \cdot p(M)$  
- Even big Bayes factors may not sway implausible models  
- Prior odds encode skepticism  
- Bayesian methods make this explicit

## Why p-values can’t do this
- p-values don't compare models  
- Don’t provide evidence for the null  
- Depend on hypothetical outcomes  
- Require fixed plans to be valid  
- Can’t say “this model is better”

## Optional stopping
- Bayes allows data collection to stop anytime  
- p-values require pre-specified $n$  
- Bayes factors stay valid during interim checks  
- Enables adaptive design  
- More flexible, more realistic

## Conceptual and computational challenges
- Priors matter: vague priors can be a liability because they can make a model bad 
- Marginal likelihood is sometimes hard to compute  
- Complex models need careful approximation  
- Broad priors lower predictive accuracy  
- Precision in priors improves inference

## Specifying priors
- Priors must reflect real uncertainty  
- Two strategies:  
  - Subjective (expert-driven)  
  - Objective (e.g., unit-information priors)  
- Objective priors = default baseline  
- Useful for reproducibility  
- Can be refined with context

## Common confusion about priors
- Model priors are priors over models, parameter priors are priors over parameters. 
- Bayes factor does **not** depend on model priors  
- It **does** depend on parameter priors because these are part of models 
- Posterior model probability depends on both  
- So:  
  - Priors on models → posterior odds  
  - Priors on parameters → Bayes factor

## Prior sensitivity
- Different priors = different conclusions  
- That’s not a bug — it’s honest uncertainty  
- Test robustness with sensitivity analysis  
- Try narrower and wider priors  
- Some models are robust, others fragile

## Computational solutions
- Exact marginal likelihood often unavailable  
- Approximate methods include:  
  - Free energy minimisation
  - Variational Bayes
  - MCMC methods *like we have used*



<!-- #endregion -->

<!-- #region Misc lectures -->

<!-- #region Lecture#9 Savage Dickey -->
# Savage-Dickkey method of model comparison
- In this method two models are compared:
  - **Null hypothesis ($H_0$)**: fixes parameter to a specific value, e.g., $\phi = \phi_0$
  - **Alternative hypothesis ($H_1$)**: parameter free to vary, e.g., $\phi \ne \phi_0$
- $H_0$ is nested within $H_1$ (by constraining parameter).
- Classical null hypothesis usually sharp (point-null).
 
## Savage–Dickey Density Ratio 
- Defines Bayes factor for nested models:
$BF_{01} = \frac{p(D \mid H_0)}{p(D \mid H_1)} = \frac{p(\phi = \phi_0 \mid D, H_1)}{p(\phi = \phi_0 \mid H_1)}$
- Simply the ratio of posterior to prior densities at the point of interest $\phi_0$ under the alternative hypothesis.
 
## Example: Binomial Scenario
- Binomial scenario: $\theta$ parameter, observing 9 correct and 1 incorrect response.
- Null hypothesis ($H_0$): $\theta = 0.5$
- Alternative hypothesis ($H_1$): $\theta$ free to vary, prior $\theta \sim Beta(1,1)$
- Bayes factor is the ratio of posterior and prior densities at $\theta=0.5$

## Visual Interpretation of Savage-Dickey
- ![](Fig7.1-example.jpg)
- Prior (uniform) and posterior distributions shown.
- Density ratio at $\theta=0.5$ gives Bayes factor.
 
## MCMC-Based Estimation for Savage-Dickey
- When analytical solutions are difficult, use MCMC:
-![](Fig7.2-MCMC-example.jpg)
- Posterior and prior estimated from MCMC samples.
- Heights of posterior and prior at the null point give Bayes factor.

## Advantages of Savage–Dickey
- Direct interpretation as density ratio.
- Simplifies computation —no separate marginal likelihood calculation needed.
- Works well for nested models.
<!-- #endregion    -->

<!-- #region Lecture#10 Compare guassian means    -->
# Commpare Gaussian means
- Common task: test if two Gaussian means differ
- Example: does glucose improve detection performance
- Focus: test claim that glucose boost has larger effect in summer

## Data
| Season | N  | Mean | SD  |
|--------|----|------|-----|
| Winter | 41 | 0.11 | 0.15|
| Summer | 41 | 0.07 | 0.23|
- Difference not significant  
- t = 0.79, p = 0.44

## p-values and the Null Hypothesis
- “From a null result, we cannot conclude that no difference exists…”
- p = 0.44 does not support H₀
- It just means data are not incompatible with H₀
- Need a Bayes factor to quantify support for H₀

## Bayes Factor Overview
- Bayes factor compares posterior vs prior odds
- Quantifies evidence for or against H₀
- Unlike p-values, can support H₀
 
## One-Sample Comparison Model
- Test standardized difference scores (e.g., winter - summer)
- Assume:
  - δ ~ Cauchy(0,1)
  - xᵢ ~ Gaussian(μ, 1/σ²)
  - μ = δσ

## One-Sample Graphical Model
- ![Fig 8.1](figures/graphical_model_onesample.png)
- Prior on δ: Cauchy(0,1)
- Prior on σ: Half-Cauchy
- Estimate posterior with MCMC
 
## Posterior vs Prior
- ![Figure 8.2](figures/posterior_prior_onesample.png)
- Posterior peaks near δ = 0
- Bayes Factor ≈ 5:1 in favor of H₀

## Order-Restricted Model
- SMM predicts **δ < 0**
- Use order-restricted prior:
  - δ ~ Cauchy(0,1) truncated to (-∞, 0)

## Updated Bayes Factor 
- ![Figure 8.4](figures/posterior_prior_orderrestricted.png)
- Stronger evidence for H₀: BF ≈ 10:1

## Summary 
- p-values can't confirm H₀
- Bayes factors can
- "Evidence of absence" of support for SMM's prediction
 
## Two-Sample Comparison
- Compare oxygenated vs plain water on memory
- Two independent groups

## Two-Sample Model Structure
- ![Figure 8.5](figures/graphical_model_twosample.png)
- Shared variance σ²
- δ = α / σ  
- α = μₓ - μᵧ
 
## Large Effect Example
| Group         | N  | Mean   | SD   |
|---------------|----|--------|------|
| Plain Water   | 20 | 68.35  | 6.38 |
| Oxygenated    | 20 | 76.65  | 4.06 |
| t(38) = 4.47, p < .01 |
 
## Two-Sample Bayes Factor
-![Figure 8.6](figures/posterior_prior_twosample.png)
- Posterior moves away from 0
- BF ≈ 447:1 in favor of H₁  
- Decisive evidence for oxygenated water effect 
<!-- #endregion   -->

<!-- #region Lecture#11 Compare binomial rates    -->
# Comparing binomial rates
- We will naturally compute binomial rates for different groups or conditions
- And ask which is larger? 
- We thus need to compare binomial rates and test hypotheses about which is bigger etc.

## Bayesian graphical model
- ![Figure 9.1](figures/Fig9.1.png)
- graphical model for comparing two proportions

## Bayesian model
- We model the observed counts using binomial likelihoods and assign uniform Beta priors:
    s1 ~ Binomial(theta1, n1)  
    s2 ~ Binomial(theta2, n2)  
    theta1 ~ Beta(1, 1)  
    theta2 ~ Beta(1, 1)  
    delta <- theta1 - theta2
- theta1: 
- theta2: 
- delta = theta1 - theta2: difference in proportions
- We are interested in the posterior distribution of delta.
 
## Model Code 
Here is the model used for posterior simulation:
    model {
      theta1 ~ dbeta(1,1)
      theta2 ~ dbeta(1,1)
      delta <- theta1 - theta2
      s1 ~ dbin(theta1, n1)
      s2 ~ dbin(theta2, n2)
      theta1prior ~ dbeta(1,1)
      theta2prior ~ dbeta(1,1)
      deltaprior <- theta1prior - theta2prior
    }
This allows us to compare the prior and posterior density of delta at zero.
 
## Prior and Posterior Distributions
- ![Figure 9.2](figures/Fig9.2.png)
- We estimate the posterior distribution for the rate difference delta = theta1 - theta2 using Bayesian inference.
- The left plot shows prior and posterior distributions for delta across its full range.
- The right plot zooms in near delta = 0.
- This is used in the Savage–Dickey density ratio to compute the Bayes factor.
- The Savage–Dickey method compares:
    BF_01 = prior density at delta = 0 / posterior density at delta = 0
 
## Interpreting the Bayes Factor
- The posterior density at delta = 0 is about half the prior density.
- This gives a Bayes factor ≈ 2 in favor of the alternative hypothesis H1: delta ≠ 0.
- The 95% credible interval for delta is approximately [-0.09, 0.01], which does not include 0.

**Interpretation**:
- There is only modest evidence that one rate is higher than another 
- The Bayes factor penalizes H1 for spreading prior mass over implausible values. 
<!-- #endregion    -->

<!-- #region Lecture#4 Inference with Gaussians   -->
# Inferences with Gaussians  
- Due to central limit theorem, data and parameters are frequently Gaussian
- Gaussians have 2 parameters, a mean and a measure of their spread
- Spread can be expressed as a variance, std, or a precision (1/var) 
 
## Graphical model for Gaussians 
- Simple model for inferring Gaussian with unknown mean and std
- ![](fig.4.1.jpg)
 
## Interactive demo of Gaussian
- Jupyter notebook - interactive plotting of gaussian
 
## Sampling model for inferring Gaussians  
model { 
for (i in 1:n){
x[i]~dnorm(mu,lambda)
}
mu~dnorm(0,0.001)
sigma~dunif(0,10)
lambda<-1/pow(sigma,2)
} 
 
## Repeated measures of IQ 
- Imagine taking a cognitive test like IQ multiple times
- The mean is your IQ, and the spread models fluctuations in your performance. e.g. attention, fatigue, emotion, venus orbitting satturn
- We can model this as a Gaussian for each person

## Graphical model for IQ
- ![](fig.4.3.jpg)
- What parameter is common to all subjects?
- No index on the std. This means it is fixed.
- Is this justified?
- How to change it?
 
## Sampling code for IQ 
model{
  for (i in 1:n) {
    for (j in 1:m) {
      x[i,j]~dnorm(mu[i],lambda)
    }
  } 
sigma~dunif(0,100)
lambda <-1/pow(sigma,2)
for (i in 1:n) {
  mu([i]~ dunif(0,300))
}

}
<!-- #endregion                            -->

<!-- #endregion -->
