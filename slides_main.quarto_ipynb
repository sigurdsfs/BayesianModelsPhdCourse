{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "title: \"Bayesian models of brains, minds, & behaviors\"\n",
        "title_acronym: \"bayesMod\" # set up variables for re-use \n",
        "subtitle: \"Stop describing, start explaining 😊\"\n",
        "format:\n",
        "  revealjs: \n",
        "    code-fold: true\n",
        "    incremental: true\n",
        "    toc: true  # Enables a sidebar with slide titles\n",
        "    toc-depth: 1 \n",
        "    toc-title: \"Lecture topics\"  # Custom title for TOC\n",
        "    slide-number: c/t\n",
        "    execute: \n",
        "      enabled: true  # Ensures code execution\n",
        "    navigation-mode: vertical  # Optional: Default allows 2D navigation\n",
        "    chalkboard: \n",
        "      buttons: true\n",
        "    preview-links: auto\n",
        "    logo: images/drcmr.jpeg\n",
        "    css: styles.css\n",
        "    #footer: #[insert binder link]\n",
        "jupyter: python3  # Ensure Quarto uses Jupyter instead of R\n",
        "resources:\n",
        "  - bayesModels.pdf\n",
        "---\n",
        "\n",
        "\n",
        "# Preamble\n",
        "::: incremental\n",
        "- Course materials\n",
        "- Course schedule\n",
        "- Social 😊\n",
        ":::\n",
        "\n",
        "## Course materials\n",
        "::: incremental\n",
        "- All slides & interactive exercises run on binder → [binder link]\n",
        "- ☝️ Lets you run code without needing to install anything. Simples. 😌 \n",
        "- Also downloadable on github [github.com/ollie-hulme/BayesianModels](https://github.com/ollie-hulme/BayesianModels)\n",
        ":::\n",
        "\n",
        "## Course schedule\n",
        "::: incremental\n",
        "- *insert schedule*\n",
        ":::\n",
        "\n",
        "## WhatsApp social group\n",
        "::: incremental\n",
        "- Here you can talk to each other\n",
        "- Answer each others questions\n",
        "- Stay in touch ❤️\n",
        "- Plan friday bar 😄\n",
        ":::\n",
        "\n",
        "# Introduction\n",
        "::: incremental\n",
        "- Aims \n",
        "- Expectations\n",
        "- Bayesian preliminaries\n",
        ":::\n",
        "\n",
        "## Overarching aim\n",
        "::: incremental\n",
        "-  To use the tools of probability theory to scientifically understand brains minds annd behavior\n",
        ":::\n",
        "\n",
        "## Specific aims\n",
        "::: incremental\n",
        "- Hands-on / interactive\n",
        "- Keep it simple, duh\n",
        "- Beyond description → explain via theory \n",
        "- Emphasis on the models of mind & behavior\n",
        "- Simple methods for connecting these to neural data\n",
        "- Intuitive, playful, enlightening, soul-nourishing, life-affirming, fun!, useful! ✨\n",
        ":::\n",
        "\n",
        "## Our expectations of you\n",
        "::: incremental\n",
        "- Life is too to short to let things pass that you dont understand. \n",
        "- Your life matters. Your understanding matters. Actively take part.\n",
        "- You are entitled to understand this. \n",
        "- Don't listen, wrestle.\n",
        "- Ask questions, interupt.\n",
        "- Be sceptical. You cant offend me. 🤨\n",
        "::: \n",
        "\n",
        "## Do say:\n",
        "::: incremental\n",
        "- \"I'm probably being stupid...\" 🤪\n",
        "- \"I might have missed this...\"\n",
        "- \"i dont understand why...\"\n",
        "- \"do you have an intuition for why...\"\n",
        "- \"i'm confused...\" 🤔\n",
        "- If shy, say it in the break, find a TA, write a note\n",
        ":::\n",
        "\n",
        "## Don't:\n",
        "::: incremental\n",
        "- Nod and pretend you get something. 🙅‍♂️ \n",
        "- Assume you are the only one confused. 😵‍💫😵‍💫😵‍💫\n",
        ":::\n",
        "\n",
        "## What is a Bayesian model?\n",
        "::: incremental\n",
        "- Bayesian models → probabilistic models\n",
        "- Probabilistic models are essential because uncertainty is fundamental to all real-world phenomena\n",
        "- Brains / minds / bodies need probabilistic models to deal with this uncertainty\n",
        "- As scientists, data about brains / minds / bodies is also uncertain \n",
        "- → we need probabilistic models for the same reason \n",
        ":::\n",
        "\n",
        "## What can Bayesian models model?\n",
        "::: incremental\n",
        "- Anything. Literally. 🤯\n",
        "- Here we apply it to brains/minds/behavior, with emphasis on experimental data. \n",
        "- Particular emphasis on: \n",
        "  - cognitive models which can be inferred from behavior \n",
        "  - integration into analyses of neuroimaging data\n",
        "- The heavy lifting is all on the cogntive modelling, but principles apply to any phenomena of interest.\n",
        "- These modelling principles apply to anything. I promise.    \n",
        ":::\n",
        "\n",
        "## The esesence of Bayesianism?\n",
        "::: incremental\n",
        "- Bayesianism just reduces to bein consistent with your beliefs... \n",
        "- ...and how you update them according to evidence.\n",
        "- Being consistent with probabilitistic beliefs is just logic.\n",
        "- So really, deep down, this course is just about being logical.\n",
        "- It's not a theory that could be empirically falsified, its not Newtonian physics, or evolution.\n",
        "- Its mathematically true.  \n",
        ":::\n",
        "\n",
        "## Spoiler alert: Bayes wasn't that much of a genius\n",
        "::: incremental\n",
        "- Bayes wasn't a Newton, Darwin, or Einstein.\n",
        "- He just rearranged an equation and came up with an interesting application. \n",
        "- Its not his domain of theory, he just was the first to notice this and suggest an application. \n",
        ":::\n",
        "\n",
        "# How to think like a Bayesian\n",
        "'Google slides: <a href=\"https://docs.google.com/presentation/d/1wEUf06-69HDm93K-SIimnBpMw4ken5g6j6rDQ5xTsoU/edit?usp=sharing\n",
        "\">[link]</a>'\n",
        "\n",
        "# The basics of Bayesian analysis\n",
        "::: incremental\n",
        "- Principles\n",
        "- Hidden vs. observable\n",
        "- Beliefs and evidence  \n",
        "- Estimation methods\n",
        "- Why Bayes?\n",
        ":::\n",
        "\n",
        "## General principles\n",
        "::: incremental\n",
        "-  current belief → evidence → new belief\n",
        "-   prior belief  → evidence → posterior belief\n",
        "::: \n",
        "\n",
        "## Example\n",
        "::: incremental\n",
        "- a cognitive task of 10 trials of equal difficulty\n",
        "- we want to estimate cognitive ability $\\theta$ which is the rate of correct performance\n",
        "- $\\theta$ is hidden (latent), we cannot observe it directly, only its effect on the number of trials correct\n",
        "- before we obtain data we have uncertainty over $\\theta$\n",
        "- we do know $0 < \\theta <1$ where 0 is no performance, and 1 is perfect performance\n",
        "::: \n",
        "\n",
        "## Hidden variables\n",
        "  ::: incremental\n",
        "- modeling hidden variables is central to cognitive modelling \n",
        "- it is at the heart of explanation beyond description\n",
        "- description would just plot performance score as this is observed.\n",
        "- but if you want to understand what underlies it, you need to model hidden variables.\n",
        "- psychology / neuroscience is always ultimately about hidden variables\n",
        ":::\n",
        "\n",
        "## Why the difference between hidden and observed\n",
        "::: incremental\n",
        "- number of trials correct $k$ is an observable consequence of ability which is hidden\n",
        "- they are not the same\n",
        "- the same ability can give different $\\k$ depending on chance\n",
        "- often our questions and theories are about the hidden not the observed. \n",
        ":::\n",
        "\n",
        "## Prior belief over $\\theta$\n",
        "::: incremental\n",
        "- if we are agnostic about $\\theta$ we can assign uniform probability distribution for $p(\\theta)$\n",
        "- *uniform dist*\n",
        "- we know nothing so we assign equal probability to all possible values of $\\theta$\n",
        "- we are maximally uncertain\n",
        ":::\n",
        "\n",
        "## Representing beliefs as probabiity distributions\n",
        "::: incremental\n",
        "- Probability distributions are a natural way to represent beliefs\n",
        "- They express our what we believe (.e.g is theta high or low) \n",
        "- ...and our uncertainy (how spread out is the distribution)\n",
        "- *interactive beta dist*\n",
        "- play with sliders to represent different prior beliefs\n",
        ":::\n",
        "\n",
        "## Update prior with evidence from data\n",
        "::: incremental\n",
        "- Data: k = 9/10 trials correct\n",
        "- $p(\\theta) \\rightarrow p(k|\\theta) \\rightarrow p(\\theta|data)$  \n",
        "- prior $\\rightarrow$ likelihood $\\rightarrow$ posterior \n",
        ":::\n",
        "\n",
        "## Posterior distribution\n",
        "::: incremental\n",
        "- prior was *uniform dist*\n",
        "- posterior $p(\\theta|data)$\n",
        "- is *plot beta(9,1)*\n",
        "- we are now more certain than before\n",
        "- the evidence has reduced our uncertainty\n",
        ":::\n",
        "\n",
        "\n",
        "## Bayes rule\n",
        "::: incremental\n",
        "- Bayes rule expresses how to update priors into posterior according to data\n",
        "- $p(\\theta | \\text{data}) = \\frac{p(\\text{data} | \\theta) \\, p(\\theta)}{p(\\text{data})}$\n",
        "- $\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{marginal likelihood}}$\n",
        ":::\n",
        "\n",
        "## Marginal likelihood\n",
        "::: incremental\n",
        "- $p(D)$ this is the probability of the data. \n",
        "- Its a single number that ensures the posterior sums to 1 and is therefore a probability distribution\n",
        "- It doesnt depend on $\\theta$\n",
        ":::\n",
        "\n",
        "## Alternative expression of Bayes\n",
        "::: incremental\n",
        "- $p(\\theta | \\text{data}) \\propto p(\\text{data} | \\theta) \\times p(\\theta)$\n",
        "- $\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}$\n",
        ":::\n",
        "\n",
        "## Bayesian credibility intervals express uncertainy\n",
        "::: incremental\n",
        "- The posterior can be interpreted as a probability distribution\n",
        "- if 50% of distribution is above say 0.55, then there is $p(0.5)>\\theta$ \n",
        "- Bayesian credible intervals e.g. $\\text{BCI}_{95\\%} [0.4,0.8]$ meaans that 95% probability the true value is between those limits. \n",
        "- This is what we want classical confidence intervals to be. \n",
        "- *insert drawing of how to make BCI*\n",
        ":::\n",
        "\n",
        "## Sequential updating\n",
        "::: incremental\n",
        "- suppose we collect more data\n",
        "- 5 more trials and 3 of these are correct\n",
        "- we previously had 9 out of 10 correct\n",
        "- we just add the data together so now its 12 / 15\n",
        "- unlike frequentist statistics the order of data collection doesnt matter\n",
        "- *plot new posterior*\n",
        ":::\n",
        "\n",
        "## Sequential updating\n",
        "::: incremental\n",
        "- it makes no difference if\n",
        "- compute posterior for first data, then update again\n",
        "- prior → 9/10 → posterior → prior → 3/ 5 → posterior\n",
        "- or just compute all in one go\n",
        "- prior → 12/15 → posterior\n",
        "- same final posterior. simples.\n",
        ":::\n",
        "\n",
        "## Analytic solution\n",
        "::: incremental\n",
        "- For certain cases you can calculate posteriors \"analytically\" via equations\n",
        "- e.g. in this example we represent our uniform prior as a beta distribution\n",
        "- $p(\\theta)~beta(1,1)$\n",
        "- to update to the posterior is simple\n",
        "- $p(\\theta|k,n)~beta(1+k,1+(n-k))$\n",
        "- thus for every possible observation we know how to update our posterior\n",
        "::: \n",
        "\n",
        "## Interactive beta distribution\n",
        "::: incremental\n",
        "- *plot beta with sliders for k and n*\n",
        "- play around with the data to see how it updates the uniform prior to any posterior\n",
        "- what values of k and n, make the posterior most uncertain?\n",
        "- which values make posterior most certain it theta is low? high? intermediate?\n",
        "::: \n",
        "\n",
        "## Conjugate\n",
        "::: incremental\n",
        "- in nice cases like this, it is only possible because the prior and posterior belong to the same family of distributions\n",
        "- they are \"conjugate\"\n",
        "- sadly, in most cases this is not possible 😞 \n",
        ":::\n",
        "\n",
        "## So what do we do in other cases?\n",
        "::: incremental\n",
        "- dont fear MCMC is here!\n",
        "- what?\n",
        "- Monte Carlo Markov Chain.  \n",
        "- Its a method for sampling from the posterior and it works in almost all cases.\n",
        "- It's a little bit magic. \n",
        "- This course focuses on MCMC because it works in all cases and its simple to use. \n",
        ":::\n",
        "\n",
        "## Comparing analytic vs. numerical (sampling)\n",
        "::: incremental\n",
        "- *plot analytic posterior*\n",
        "- *plot MCMC*\n",
        "- *screencap from book?*\n",
        ":::\n",
        "\n",
        "## How does MCMC work?\n",
        "::: incremental\n",
        "- There are two paths forward. \n",
        "- Red pill - you like to know the truth because truth is power. Therefore you can follow this video *insert link* on how it works\n",
        "- Blue pill - you take for granted that it works and not knowing exactly how has no material impact on your modelling. \n",
        ":::\n",
        "\n",
        "## Advantages of Bayesian modelling\n",
        "::: incremental\n",
        "- Flexibility - Bayesian models can be built to accomodate the complexity or simplicity of any data. They can deal with contamination, confounds, hierarchical data, missing data.\n",
        "- Principled - Uncertainty is always fully represented, and accounted for, no useful information is discarded. \n",
        "- Intuitive - Generally Bayes will align with your intuition, much better than other approaches. Sometimes you will need to update your intuition, but thats ok. \n",
        "- Easy - It is easy, once you know how. \n",
        ":::\n",
        "\n",
        "# Modelling a binary process \n",
        "::: incremental\n",
        "- Start simple → binary processes\n",
        "- Most tasks are binary processes 😲\n",
        ":::\n",
        "\n",
        "## Getting started\n",
        "::: incremental\n",
        "- Our first example was about inferring a rate for a binary process\n",
        "- Binary process since either correct or incorrect\n",
        "- Inferring a rate, since we were inferring the underlying ability as a probability $\\theta$\n",
        "- Lots of tasks can be represented as binary processes\n",
        ":::\n",
        "\n",
        "## Cognitive tasks that are binary processes\n",
        "::: incremental\n",
        "- Task switching\n",
        "- Go-no-go\n",
        "- Stop signal\n",
        "- etc. \n",
        ":::\n",
        "\n",
        "## From binary outcomes to rate\n",
        "::: incremental\n",
        "- Binary outcomes typically k correct out of n\n",
        "- Expressed as a rate = k / n\n",
        "- This is the observed rate\n",
        "- But what is the underlying hidden rate $/theta$\n",
        "- $k \\sim \\text{Binomial}(\\theta, n)$\n",
        ":::"
      ],
      "id": "b19f160a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}