{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93f74307",
   "metadata": {},
   "source": [
    "\n",
    "# Bayesian modeling of minds, brains and behavior\n",
    "---\n",
    "**Overview.** This notebook allows you to run Python code in your browser. We can mix text and code in a single document. This iis intended to support the lectures allowing me to demo certain concepts in a hands on way. It is also designed for you to work through it slowly after lectures, reading the text and doing the exercises by playing with the plots or even altering or writing code if you want to. We have an emphasis on play. Mess around. \"I wonder what happens if I change this?\". Let's start with a simple example of a Gaussian distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## Lecture 2\n",
    "\n",
    "**Beliefs as distributions.** Here you can adjust the sliders to change the mean (μ) and standard deviation (σ) of this Gaussian distribution. The mean moves the distribution so that its mean is higher or lower. The standard deviation changes the spread, changing the uncertainty of the belief\n",
    "\n",
    "**Exercise.** Click on code cell below and press play to run. Play around with the distributions and think about how they would represent belief. Use the sliders to generate distributions that represent different prior beliefs for theta:\n",
    "\n",
    "  - Certain belief that the theta is high\n",
    "\n",
    "  - Certain belief that the theta is low \n",
    "  \n",
    "  - Uncertain belief that the theta is high   \n",
    "  \n",
    "  - Uncertain belief that the theta is low*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dabc1f",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6174bbeb82b46e683025eb2e28d2be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='μ', max=10.0, min=-10.0), FloatSlider(value=1.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_gaussian(mu=0.0, sigma=1.0):\n",
    "    # Fixed x range\n",
    "    x = np.linspace(-20, 20, 1000)\n",
    "    y = norm.pdf(x, mu, sigma)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(x, y, label=f'N({mu:.2f}, {sigma:.2f}²)', color='purple')\n",
    "    ax.fill_between(x, y, color='plum', alpha=0.4)\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.set_xlim(-20, 20)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    plot_gaussian,\n",
    "    mu=widgets.FloatSlider(min=-10, max=10, step=0.1, value=0.0, description=\"μ\"),\n",
    "    sigma=widgets.FloatSlider(min=0.1, max=5.0, step=0.1, value=1.0, description=\"σ\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca423f",
   "metadata": {},
   "source": [
    "**Interpreting probability distributions.** There are intuitive ways to read off probabilities from probability distributions like these. According to the belief encoded by the distribution, the probability of theta being larger than say 0.5 or 5 is the area under the curve above this value. Similarly, the probability of theta being between say 0.1 and 0.7 is the area under the curve between these two values\n",
    "\n",
    "**Exercise.** Click on the code cell below and press play to run. \n",
    "\n",
    "- Play with the sliders to see how the area under the curve changes as you change the minimum and maximum values. \n",
    "\n",
    "- The shaded area represents the probability of theta being in the range you selected.\n",
    "\n",
    "- What happens when you make the distribution more narrow, or more wide? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e04c3ddb",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f70c3076d8d48afa9e7ecff8ebbbe9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='μ', max=10.0, min=-10.0), FloatSlider(value=1.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_gaussian(mu=0.0, sigma=1.0, x_min=-1.0, x_max=1.0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_gaussian(mu=0.0, sigma=1.0, x_min=-1.0, x_max=1.0):\n",
    "    # Fixed x range\n",
    "    x = np.linspace(-20, 20, 1000)\n",
    "    y = norm.pdf(x, mu, sigma)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(x, y, label=f'N({mu:.2f}, {sigma:.2f}²)', color='purple')\n",
    "\n",
    "    # Fill full curve lightly\n",
    "    ax.fill_between(x, y, color='plum', alpha=0.2)\n",
    "\n",
    "    # Highlight area between x_min and x_max\n",
    "    mask = (x >= x_min) & (x <= x_max)\n",
    "    ax.fill_between(x[mask], y[mask], color='mediumvioletred', alpha=0.6,\n",
    "                    label=f\"P({x_min:.2f} < θ < {x_max:.2f}) ≈ {norm.cdf(x_max, mu, sigma) - norm.cdf(x_min, mu, sigma):.2f}\")\n",
    "\n",
    "    ax.set_title(\"Probability as Area Under the Curve\")\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.set_xlim(-20, 20)\n",
    "    ax.set_ylim(0, max(y) * 1.1)\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    plot_gaussian,\n",
    "    mu=widgets.FloatSlider(min=-10, max=10, step=0.1, value=0.0, description=\"μ\"),\n",
    "    sigma=widgets.FloatSlider(min=0.1, max=5.0, step=0.1, value=1.0, description=\"σ\"),\n",
    "    x_min=widgets.FloatSlider(min=-10, max=10, step=0.1, value=-1.0, description=\"x min\"),\n",
    "    x_max=widgets.FloatSlider(min=-10, max=10, step=0.1, value=1.0, description=\"x max\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b178de",
   "metadata": {},
   "source": [
    "**Prior beliefs for theta.** We need to think carefully about what theta means. Theta represents cognitive ability in our go-no-go task. 0 is the worst possible cognitive ability, and 1 is the best. So we should set a prior distribution for theta between 0 and 1. The distributions above do not do this, so let's fix this. We can use a beta distribution to represent our prior belief about theta. The beta distribution is a continuous probability distribution defined on the interval [0, 1], so it is perfect for our needs.\n",
    "\n",
    "\n",
    "**Exercise.** Play around with beta distribution parameters to see how the shape of the distribution changes:\n",
    "\n",
    "  - You are complete uncertain about the ability → Set the widest prior\n",
    "\n",
    "  - You are quite certain that ability will be high  → Set a narrow prior centered on a high value\n",
    "\n",
    "  - You are quite certain that ability will be low  → Set a narrow prior centered on a low value\n",
    "\n",
    "  - You are uncertain but you think ability is low  → Set a wide prior centered on a low value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a19e2dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9275d3b9e2864c0db9d20a435d1a04de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=10, description='n', min=1), IntSlider(value=5, description='k')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb195bf56754642893547ec02fbe914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Sliders\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=100, step=1, description=\"n\")\n",
    "k_slider = widgets.IntSlider(value=5, min=0, max=100, step=1, description=\"k\")\n",
    "\n",
    "# Output area\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "# Plot function\n",
    "def plot_beta(k, n):\n",
    "    x = np.linspace(0, 1, 500)\n",
    "    a = k + 1\n",
    "    b = (n - k) + 1\n",
    "    y = beta.pdf(x, a, b)\n",
    "\n",
    "    with plot_output:\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.plot(x, y, label=f\"Beta({a}, {b})\", color='blue')\n",
    "        ax.fill_between(x, y, color='skyblue', alpha=0.3)\n",
    "        ax.set_title(\"Prior beliefs over θ\")\n",
    "        ax.set_xlabel(\"θ\")\n",
    "        ax.set_ylabel(\"Probability Density\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Unified update function\n",
    "def on_slider_change(change=None):\n",
    "    # Avoid recursive triggers by updating value only if needed\n",
    "    if k_slider.value > n_slider.value:\n",
    "        k_slider.unobserve(on_slider_change, names='value')\n",
    "        k_slider.value = n_slider.value\n",
    "        k_slider.observe(on_slider_change, names='value')\n",
    "    plot_beta(k_slider.value, n_slider.value)\n",
    "\n",
    "# Set up observers (after defining callback)\n",
    "n_slider.observe(on_slider_change, names='value')\n",
    "k_slider.observe(on_slider_change, names='value')\n",
    "\n",
    "# Layout and display\n",
    "display(widgets.VBox([n_slider, k_slider]), plot_output)\n",
    "\n",
    "# Initial plot\n",
    "plot_beta(k_slider.value, n_slider.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd3b71-1526-4dc8-ac1b-b197119e0448",
   "metadata": {},
   "source": [
    "**Multiplying prior and likelihood.**  We’ve learned that the posterior is proportional to the prior times the likelihood:\n",
    "\n",
    "$$\n",
    "\\text{Posterior} \\propto \\text{Prior} \\times \\text{Likelihood}\n",
    "$$\n",
    "\n",
    "The likelihood tells us how well each value of $\\theta$ predicted the data. If a value of $\\theta$ predicts the data well → the posterior belief increases. If it predicts the data poorly → the posterior belief decreases. So the posterior reshapes your belief, using the likelihood as a reweighting function over your prior.\n",
    "\n",
    "**Exercise.** Now you can explore this directly: Use the sliders to define a prior distribution and a likelihood. The resulting product is shown as the posterior. By default, this is not normalized (it doesnt inegrate to 1) — you are just seeing the raw shape. This shape still carries meaning: it shows which values of $\\theta$ are most consistent with both your prior beliefs and the observed data.\n",
    "- Set a prior and likelihood.\n",
    "- Pick a few values of theta, and multiply the prior by the likelihood.\n",
    "- See if the height of the posterior curve matches your expectation.\n",
    "\n",
    "**Normalising the Posterior.** You can press the button to normalize the posterior.\n",
    "To turn the posterior into a true probability distribution, we need to divide by the marginal likelihood — the constant that ensures the posterior integrates to 1 and is therefore a proper probability distribution.\n",
    "\n",
    "$$\n",
    "p(\\theta \\mid \\text{data}) = \\frac{p(\\theta) \\cdot p(\\text{data} \\mid \\theta)}{p(\\text{data})}\n",
    "$$\n",
    "\n",
    "**Exercise.** Click on the button to normalize the posterior. \n",
    "\n",
    "- Notice how the area under the curve is now 1, and the posterior is a proper probability distribution.\n",
    "\n",
    "- The area under the prior and the posterior should be the same. \n",
    "\n",
    "- Do you need the same amount of paint to colour each in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e54d12ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9153f1b70cde49b68e120f82ab4ca9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<b style='color:blue'>Prior sliders</b>\"), HBox(children=(IntSlider(value=2, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fe039471ff4e7b90ce5fca08dfc2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# --- Plotting function ---\n",
    "def plot_bayes(k_prior, n_prior, k_like, n_like, show_norm_post):\n",
    "    # Prior\n",
    "    a_prior = k_prior + 1\n",
    "    b_prior = (n_prior - k_prior) + 1\n",
    "    y_prior = beta.pdf(x, a_prior, b_prior)\n",
    "\n",
    "    # Likelihood (visual only)\n",
    "    a_like = k_like + 1\n",
    "    b_like = (n_like - k_like) + 1\n",
    "    y_like = beta.pdf(x, a_like, b_like)\n",
    "\n",
    "    # Unnormalized Posterior\n",
    "    y_post = y_prior * y_like\n",
    "\n",
    "    # Normalize if requested\n",
    "    if show_norm_post:\n",
    "        norm_const = np.trapz(y_post, x)\n",
    "        y_post = y_post / norm_const if norm_const > 0 else np.zeros_like(x)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(x, y_prior, label=\"Prior\", color=\"blue\")\n",
    "    ax.plot(x, y_like, label=\"Likelihood\", color=\"red\")\n",
    "    ax.plot(x, y_post, label=\"Posterior\", color=\"green\")\n",
    "\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 10)  # Or another sensible fixed upper limit\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# --- Sliders ---\n",
    "k_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=1, description=\"k_prior\")\n",
    "n_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=2, description=\"n_prior\")\n",
    "\n",
    "k_like_slider = widgets.IntSlider(min=0, max=100, step=1, value=1, description=\"k_like\")\n",
    "n_like_slider = widgets.IntSlider(min=0, max=100, step=1, value=2, description=\"n_like\")\n",
    "\n",
    "# --- Checkbox to toggle normalization\n",
    "show_norm_post = widgets.Checkbox(value=False, description=\"Normalize Posterior\", indent=False)\n",
    "\n",
    "# --- Ensure k ≤ n ---\n",
    "def update_k_max(slider_k, slider_n):\n",
    "    slider_k.max = slider_n.value\n",
    "    if slider_k.value > slider_k.max:\n",
    "        slider_k.value = slider_k.max\n",
    "\n",
    "def update_k_max_prior(*args):\n",
    "    update_k_max(k_prior_slider, n_prior_slider)\n",
    "\n",
    "def update_k_max_like(*args):\n",
    "    update_k_max(k_like_slider, n_like_slider)\n",
    "\n",
    "n_prior_slider.observe(update_k_max_prior, names='value')\n",
    "n_like_slider.observe(update_k_max_like, names='value')\n",
    "update_k_max_prior()\n",
    "update_k_max_like()\n",
    "\n",
    "# --- UI layout ---\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<b style='color:blue'>Prior sliders</b>\"),\n",
    "    widgets.HBox([n_prior_slider, k_prior_slider]),\n",
    "    widgets.HTML(\"<b style='color:red'>Likelihood sliders</b>\"),\n",
    "    widgets.HBox([n_like_slider, k_like_slider]),\n",
    "    show_norm_post\n",
    "])\n",
    "\n",
    "# --- Plot binding\n",
    "out = widgets.interactive_output(plot_bayes, {\n",
    "    'k_prior': k_prior_slider,\n",
    "    'n_prior': n_prior_slider,\n",
    "    'k_like': k_like_slider,\n",
    "    'n_like': n_like_slider,\n",
    "    'show_norm_post': show_norm_post\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac8b3c",
   "metadata": {},
   "source": [
    "**Is the Likelihood a proper probability distribution?** Does the likelihood integrate to 1? Not the way we plot it. The likelihood is $p(\\mathrm{data} \\mid \\theta)$: a probability distribution over the data, given a fixed value of $\\theta$. It is not a probability distribution over $\\theta$. So when we plot $\\theta \\mapsto p(\\mathrm{data} \\mid \\theta)$, we are not plotting a probability density function — we're plotting a likelihood function, and it does not need to integrate to 1 over $\\theta$. However, for each fixed $\\theta$, the function is a proper probability distribution over data, and it satisfies:\n",
    "\n",
    "$$\n",
    "\\int p(\\mathrm{data} \\mid \\theta) \\, d\\,\\mathrm{data} = 1\n",
    "$$\n",
    "\n",
    "So yes — the likelihood does integrate to 1, but only over the data, not over $\\theta$.\n",
    "\n",
    "**Exercise.** Wiggle the sliders to get different likelihoods:\n",
    "\n",
    "- Left, when you plot the likelihood, over different values of theta, you can see that the likelihood does not integrate to 1.\n",
    "\n",
    "- Right, when you plot the likelihood over different values of data, you can see that the likelihood does integrate to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8238e17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7eade293014117a48e3708fb514355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Left: Likelihood as a function of θ'), HTML(value='Right: Likelihood as a function …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c43ad188ee4e67b085cfd55dfeb40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Plotting function ---\n",
    "def plot_likelihood_views(n, k, theta_fixed):\n",
    "    theta_vals = np.linspace(0, 1, 500)\n",
    "    likelihood_theta = binom.pmf(k, n, theta_vals)\n",
    "    area_theta = np.trapz(likelihood_theta, theta_vals)\n",
    "\n",
    "    k_vals = np.arange(0, n + 1)\n",
    "    likelihood_data = binom.pmf(k_vals, n, theta_fixed)\n",
    "    area_data = np.sum(likelihood_data)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Left: likelihood as function of θ\n",
    "    axes[0].plot(theta_vals, likelihood_theta, color='black', lw=2)\n",
    "    axes[0].set_title(rf\"$\\theta \\mapsto p(k={k} \\mid \\theta, n={n})$\")\n",
    "    axes[0].set_xlabel(\"θ\")\n",
    "    axes[0].set_ylabel(r\"$p(k \\mid \\theta, n)$\")\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.5)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].text(0.95, 0.95, f\"∫ dθ ≈ {area_theta:.3f}\", transform=axes[0].transAxes,\n",
    "                 ha='right', va='top', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'))\n",
    "\n",
    "    # Right: likelihood as function of data (k)\n",
    "    axes[1].bar(k_vals, likelihood_data, color='black', alpha=0.8)\n",
    "    axes[1].set_title(rf\"$k \\mapsto p(k \\mid \\theta={theta_fixed:.2f}, n={n})$\")\n",
    "    axes[1].set_xlabel(\"k (number of successes)\")\n",
    "    axes[1].set_ylabel(\"Probability\")\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.5)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].text(0.95, 0.95, f\"∑ = {area_data:.3f}\", transform=axes[1].transAxes,\n",
    "                 ha='right', va='top', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Sliders ---\n",
    "n_slider = widgets.IntSlider(min=1, max=50, value=10, step=1, description=\"n (trials)\")\n",
    "k_slider = widgets.IntSlider(min=0, max=50, value=5, step=1, description=\"k (successes)\")\n",
    "theta_slider = widgets.FloatSlider(min=0.01, max=0.99, value=0.5, step=0.01, description=\"θ (fixed)\")\n",
    "\n",
    "# --- Enforce k ≤ n ---\n",
    "def update_k_max(*args):\n",
    "    k_slider.max = n_slider.value\n",
    "    if k_slider.value > k_slider.max:\n",
    "        k_slider.value = k_slider.max\n",
    "\n",
    "n_slider.observe(update_k_max, names='value')\n",
    "update_k_max()\n",
    "\n",
    "# --- Layout ---\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"Left: Likelihood as a function of θ\"),\n",
    "    widgets.HTML(\"Right: Likelihood as a function of data\"),\n",
    "    widgets.HBox([n_slider, k_slider, theta_slider])\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(plot_likelihood_views, {\n",
    "    'n': n_slider,\n",
    "    'k': k_slider,\n",
    "    'theta_fixed': theta_slider\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9267205e",
   "metadata": {},
   "source": [
    "**Bayesian credibility intervals.** A Bayesian credibility interval is a range of values that contains the true value of the parameter with a certain probability. \n",
    "It is similar to a confidence interval in frequentist statistics, but it is based on the posterior distribution of the parameter rather than the sampling distribution. Its interpretation is actually what most people think of when they hear the term \"confidence interval\". Its what they are looking for when they ask for a confidence interval. You can construct a Bayesian credibility interval by picking the probability you want to contain the true value of the parameter, and then finding the range of values that contains that probability. \n",
    "\n",
    "**Exercise.** Click on the code cell below and press play to run. \n",
    "\n",
    "- Play with the sliders to see how the credibility interval changes as you change the probability:\n",
    "\n",
    "- The most typical is the 95% credibility interval $BCI_{95}$, which contains the value of the parameter with 95% probability. \n",
    "\n",
    "- The 50% credibility interval $BCI_{50}$ would contains the value of the parameter with 50% probability. And so on.\n",
    "\n",
    "- How is this different from the frequentist confidence interval? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fe9531a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa61df88b044c42ae024e00b530cad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='μ', max=5.0, min=-5.0), FloatSlider(value=1.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_gaussian_bci(mu=0.0, sigma=1.0, bci=95.0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_gaussian_bci(mu=0.0, sigma=1.0, bci=95.0):\n",
    "    # Fixed x and y ranges\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = norm.pdf(x, mu, sigma)\n",
    "\n",
    "    # Compute credible interval bounds\n",
    "    alpha = 1 - bci / 100\n",
    "    lower_bound = norm.ppf(alpha / 2, mu, sigma)\n",
    "    upper_bound = norm.ppf(1 - alpha / 2, mu, sigma)\n",
    "    y_bound = norm.pdf([lower_bound, upper_bound], mu, sigma).min()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot curve and shading\n",
    "    ax.plot(x, y, label=f'N({mu:.2f}, {sigma:.2f}²)', color='purple')\n",
    "    ax.fill_between(x, y, color='plum', alpha=0.2)\n",
    "\n",
    "    # Highlight credible interval\n",
    "    mask = (x >= lower_bound) & (x <= upper_bound)\n",
    "    ax.fill_between(x[mask], y[mask], color='mediumvioletred', alpha=0.6,\n",
    "                    label=f\"{bci:.0f}% BCI: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "\n",
    "    # Dashed lines at interval bounds\n",
    "    ax.hlines(y=y_bound, xmin=lower_bound, xmax=upper_bound,\n",
    "              color='black', linestyle='--', linewidth=1)\n",
    "    ax.vlines([lower_bound, upper_bound], ymin=0, ymax=y_bound,\n",
    "              color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Fixed axes\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(0, 0.5)  # Fixed y-range to accommodate all practical normal densities\n",
    "\n",
    "    # Labels and legend\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "# Interactive sliders\n",
    "interact(\n",
    "    plot_gaussian_bci,\n",
    "    mu=widgets.FloatSlider(min=-5, max=5, step=0.1, value=0.0, description=\"μ\"),\n",
    "    sigma=widgets.FloatSlider(min=0.1, max=3.0, step=0.1, value=1.0, description=\"σ\"),\n",
    "    bci=widgets.FloatSlider(min=50, max=99, step=1, value=95, description=\"BCI (%)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d902012-4c10-4f8b-9ead-4fe6147bc4fe",
   "metadata": {},
   "source": [
    "**Summarising the posterior.** With a proper normalised posterior, we may want to summarise it with credible intervals. These are the Bayesian equivalent of confidence intervals. But better, obvs. \n",
    "\n",
    "**Exercise.** Play with the slider that finds the Xth percentile credible interval.\n",
    "\n",
    "- What happens to the interval when you drop the percentile lower?\n",
    "\n",
    "- Plot the MAP - Maximum a posteriori. What is it? \n",
    "\n",
    "- How would you define it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870dc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612d081f4599401cb781c0372374c581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<b style='color:blue'>Prior sliders</b>\"), HBox(children=(IntSlider(value=2, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8441052141984d359152f736363630fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "def plot_bayes(k_prior, n_prior, k_like, n_like, ci_width, show_shading, show_mean, show_map, show_curves):\n",
    "    # Prior parameters\n",
    "    a_prior = k_prior + 1\n",
    "    b_prior = (n_prior - k_prior) + 1\n",
    "    y_prior = beta.pdf(x, a_prior, b_prior)\n",
    "\n",
    "    # Likelihood as Beta PDF (for visualization only)\n",
    "    a_like = k_like + 1\n",
    "    b_like = (n_like - k_like) + 1\n",
    "    y_like = beta.pdf(x, a_like, b_like)\n",
    "\n",
    "    # Posterior parameters (from conjugate Beta-Binomial update)\n",
    "    a_post = a_prior + k_like\n",
    "    b_post = b_prior + n_like - k_like\n",
    "    y_post = beta.pdf(x, a_post, b_post)\n",
    "\n",
    "    # Compute proper marginal likelihood\n",
    "    log_ml = betaln(k_like + a_prior, n_like - k_like + b_prior) - betaln(a_prior, b_prior)\n",
    "    marginal_likelihood = np.exp(log_ml)\n",
    "\n",
    "    # Compute Bayesian credible interval (BCI)\n",
    "    lower = beta.ppf((1 - ci_width / 100) / 2, a_post, b_post)\n",
    "    upper = beta.ppf(1 - (1 - ci_width / 100) / 2, a_post, b_post)\n",
    "\n",
    "    # Posterior Mean and MAP\n",
    "    posterior_mean = a_post / (a_post + b_post)\n",
    "    map_index = np.argmax(y_post)\n",
    "    posterior_map = x[map_index]\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    if show_curves:\n",
    "        ax.plot(x, y_prior, label=\"Prior\", color=\"blue\")\n",
    "        ax.plot(x, y_like, label=\"Likelihood (visualized)\", color=\"red\")\n",
    "\n",
    "    ml_text = f\"Posterior (marginal likelihood ≈ {marginal_likelihood:.3f})\"\n",
    "    ax.plot(x, y_post, label=ml_text, color=\"green\")\n",
    "\n",
    "    if show_shading:\n",
    "        ax.fill_between(x, y_post, where=(x >= lower) & (x <= upper), color='gray', alpha=0.3, label=f\"{ci_width}% BCI\")\n",
    "    else:\n",
    "        ax.axvline(lower, color='gray', linestyle='--', label=f\"{ci_width}% BCI\")\n",
    "        ax.axvline(upper, color='gray', linestyle='--')\n",
    "\n",
    "    if show_mean:\n",
    "        ax.axvline(posterior_mean, color='black', linestyle=':', label=\"Posterior Mean\")\n",
    "\n",
    "    if show_map:\n",
    "        ax.axvline(posterior_map, color='purple', linestyle='-.', label=\"Posterior MAP\")\n",
    "\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Sliders\n",
    "k_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=1, description=\"k_prior\", style={'description_width': 'initial'})\n",
    "n_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=2, description=\"n_prior\", style={'description_width': 'initial'})\n",
    "\n",
    "k_like_slider = widgets.IntSlider(min=0, max=100, step=1, value=1, description=\"k_like\", style={'description_width': 'initial'})\n",
    "n_like_slider = widgets.IntSlider(min=0, max=100, step=1, value=2, description=\"n_like\", style={'description_width': 'initial'})\n",
    "\n",
    "ci_slider = widgets.IntSlider(min=50, max=99, step=1, value=95, description=\"BCI (%)\", style={'description_width': 'initial'})\n",
    "\n",
    "# Toggles\n",
    "shading_toggle = widgets.Checkbox(value=False, description='Shade BCI')\n",
    "mean_toggle = widgets.Checkbox(value=False, description='Show Posterior Mean')\n",
    "map_toggle = widgets.Checkbox(value=False, description='Show MAP')\n",
    "curves_toggle = widgets.Checkbox(value=False, description='Show Prior & Likelihood')\n",
    "\n",
    "# Reusable fix for k ≤ n\n",
    "def enforce_k_leq_n(k_slider, n_slider):\n",
    "    k_slider.max = n_slider.value\n",
    "    if k_slider.value > k_slider.max:\n",
    "        k_slider.value = k_slider.max\n",
    "\n",
    "# Observers\n",
    "def update_k_max_prior(*args):\n",
    "    enforce_k_leq_n(k_prior_slider, n_prior_slider)\n",
    "\n",
    "def update_k_max_like(*args):\n",
    "    enforce_k_leq_n(k_like_slider, n_like_slider)\n",
    "\n",
    "n_prior_slider.observe(update_k_max_prior, names='value')\n",
    "n_like_slider.observe(update_k_max_like, names='value')\n",
    "\n",
    "# Initial sync\n",
    "update_k_max_prior()\n",
    "update_k_max_like()\n",
    "\n",
    "# Layout\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<b style='color:blue'>Prior sliders</b>\"),\n",
    "    widgets.HBox([n_prior_slider, k_prior_slider]),\n",
    "    widgets.HTML(\"<b style='color:red'>Likelihood sliders</b>\"),\n",
    "    widgets.HBox([n_like_slider, k_like_slider]),\n",
    "    widgets.HTML(\"<b>Posterior Display Options</b>\"),\n",
    "    ci_slider,\n",
    "    shading_toggle,\n",
    "    mean_toggle,\n",
    "    map_toggle,\n",
    "    curves_toggle\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(plot_bayes, {\n",
    "    'k_prior': k_prior_slider,\n",
    "    'n_prior': n_prior_slider,\n",
    "    'k_like': k_like_slider,\n",
    "    'n_like': n_like_slider,\n",
    "    'ci_width': ci_slider,\n",
    "    'show_shading': shading_toggle,\n",
    "    'show_mean': mean_toggle,\n",
    "    'show_map': map_toggle,\n",
    "    'show_curves': curves_toggle\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a074f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc90f83a2ad441ea9802bfa672333de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<b style='color:blue'>Prior sliders</b>\"), HBox(children=(IntSlider(value=2, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfdb18ad4b74b0f8f0c12ba4ffa1b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# --- Plotting function ---\n",
    "def plot_bayes(k_prior, n_prior, k_like, n_like, ci_width, show_shading, show_mean, show_map, show_curves):\n",
    "    # Prior parameters\n",
    "    a_prior = k_prior + 1\n",
    "    b_prior = (n_prior - k_prior) + 1\n",
    "    y_prior = beta.pdf(x, a_prior, b_prior)\n",
    "\n",
    "    # Likelihood as Beta PDF (for visualization only)\n",
    "    a_like = k_like + 1\n",
    "    b_like = (n_like - k_like) + 1\n",
    "    y_like = beta.pdf(x, a_like, b_like)\n",
    "\n",
    "    # Posterior parameters (from conjugate Beta-Binomial update)\n",
    "    a_post = a_prior + k_like\n",
    "    b_post = b_prior + n_like - k_like\n",
    "    y_post = beta.pdf(x, a_post, b_post)\n",
    "\n",
    "    # Compute proper marginal likelihood\n",
    "    log_ml = betaln(k_like + a_prior, n_like - k_like + b_prior) - betaln(a_prior, b_prior)\n",
    "    marginal_likelihood = np.exp(log_ml)\n",
    "\n",
    "    # Compute Bayesian credible interval (BCI)\n",
    "    lower = beta.ppf((1 - ci_width / 100) / 2, a_post, b_post)\n",
    "    upper = beta.ppf(1 - (1 - ci_width / 100) / 2, a_post, b_post)\n",
    "\n",
    "    # Posterior Mean and MAP\n",
    "    posterior_mean = a_post / (a_post + b_post)\n",
    "    map_index = np.argmax(y_post)\n",
    "    posterior_map = x[map_index]\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    if show_curves:\n",
    "        ax.plot(x, y_prior, label=\"Prior\", color=\"blue\")\n",
    "        ax.plot(x, y_like, label=\"Likelihood (visualized)\", color=\"red\")\n",
    "\n",
    "    ml_text = f\"Posterior (marginal likelihood ≈ {marginal_likelihood:.3f})\"\n",
    "    ax.plot(x, y_post, label=ml_text, color=\"green\")\n",
    "\n",
    "    if show_shading:\n",
    "        ax.fill_between(x, y_post, where=(x >= lower) & (x <= upper), color='gray', alpha=0.3, label=f\"{ci_width}% BCI\")\n",
    "    else:\n",
    "        ax.axvline(lower, color='gray', linestyle='--', label=f\"{ci_width}% BCI\")\n",
    "        ax.axvline(upper, color='gray', linestyle='--')\n",
    "\n",
    "    if show_mean:\n",
    "        ax.axvline(posterior_mean, color='black', linestyle=':', label=\"Posterior Mean\")\n",
    "\n",
    "    if show_map:\n",
    "        ax.axvline(posterior_map, color='purple', linestyle='-.', label=\"Posterior MAP\")\n",
    "\n",
    "    ax.set_title(\"Bayesian Updating with BCI, Mean, and MAP\")\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Sliders\n",
    "k_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=1, description=\"k_prior\", style={'description_width': 'initial'})\n",
    "n_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=2, description=\"n_prior\", style={'description_width': 'initial'})\n",
    "\n",
    "k_like_slider = widgets.IntSlider(min=0, max=100, step=1, value=1, description=\"k_like\", style={'description_width': 'initial'})\n",
    "n_like_slider = widgets.IntSlider(min=0, max=100, step=1, value=2, description=\"n_like\", style={'description_width': 'initial'})\n",
    "\n",
    "ci_slider = widgets.IntSlider(min=50, max=99, step=1, value=95, description=\"BCI (%)\", style={'description_width': 'initial'})\n",
    "\n",
    "# Toggles (defaults set to False)\n",
    "shading_toggle = widgets.Checkbox(value=False, description='Shade BCI')\n",
    "mean_toggle = widgets.Checkbox(value=False, description='Show Posterior Mean')\n",
    "map_toggle = widgets.Checkbox(value=False, description='Show MAP')\n",
    "curves_toggle = widgets.Checkbox(value=False, description='Show Prior & Likelihood')\n",
    "\n",
    "# Reusable fix for k ≤ n\n",
    "def enforce_k_leq_n(k_slider, n_slider):\n",
    "    k_slider.max = n_slider.value\n",
    "    if k_slider.value > k_slider.max:\n",
    "        k_slider.value = k_slider.max\n",
    "\n",
    "# Observers\n",
    "def update_k_max_prior(*args):\n",
    "    enforce_k_leq_n(k_prior_slider, n_prior_slider)\n",
    "\n",
    "def update_k_max_like(*args):\n",
    "    enforce_k_leq_n(k_like_slider, n_like_slider)\n",
    "\n",
    "n_prior_slider.observe(update_k_max_prior, names='value')\n",
    "n_like_slider.observe(update_k_max_like, names='value')\n",
    "\n",
    "# Initial sync\n",
    "update_k_max_prior()\n",
    "update_k_max_like()\n",
    "\n",
    "# Layout\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<b style='color:blue'>Prior sliders</b>\"),\n",
    "    widgets.HBox([n_prior_slider, k_prior_slider]),\n",
    "    widgets.HTML(\"<b style='color:red'>Likelihood sliders</b>\"),\n",
    "    widgets.HBox([n_like_slider, k_like_slider]),\n",
    "    widgets.HTML(\"<b>Posterior Display Options</b>\"),\n",
    "    ci_slider,\n",
    "    shading_toggle,\n",
    "    mean_toggle,\n",
    "    map_toggle,\n",
    "    curves_toggle\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(plot_bayes, {\n",
    "    'k_prior': k_prior_slider,\n",
    "    'n_prior': n_prior_slider,\n",
    "    'k_like': k_like_slider,\n",
    "    'n_like': n_like_slider,\n",
    "    'ci_width': ci_slider,\n",
    "    'show_shading': shading_toggle,\n",
    "    'show_mean': mean_toggle,\n",
    "    'show_map': map_toggle,\n",
    "    'show_curves': curves_toggle\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf4e02f-4a3d-4275-8668-56707da36214",
   "metadata": {},
   "source": [
    "**Compute the posterior by updating the beta.** The beta distribution allows for a very simple way to compute the posterior just by adding n and k to the inputs to the Beta function. This is called computing the posterior analytically, and it is a special case of the *conjugate* prior. This simplicity is, alas, not always possible.\n",
    "\n",
    "**Exercise.** Click on the code cell below and press play to run.\n",
    "\n",
    "- Play with the sliders to see how the posterior changes as you change the number of successes and failures.\n",
    "\n",
    "- Notice that the posterior is another beta distribution, with parameters $n + k$ and $m + n - k$.\n",
    "\n",
    "- This makes it easy to compute the posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d38c24d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97edea2b8b0443291aa8fa72e8337fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=56, description='n (trials)', min=1, style=SliderStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c039cb78394fdc8d03c07c418a77ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "def plot_special_case(n, k):\n",
    "    if k > n:\n",
    "        print(\"Error: k must be ≤ n.\")\n",
    "        return\n",
    "\n",
    "    # Prior: Beta(1, 1)\n",
    "    a_prior, b_prior = 1, 1\n",
    "    y_prior = beta.pdf(x, a_prior, b_prior)\n",
    "\n",
    "    # Posterior: Beta(1 + k, 1 + n - k)\n",
    "    a_post = 1 + k\n",
    "    b_post = 1 + (n - k)\n",
    "    y_post = beta.pdf(x, a_post, b_post)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot prior\n",
    "    ax.plot(x, y_prior, label=\"Prior: Beta(1, 1)\", color=\"blue\")\n",
    "    ax.fill_between(x, y_prior, color=\"skyblue\", alpha=0.4)\n",
    "\n",
    "    # Plot posterior\n",
    "    full_label = f\"Posterior: Beta(1 + k, 1 + (n − k)) = Beta({a_post}, {b_post})\"\n",
    "    ax.plot(x, y_post, label=full_label, color=\"green\")\n",
    "    ax.fill_between(x, y_post, color=\"lightgreen\", alpha=0.4)\n",
    "\n",
    "    # Adjust layout and axis\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.set_ylim(0, max(np.max(y_post), np.max(y_prior)) * 1.2)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Move legend outside to the right\n",
    "    fig.subplots_adjust(right=0.75)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Sliders\n",
    "n_slider = widgets.IntSlider(min=1, max=100, step=1, value=56, description=\"n (trials)\", style={'description_width': 'initial'})\n",
    "k_slider = widgets.IntSlider(min=0, max=56, step=1, value=43, description=\"k (correct)\", style={'description_width': 'initial'})\n",
    "\n",
    "# Auto-adjust k bounds\n",
    "def update_k_slider(*args):\n",
    "    k_slider.max = n_slider.value\n",
    "    if k_slider.value > k_slider.max:\n",
    "        k_slider.value = k_slider.max\n",
    "\n",
    "n_slider.observe(update_k_slider, names='value')\n",
    "update_k_slider()  # Initial sync\n",
    "\n",
    "# Layout\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([n_slider, k_slider])\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(plot_special_case, {'n': n_slider, 'k': k_slider})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437e64",
   "metadata": {},
   "source": [
    "**Sequential vs aggregated updating.**  Bayesian updating can be done in two ways. You can update your beliefs step by step as new data arrives (sequential updating), or you can combine all the data and update in one go (aggregated updating). With conjugate priors like the Beta distribution, both methods give the same result. Let’s see how this works in a simple example.\n",
    "\n",
    "We’ll start with a uniform prior: Beta(1, 1). You then observe two datasets:\n",
    "- First: 9 correct out of 10 (k = 9, n = 10)\n",
    "- Second: 3 correct out of 5 (k = 3, n = 5)\n",
    "\n",
    "There are two ways to compute the posterior:\n",
    "\n",
    "**Sequential updating**  \n",
    "   - Start with Beta(1, 1)  \n",
    "   - Update with the first dataset → Beta(10, 2)  \n",
    "   - Use that as the new prior, update with the second dataset → Beta(13, 4)\n",
    "\n",
    "**Aggregated updating**  \n",
    "   - Combine the data: k = 12, n = 15  \n",
    "   - Start with Beta(1, 1), update once → Beta(13, 4)\n",
    "\n",
    "\n",
    "**Exercise.** Use the sliders to try both methods and confirm they give the same result.\n",
    "\n",
    "- First, try the sequential approach:  \n",
    "  Set prior to Beta(1, 1), then enter k = 9, n = 10.  \n",
    "  Use the resulting posterior as the new prior, then enter k = 3, n = 5.\n",
    "\n",
    "- Then try the aggregated approach:  \n",
    "  Set prior to Beta(1, 1), then enter k = 12, n = 15.\n",
    "\n",
    "- Do you get the same posterior in both cases?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "385e7e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78107b2ddb6047b290774f178c03fc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='🔵 <b>Prior</b>'), HBox(children=(IntSlider(value=20, description='n_prior'), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34460249b6a4c8399ea1cd9f21de81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "def plot_update(n_prior, k_prior, n_new, k_new):\n",
    "    if k_prior > n_prior or k_new > n_new:\n",
    "        print(\"Error: k must be ≤ n.\")\n",
    "        return\n",
    "\n",
    "    # Prior = Beta(1 + k_prior, 1 + n_prior - k_prior)\n",
    "    a_prior = 1 + k_prior\n",
    "    b_prior = 1 + (n_prior - k_prior)\n",
    "    y_prior = beta.pdf(x, a_prior, b_prior)\n",
    "\n",
    "    # Posterior = Beta(1 + total_k, 1 + total_n - total_k)\n",
    "    total_k = k_prior + k_new\n",
    "    total_n = n_prior + n_new\n",
    "    a_post = 1 + total_k\n",
    "    b_post = 1 + (total_n - total_k)\n",
    "    y_post = beta.pdf(x, a_post, b_post)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    ax.plot(x, y_prior, label=f\"Prior: Beta(1 + {k_prior}, 1 + {n_prior - k_prior})\", color=\"blue\")\n",
    "    ax.fill_between(x, y_prior, color=\"skyblue\", alpha=0.4)\n",
    "\n",
    "    ax.plot(x, y_post, label=f\"Posterior: Beta(1 + {total_k}, 1 + {total_n - total_k})\", color=\"green\")\n",
    "    ax.fill_between(x, y_post, color=\"lightgreen\", alpha=0.4)\n",
    "\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.set_ylim(0, max(np.max(y_prior), np.max(y_post)) * 1.2)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Adjust spacing and move legend to right\n",
    "    fig.subplots_adjust(right=0.75)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Sliders\n",
    "n_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=20, description=\"n_prior\")\n",
    "k_prior_slider = widgets.IntSlider(min=0, max=20, step=1, value=10, description=\"k_prior\")\n",
    "\n",
    "n_new_slider = widgets.IntSlider(min=0, max=100, step=1, value=30, description=\"n_new\")\n",
    "k_new_slider = widgets.IntSlider(min=0, max=30, step=1, value=15, description=\"k_new\")\n",
    "\n",
    "# Auto-limit k sliders\n",
    "def update_k_prior_max(*args):\n",
    "    k_prior_slider.max = n_prior_slider.value\n",
    "    if k_prior_slider.value > k_prior_slider.max:\n",
    "        k_prior_slider.value = k_prior_slider.max\n",
    "\n",
    "def update_k_new_max(*args):\n",
    "    k_new_slider.max = n_new_slider.value\n",
    "    if k_new_slider.value > k_new_slider.max:\n",
    "        k_new_slider.value = k_new_slider.max\n",
    "\n",
    "n_prior_slider.observe(update_k_prior_max, names='value')\n",
    "n_new_slider.observe(update_k_new_max, names='value')\n",
    "\n",
    "# Initial sync\n",
    "update_k_prior_max()\n",
    "update_k_new_max()\n",
    "\n",
    "# Layout\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"🔵 <b>Prior</b>\"),\n",
    "    widgets.HBox([n_prior_slider, k_prior_slider]),\n",
    "    widgets.HTML(\"🟢 <b>New Data</b>\"),\n",
    "    widgets.HBox([n_new_slider, k_new_slider])\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(plot_update, {\n",
    "    'n_prior': n_prior_slider,\n",
    "    'k_prior': k_prior_slider,\n",
    "    'n_new': n_new_slider,\n",
    "    'k_new': k_new_slider\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678268e1",
   "metadata": {},
   "source": [
    "**MCMC.** To illustrate the power of MCMC, we demo with this interactive example. The demo runs JAGS via python. JAGS is a sampler that implements MCMC, though there are many samplers available. The model is set up via as specific language. It is everything inside \"model {...}\" in the code cell below. Here the model is a simple Bernoulli model, where we have a prior on theta, and we have a likelihood for observe k successes out of n trials.\n",
    "\n",
    "**Exercise.** Play with the data k and n and set the number of samples that sampler runs. Keep it low to begin with.\n",
    "\n",
    "-  Compare sampling to an analytical posterior for the same data. Click on the show analytic posteror button to overlay the analytical posterior. \n",
    "\n",
    "- How do you get the MCMC posterior to better match the analytical posterior? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29872727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca67d5abc7e4e9abeb25edcfdef0fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=6, description='Successes (k):', max=20), IntSlider(value=10, description='Tria…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa7c4bc00f549169563cf3cbab3f887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from scipy.stats import beta\n",
    "\n",
    "# Ensure plots show inline\n",
    "%matplotlib inline\n",
    "\n",
    "def run_jags(k=6, n=10, samples=100, show_analytic=False):\n",
    "    if k > n:\n",
    "        print(\"k must be ≤ n\")\n",
    "        return\n",
    "\n",
    "    # Data setup\n",
    "    y = np.array([1]*k + [0]*(n-k))\n",
    "\n",
    "    # JAGS model string\n",
    "    model_code = \"\"\"\n",
    "    model {\n",
    "      for (i in 1:N) {\n",
    "        y[i] ~ dbern(theta)\n",
    "      }\n",
    "      theta ~ dbeta(1, 1)\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    data = {\"y\": y, \"N\": len(y)}\n",
    "    inits = [{\"theta\": 0.5}]\n",
    "\n",
    "    # Run JAGS model\n",
    "    model = pyjags.Model(code=model_code, data=data, init=inits, chains=1)\n",
    "    model.update(100)  # Burn-in\n",
    "    result = model.sample(samples, vars=[\"theta\"])\n",
    "    theta_samples = result[\"theta\"].reshape(-1)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # --- Trace plot ---\n",
    "    axs[0].plot(theta_samples)\n",
    "    axs[0].set_title(\"Trace of θ (Chain 1)\")\n",
    "    axs[0].set_xlabel(\"Sample\")\n",
    "    axs[0].set_ylabel(\"θ\")\n",
    "    axs[0].set_ylim(0, 1)\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # --- Horizontal histogram ---\n",
    "    axs[1].hist(theta_samples, bins=30, density=True, alpha=0.6, color='gray', orientation='horizontal', label='MCMC posterior')\n",
    "    axs[1].set_title(\"Posterior of θ\")\n",
    "    axs[1].set_xlabel(\"Density\")\n",
    "    axs[1].set_ylabel(\"θ\")\n",
    "    axs[1].set_ylim(0, 1)\n",
    "\n",
    "    # --- Line-over-histogram plot ---\n",
    "    axs[2].hist(theta_samples, bins=30, density=True, alpha=0.5, color='steelblue', label='MCMC posterior')\n",
    "\n",
    "    # Optional analytical overlay\n",
    "    if show_analytic:\n",
    "        alpha_post = 1 + k\n",
    "        beta_post = 1 + (n - k)\n",
    "        theta_vals = np.linspace(0, 1, 200)\n",
    "        analytical_posterior = beta.pdf(theta_vals, alpha_post, beta_post)\n",
    "        axs[1].plot(analytical_posterior, theta_vals, 'r-', lw=2, label='Analytical posterior')\n",
    "        axs[1].legend()\n",
    "\n",
    "        axs[2].plot(theta_vals, analytical_posterior, 'r-', lw=2, label='Analytical posterior')\n",
    "        axs[2].legend()\n",
    "\n",
    "    axs[2].set_title(f\"Posterior of θ (k={k}, n={n})\")\n",
    "    axs[2].set_xlabel(\"θ\")\n",
    "    axs[2].set_ylabel(\"Density\")\n",
    "    axs[2].set_xlim(0, 1)\n",
    "    axs[2].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Widgets\n",
    "k_slider = widgets.IntSlider(value=6, min=0, max=20, step=1, description='Successes (k):')\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=20, step=1, description='Trials (n):')\n",
    "sample_slider = widgets.IntSlider(value=100, min=100, max=10000, step=100, description='Samples:')\n",
    "\n",
    "analytic_toggle = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Show analytic posterior',\n",
    "    tooltip='Toggle analytical posterior overlay',\n",
    "    icon='line-chart'\n",
    ")\n",
    "\n",
    "# Link function to widgets\n",
    "ui = widgets.VBox([k_slider, n_slider, sample_slider, analytic_toggle])\n",
    "out = widgets.interactive_output(run_jags, {\n",
    "    'k': k_slider,\n",
    "    'n': n_slider,\n",
    "    'samples': sample_slider,\n",
    "    'show_analytic': analytic_toggle\n",
    "})\n",
    "\n",
    "# Display\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca4255",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lecture 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c35c64",
   "metadata": {},
   "source": [
    "**Binomial distribution.**  Here we can see the binomial distribution: the probability of getting `k` successes in `n` independent trials, where each trial has a success probability of `θ`. Use the sliders below to adjust `n` and `θ`, and watch how the shape of the distribution changes. The equation updates to show how the probability is computed based on the current values.\n",
    "\n",
    "**Exercise.** Play with the sliders and answer the following:\n",
    "\n",
    "- What happens when θ is close to 0 or close to 1?\n",
    "\n",
    "- How does the distribution change as you increase n?\n",
    "\n",
    "- When is the distribution symmetric, and when is it skewed?\n",
    "\n",
    "- What value of `k` seems most likely, and how does that relate to `θ × n`?\n",
    "\n",
    "Try predicting the outcome before you move the sliders — then use the plot to check your intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3700289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec9bc8819724b00890fe7313d609c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='θ', max=0.99, min=0.01, step=0.01), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(theta, n)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n",
    "# Define sliders\n",
    "theta_slider = widgets.FloatSlider(value=0.5, min=0.01, max=0.99, step=0.01, description='θ')\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=100, step=1, description='n')\n",
    "\n",
    "# Define function to update the plot and LaTeX\n",
    "def update_plot(theta, n):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Binomial support and PMF\n",
    "    k = np.arange(0, n+1)\n",
    "    pmf = [np.math.comb(n, ki) * theta**ki * (1 - theta)**(n - ki) for ki in k]\n",
    "    \n",
    "    # Display formula with current values\n",
    "    display(Math(f\"P(k\\\\mid\\\\theta={theta:.2f},\\\\ n={n}) = \\\\binom{{n}}{{k}} \\\\theta^k (1 - \\\\theta)^{{n - k}}\"))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(k, pmf, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Binomial Distribution: n={n}, θ={theta:.2f}')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('P(k | θ, n)')\n",
    "    plt.grid(True, axis='y', linestyle=':')\n",
    "    plt.show()\n",
    "\n",
    "# Interactive output\n",
    "widgets.interact(update_plot, theta=theta_slider, n=n_slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b385ebf",
   "metadata": {},
   "source": [
    "**MCMC convergaence checks.** This is the same model as the one we used in the MCMC demo. The model is a simple Bernoulli model, where we have a prior on theta, and we have a likelihood for observe k successes out of n trials. The model is set up via as specific language. It is everything inside \"model {...}\" in the code cell below. We run it here so that we can look at the model outputs and see if they look ok. \n",
    "\n",
    "**Exercise.** Play with the sliders, run the model, and check the convergence diagnostics.\n",
    "- Do the chains look like they have converged?\n",
    "- Do they look like \"hairy caterpillars\"?\n",
    "- Is the R-hat statistic close to 1?\n",
    "- Is the Pope Catholic?\n",
    "- Just checking you are awake. x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "795161db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6a2e26154443f4b8b268b01069f7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=6, description='Successes (k)', max=20), IntSlider(value=10, description='Trial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n",
    "# Define the JAGS model as a string\n",
    "model_code = \"\"\"\n",
    "model {\n",
    "  theta ~ dbeta(1,1)\n",
    "  k ~ dbin(theta, n)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# R-hat computation\n",
    "def compute_rhat(chains):\n",
    "    m = len(chains)\n",
    "    n = len(chains[0])\n",
    "    \n",
    "    chain_means = np.array([np.mean(c) for c in chains])\n",
    "    chain_vars = np.array([np.var(c, ddof=1) for c in chains])\n",
    "\n",
    "    B = n * np.var(chain_means, ddof=1)\n",
    "    W = np.mean(chain_vars)\n",
    "    var_hat = ((n - 1) / n) * W + (1 / n) * B\n",
    "    Rhat = np.sqrt(var_hat / W)\n",
    "\n",
    "    return Rhat, B, W, var_hat, chain_means, chain_vars\n",
    "\n",
    "# Main model runner\n",
    "def run_model(k, n, samples):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Running with k={k}, n={n}, samples={samples} per chain\\n\")\n",
    "    \n",
    "    chains = []\n",
    "    for i in range(4):\n",
    "        data = {'k': k, 'n': n}\n",
    "        model = pyjags.Model(code=model_code, data=data, chains=1, adapt=500)\n",
    "        model.update(1000)\n",
    "        result = model.sample(samples, vars=['theta'])\n",
    "        theta_samples = result['theta'].reshape(-1)\n",
    "        chains.append(theta_samples)\n",
    "        plt.plot(theta_samples, label=f\"Chain {i+1}\")\n",
    "\n",
    "    plt.title(\"Trace plots of θ for 4 chains\")\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"θ\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Compute R-hat\n",
    "    Rhat, B, W, var_hat, chain_means, chain_vars = compute_rhat(chains)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Per-chain statistics:\")\n",
    "    for i, (mean, var) in enumerate(zip(chain_means, chain_vars)):\n",
    "        print(f\"  Chain {i+1}: mean = {mean:.4f}, var = {var:.5f}\")\n",
    "\n",
    "    print(f\"\\nBetween-chain variance (B): {B:.5f}\")\n",
    "    print(f\"Within-chain variance (W): {W:.5f}\")\n",
    "    print(f\"Estimated variance (var_hat): {var_hat:.5f}\")\n",
    "    print(f\"R-hat: {Rhat:.4f}\\n\")\n",
    "\n",
    "    display(Math(r\"\"\"\n",
    "    \\hat{R} = \\sqrt{ \\frac{ \\left( \\frac{n-1}{n} \\right) W + \\left( \\frac{1}{n} \\right) B }{W} }\n",
    "    \"\"\"))\n",
    "\n",
    "# Interactive widgets\n",
    "k_slider = widgets.IntSlider(value=6, min=0, max=20, description='Successes (k)')\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=20, description='Trials (n)')\n",
    "samples_slider = widgets.IntSlider(value=1000, min=100, max=5000, step=100, description='Samples')\n",
    "\n",
    "run_button = widgets.Button(description=\"Run model\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_click(b):\n",
    "    with output:\n",
    "        run_model(k_slider.value, n_slider.value, samples_slider.value)\n",
    "\n",
    "run_button.on_click(on_click)\n",
    "display(widgets.VBox([k_slider, n_slider, samples_slider, run_button, output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270a4e61",
   "metadata": {},
   "source": [
    "**Inferring the difference between two rates.** In this model, we observe two processes — for example, two groups or experimental conditions — each with a number of successes out of a number of trials. We assume the underlying success rates are governed by parameters $\\theta_1$ and $\\theta_2$, with uniform Beta priors:\n",
    "\n",
    "$\\theta_1 \\sim \\text{Beta}(1, 1)$, $\\theta_2 \\sim \\text{Beta}(1, 1)$  \n",
    "$k_1 \\sim \\text{Binomial}(n_1, \\theta_1)$, $k_2 \\sim \\text{Binomial}(n_2, \\theta_2)$\n",
    "\n",
    "Our quantity of interest is the difference in success rates:\n",
    "\n",
    "$\\delta = \\theta_1 - \\theta_2$\n",
    "\n",
    "This tells us how much more (or less) likely success is in one group than the other.\n",
    "\n",
    "Use the sliders to adjust $k_1$, $n_1$, $k_2$, and $n_2$. The trace plots show MCMC samples of $\\delta$, and the histogram shows its posterior distribution. You can also overlay an approximate analytical solution for comparison.\n",
    "\n",
    "**Exercise.** Explore the model using the sliders and answer the following:\n",
    "\n",
    "- When does the posterior of $\\delta$ look symmetric? When is it skewed?\n",
    "- How does increasing $n_1$ or $n_2$ affect the width of the posterior?\n",
    "- What happens when $k_1 = k_2$ but $n_1 \\ne n_2$?\n",
    "- Use the checkbox to compare the MCMC posterior with the analytical approximation. How close are they?\n",
    "\n",
    "Try predicting what the posterior will look like before adjusting the sliders — then use the plot to check your intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114766e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4849afe15614cc6be8970f459b103f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=15, description='k₁', max=30), IntSlider(value=20, description='n₁', max=40, mi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Math, clear_output\n",
    "\n",
    "# JAGS model code\n",
    "model_code = \"\"\"\n",
    "model {\n",
    "  k1 ~ dbin(theta1, n1)\n",
    "  k2 ~ dbin(theta2, n2)\n",
    "  theta1 ~ dbeta(1,1)\n",
    "  theta2 ~ dbeta(1,1)\n",
    "  delta <- theta1 - theta2\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# R-hat calculation\n",
    "def compute_rhat(chains):\n",
    "    m = len(chains)\n",
    "    n = len(chains[0])\n",
    "    chain_means = np.array([np.mean(c) for c in chains])\n",
    "    chain_vars = np.array([np.var(c, ddof=1) for c in chains])\n",
    "    B = n * np.var(chain_means, ddof=1)\n",
    "    W = np.mean(chain_vars)\n",
    "    var_hat = ((n - 1) / n) * W + (1 / n) * B\n",
    "    Rhat = np.sqrt(var_hat / W)\n",
    "    return Rhat, B, W, var_hat, chain_means, chain_vars\n",
    "\n",
    "# Main model function\n",
    "def run_model(k1, n1, k2, n2, samples):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Running with (k1={k1}, n1={n1}), (k2={k2}, n2={n2}), samples={samples} per chain\")\n",
    "\n",
    "    data = {\"k1\": k1, \"n1\": n1, \"k2\": k2, \"n2\": n2}\n",
    "    chains = []\n",
    "\n",
    "    for i in range(4):\n",
    "        model = pyjags.Model(code=model_code, data=data, chains=1, adapt=500)\n",
    "        model.update(1000)\n",
    "        result = model.sample(samples, vars=[\"delta\"])\n",
    "        delta_chain = result[\"delta\"].reshape(-1)\n",
    "        chains.append(delta_chain)\n",
    "\n",
    "    # Plot traces and histogram\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    grid = plt.GridSpec(1, 2, width_ratios=[3, 1], wspace=0.3)\n",
    "\n",
    "    ax_traces = fig.add_subplot(grid[0])\n",
    "    for i in range(4):\n",
    "        ax_traces.plot(chains[i], label=f\"Chain {i+1}\")\n",
    "    ax_traces.set_title(\"Trace plots of δ (4 chains)\")\n",
    "    ax_traces.set_xlabel(\"Sample\")\n",
    "    ax_traces.set_ylabel(\"δ = θ₁ − θ₂\")\n",
    "    ax_traces.axhline(0, color='black', linestyle='--', lw=1)\n",
    "    ax_traces.legend()\n",
    "    ax_traces.grid(True)\n",
    "\n",
    "    ax_hist = fig.add_subplot(grid[1])\n",
    "    all_samples = np.concatenate(chains)\n",
    "    ax_hist.hist(all_samples, bins=30, orientation='horizontal', density=True, color='gray', alpha=0.6, label='Posterior')\n",
    "    ax_hist.set_title(\"Posterior of δ\")\n",
    "    ax_hist.set_xlabel(\"Density\")\n",
    "    ax_hist.set_ylabel(\"δ\")\n",
    "    ax_hist.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # R-hat diagnostics\n",
    "    Rhat, B, W, var_hat, chain_means, chain_vars = compute_rhat(chains)\n",
    "\n",
    "    print(\"Per-chain statistics:\")\n",
    "    for i, (mean, var) in enumerate(zip(chain_means, chain_vars)):\n",
    "        print(f\"  Chain {i+1}: mean = {mean:.4f}, var = {var:.5f}\")\n",
    "\n",
    "    print(f\"\\nBetween-chain variance (B): {B:.5f}\")\n",
    "    print(f\"Within-chain variance (W): {W:.5f}\")\n",
    "    print(f\"Estimated variance (var_hat): {var_hat:.5f}\")\n",
    "    print(f\"R-hat: {Rhat:.4f}\\n\")\n",
    "\n",
    "    display(Math(r\"\"\"\n",
    "    \\hat{R} = \\sqrt{ \\frac{ \\left( \\frac{n-1}{n} \\right) W + \\left( \\frac{1}{n} \\right) B }{W} }\n",
    "    \"\"\"))\n",
    "\n",
    "# Sliders\n",
    "k1_slider = widgets.IntSlider(value=15, min=0, max=30, description='k₁')\n",
    "n1_slider = widgets.IntSlider(value=20, min=1, max=40, description='n₁')\n",
    "k2_slider = widgets.IntSlider(value=10, min=0, max=30, description='k₂')\n",
    "n2_slider = widgets.IntSlider(value=20, min=1, max=40, description='n₂')\n",
    "samples_slider = widgets.IntSlider(value=1000, min=100, max=5000, step=100, description='Samples')\n",
    "\n",
    "run_button = widgets.Button(description=\"Run model\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_click(b):\n",
    "    with output:\n",
    "        run_model(\n",
    "            k1_slider.value, n1_slider.value,\n",
    "            k2_slider.value, n2_slider.value,\n",
    "            samples_slider.value\n",
    "        )\n",
    "\n",
    "run_button.on_click(on_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    k1_slider, n1_slider, k2_slider, n2_slider,\n",
    "    samples_slider, run_button, output\n",
    "]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
