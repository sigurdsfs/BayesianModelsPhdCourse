{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93f74307",
   "metadata": {},
   "source": [
    "\n",
    "# Bayesian modeling of minds, brains and behavior\n",
    "---\n",
    "**Overview.** This notebook allows you to run Python code in your browser. We can mix text and code in a single document. This iis intended to support the lectures allowing me to demo certain concepts in a hands on way. It is also designed for you to work through it slowly after lectures, reading the text and doing the exercises by playing with the plots or even altering or writing code if you want to. We have an emphasis on play. Mess around. \"I wonder what happens if I change this?\". Let's start with a simple example of a Gaussian distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## Lecture 2\n",
    "\n",
    "**Beliefs as distributions.** Here you can adjust the sliders to change the mean (μ) and standard deviation (σ) of this Gaussian distribution. The mean moves the distribution so that its mean is higher or lower. The standard deviation changes the spread, changing the uncertainty of the belief\n",
    "\n",
    "**Exercise.** Click on code cell below and press play to run. Play around with the distributions and think about how they would represent belief. Use the sliders to generate distributions that represent different prior beliefs for theta:\n",
    "\n",
    "  - Certain belief that the theta is high\n",
    "\n",
    "  - Certain belief that the theta is low \n",
    "  \n",
    "  - Uncertain belief that the theta is high   \n",
    "  \n",
    "  - Uncertain belief that the theta is low*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59dabc1f",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bca2988fdec405899d3ce0706389430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='μ', max=10.0, min=-10.0), FloatSlider(value=1.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_gaussian(mu=0.0, sigma=1.0):\n",
    "    # Fixed x range\n",
    "    x = np.linspace(-20, 20, 1000)\n",
    "    y = norm.pdf(x, mu, sigma)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(x, y, label=f'N({mu:.2f}, {sigma:.2f}²)', color='purple')\n",
    "    ax.fill_between(x, y, color='plum', alpha=0.4)\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.set_xlim(-20, 20)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    plot_gaussian,\n",
    "    mu=widgets.FloatSlider(min=-10, max=10, step=0.1, value=0.0, description=\"μ\"),\n",
    "    sigma=widgets.FloatSlider(min=0.1, max=5.0, step=0.1, value=1.0, description=\"σ\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca423f",
   "metadata": {},
   "source": [
    "**Interpreting probability distributions.** There are intuitive ways to read off probabilities from probability distributions like these. According to the belief encoded by the distribution, the probability of theta being larger than say 0.5 or 5 is the area under the curve above this value. Similarly, the probability of theta being between say 0.1 and 0.7 is the area under the curve between these two values\n",
    "\n",
    "**Exercise.** Click on the code cell below and press play to run. \n",
    "\n",
    "- Play with the sliders to see how the area under the curve changes as you change the minimum and maximum values. \n",
    "\n",
    "- The shaded area represents the probability of theta being in the range you selected.\n",
    "\n",
    "- What happens when you make the distribution more narrow, or more wide? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04c3ddb",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe4506d171f4f9ebca5570e68e279fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='μ', max=10.0, min=-10.0), FloatSlider(value=1.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_gaussian(mu=0.0, sigma=1.0, x_min=-1.0, x_max=1.0)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_gaussian(mu=0.0, sigma=1.0, x_min=-1.0, x_max=1.0):\n",
    "    # Fixed x range\n",
    "    x = np.linspace(-20, 20, 1000)\n",
    "    y = norm.pdf(x, mu, sigma)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(x, y, label=f'N({mu:.2f}, {sigma:.2f}²)', color='purple')\n",
    "\n",
    "    # Fill full curve lightly\n",
    "    ax.fill_between(x, y, color='plum', alpha=0.2)\n",
    "\n",
    "    # Highlight area between x_min and x_max\n",
    "    mask = (x >= x_min) & (x <= x_max)\n",
    "    ax.fill_between(x[mask], y[mask], color='mediumvioletred', alpha=0.6,\n",
    "                    label=f\"P({x_min:.2f} < θ < {x_max:.2f}) ≈ {norm.cdf(x_max, mu, sigma) - norm.cdf(x_min, mu, sigma):.2f}\")\n",
    "\n",
    "    ax.set_title(\"Probability as Area Under the Curve\")\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.set_xlim(-20, 20)\n",
    "    ax.set_ylim(0, max(y) * 1.1)\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    plot_gaussian,\n",
    "    mu=widgets.FloatSlider(min=-10, max=10, step=0.1, value=0.0, description=\"μ\"),\n",
    "    sigma=widgets.FloatSlider(min=0.1, max=5.0, step=0.1, value=1.0, description=\"σ\"),\n",
    "    x_min=widgets.FloatSlider(min=-10, max=10, step=0.1, value=-1.0, description=\"x min\"),\n",
    "    x_max=widgets.FloatSlider(min=-10, max=10, step=0.1, value=1.0, description=\"x max\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b178de",
   "metadata": {},
   "source": [
    "**Prior beliefs for theta.** We need to think carefully about what theta means. Theta represents cognitive ability in our go-no-go task. 0 is the worst possible cognitive ability, and 1 is the best. So we should set a prior distribution for theta between 0 and 1. The distributions above do not do this, so let's fix this. We can use a beta distribution to represent our prior belief about theta. The beta distribution is a continuous probability distribution defined on the interval [0, 1], so it is perfect for our needs.\n",
    "\n",
    "\n",
    "**Exercise.** Play around with beta distribution parameters to see how the shape of the distribution changes:\n",
    "\n",
    "  - You are complete uncertain about the ability → Set the widest prior\n",
    "\n",
    "  - You are quite certain that ability will be high  → Set a narrow prior centered on a high value\n",
    "\n",
    "  - You are quite certain that ability will be low  → Set a narrow prior centered on a low value\n",
    "\n",
    "  - You are uncertain but you think ability is low  → Set a wide prior centered on a low value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19e2dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54f84537a3b45c284327be0d27f9f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=10, description='n', min=1), IntSlider(value=5, description='k')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975f9c970bbd41fcb261178838e99ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Sliders\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=100, step=1, description=\"n\")\n",
    "k_slider = widgets.IntSlider(value=5, min=0, max=100, step=1, description=\"k\")\n",
    "\n",
    "# Output area\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "# Plot function\n",
    "def plot_beta(k, n):\n",
    "    x = np.linspace(0, 1, 500)\n",
    "    a = k + 1\n",
    "    b = (n - k) + 1\n",
    "    y = beta.pdf(x, a, b)\n",
    "\n",
    "    with plot_output:\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.plot(x, y, label=f\"Beta({a}, {b})\", color='blue')\n",
    "        ax.fill_between(x, y, color='skyblue', alpha=0.3)\n",
    "        ax.set_title(\"Prior beliefs over θ\")\n",
    "        ax.set_xlabel(\"θ\")\n",
    "        ax.set_ylabel(\"Probability Density\")\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Unified update function\n",
    "def on_slider_change(change=None):\n",
    "    # Avoid recursive triggers by updating value only if needed\n",
    "    if k_slider.value > n_slider.value:\n",
    "        k_slider.unobserve(on_slider_change, names='value')\n",
    "        k_slider.value = n_slider.value\n",
    "        k_slider.observe(on_slider_change, names='value')\n",
    "    plot_beta(k_slider.value, n_slider.value)\n",
    "\n",
    "# Set up observers (after defining callback)\n",
    "n_slider.observe(on_slider_change, names='value')\n",
    "k_slider.observe(on_slider_change, names='value')\n",
    "\n",
    "# Layout and display\n",
    "display(widgets.VBox([n_slider, k_slider]), plot_output)\n",
    "\n",
    "# Initial plot\n",
    "plot_beta(k_slider.value, n_slider.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd3b71-1526-4dc8-ac1b-b197119e0448",
   "metadata": {},
   "source": [
    "**Multiplying prior and likelihood.**  We’ve learned that the posterior is proportional to the prior times the likelihood:\n",
    "\n",
    "$$\n",
    "\\text{Posterior} \\propto \\text{Prior} \\times \\text{Likelihood}\n",
    "$$\n",
    "\n",
    "The likelihood tells us how well each value of $\\theta$ predicted the data. If a value of $\\theta$ predicts the data well → the posterior belief increases. If it predicts the data poorly → the posterior belief decreases. So the posterior reshapes your belief, using the likelihood as a reweighting function over your prior.\n",
    "\n",
    "**Exercise.** Now you can explore this directly: Use the sliders to define a prior distribution and a likelihood. The resulting product is shown as the posterior. By default, this is not normalized (it doesnt inegrate to 1) — you are just seeing the raw shape. This shape still carries meaning: it shows which values of $\\theta$ are most consistent with both your prior beliefs and the observed data.\n",
    "- Set a prior and likelihood.\n",
    "- Pick a few values of theta, and multiply the prior by the likelihood.\n",
    "- See if the height of the posterior curve matches your expectation.\n",
    "\n",
    "**Normalising the Posterior.** You can press the button to normalize the posterior.\n",
    "To turn the posterior into a true probability distribution, we need to divide by the marginal likelihood — the constant that ensures the posterior integrates to 1 and is therefore a proper probability distribution.\n",
    "\n",
    "$$\n",
    "p(\\theta \\mid \\text{data}) = \\frac{p(\\theta) \\cdot p(\\text{data} \\mid \\theta)}{p(\\text{data})}\n",
    "$$\n",
    "\n",
    "**Exercise.** Click on the button to normalize the posterior. \n",
    "\n",
    "- Notice how the area under the curve is now 1, and the posterior is a proper probability distribution.\n",
    "\n",
    "- The area under the prior and the posterior should be the same. \n",
    "\n",
    "- Do you need the same amount of paint to colour each in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d12ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2192f98502c748fe82e0f8d2e7be03ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<b style='color:blue'>Prior sliders</b>\"), HBox(children=(IntSlider(value=2, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b670b7fb9ee4421e8f49587f6808693b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# --- Plotting function ---\n",
    "def plot_bayes(k_prior, n_prior, k_like, n_like, show_norm_post):\n",
    "    # Prior\n",
    "    a_prior = k_prior + 1\n",
    "    b_prior = (n_prior - k_prior) + 1\n",
    "    y_prior = beta.pdf(x, a_prior, b_prior)\n",
    "\n",
    "    # Likelihood (visual only)\n",
    "    a_like = k_like + 1\n",
    "    b_like = (n_like - k_like) + 1\n",
    "    y_like = beta.pdf(x, a_like, b_like)\n",
    "\n",
    "    # Unnormalized Posterior\n",
    "    y_post = y_prior * y_like\n",
    "\n",
    "    # Normalize if requested\n",
    "    if show_norm_post:\n",
    "        norm_const = np.trapz(y_post, x)\n",
    "        y_post = y_post / norm_const if norm_const > 0 else np.zeros_like(x)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(x, y_prior, label=\"Prior\", color=\"blue\")\n",
    "    ax.plot(x, y_like, label=\"Likelihood\", color=\"red\")\n",
    "    ax.plot(x, y_post, label=\"Posterior\", color=\"green\")\n",
    "\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 10)  # Or another sensible fixed upper limit\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# --- Sliders ---\n",
    "k_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=1, description=\"k_prior\")\n",
    "n_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=2, description=\"n_prior\")\n",
    "\n",
    "k_like_slider = widgets.IntSlider(min=0, max=100, step=1, value=1, description=\"k_like\")\n",
    "n_like_slider = widgets.IntSlider(min=0, max=100, step=1, value=2, description=\"n_like\")\n",
    "\n",
    "# --- Checkbox to toggle normalization\n",
    "show_norm_post = widgets.Checkbox(value=False, description=\"Normalize Posterior\", indent=False)\n",
    "\n",
    "# --- Ensure k ≤ n ---\n",
    "def update_k_max(slider_k, slider_n):\n",
    "    slider_k.max = slider_n.value\n",
    "    if slider_k.value > slider_k.max:\n",
    "        slider_k.value = slider_k.max\n",
    "\n",
    "def update_k_max_prior(*args):\n",
    "    update_k_max(k_prior_slider, n_prior_slider)\n",
    "\n",
    "def update_k_max_like(*args):\n",
    "    update_k_max(k_like_slider, n_like_slider)\n",
    "\n",
    "n_prior_slider.observe(update_k_max_prior, names='value')\n",
    "n_like_slider.observe(update_k_max_like, names='value')\n",
    "update_k_max_prior()\n",
    "update_k_max_like()\n",
    "\n",
    "# --- UI layout ---\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<b style='color:blue'>Prior sliders</b>\"),\n",
    "    widgets.HBox([n_prior_slider, k_prior_slider]),\n",
    "    widgets.HTML(\"<b style='color:red'>Likelihood sliders</b>\"),\n",
    "    widgets.HBox([n_like_slider, k_like_slider]),\n",
    "    show_norm_post\n",
    "])\n",
    "\n",
    "# --- Plot binding\n",
    "out = widgets.interactive_output(plot_bayes, {\n",
    "    'k_prior': k_prior_slider,\n",
    "    'n_prior': n_prior_slider,\n",
    "    'k_like': k_like_slider,\n",
    "    'n_like': n_like_slider,\n",
    "    'show_norm_post': show_norm_post\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac8b3c",
   "metadata": {},
   "source": [
    "**Is the Likelihood a proper probability distribution?** Does the likelihood integrate to 1? Not the way we plot it. The likelihood is $p(\\mathrm{data} \\mid \\theta)$: a probability distribution over the data, given a fixed value of $\\theta$. It is not a probability distribution over $\\theta$. So when we plot $\\theta \\mapsto p(\\mathrm{data} \\mid \\theta)$, we are not plotting a probability density function — we're plotting a likelihood function, and it does not need to integrate to 1 over $\\theta$. However, for each fixed $\\theta$, the function is a proper probability distribution over data, and it satisfies:\n",
    "\n",
    "$$\n",
    "\\int p(\\mathrm{data} \\mid \\theta) \\, d\\,\\mathrm{data} = 1\n",
    "$$\n",
    "\n",
    "So yes — the likelihood does integrate to 1, but only over the data, not over $\\theta$.\n",
    "\n",
    "**Exercise.** Wiggle the sliders to get different likelihoods:\n",
    "\n",
    "- Left, when you plot the likelihood, over different values of theta, you can see that the likelihood does not integrate to 1.\n",
    "\n",
    "- Right, when you plot the likelihood over different values of data, you can see that the likelihood does integrate to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8238e17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0718fa5784e04589ae7e7dd3d8691224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Left: Likelihood as a function of θ'), HTML(value='Right: Likelihood as a function …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a21a0a27934b4f847c43a5a199198c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TraitError",
     "evalue": "The 'value' trait of an IntSlider instance expected an int, not the NoneType None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/bayesian-models/lib/python3.10/site-packages/traitlets/traitlets.py:2691\u001b[0m, in \u001b[0;36mCInt.validate\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m   2690\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2691\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTraitError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/bayesian-models/lib/python3.10/site-packages/ipywidgets/widgets/widget.py:773\u001b[0m, in \u001b[0;36mWidget._handle_msg\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffer_paths\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m    772\u001b[0m             _put_buffers(state, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffer_paths\u001b[39m\u001b[38;5;124m'\u001b[39m], msg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffers\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 773\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# Handle a state request.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest_state\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/bayesian-models/lib/python3.10/site-packages/ipywidgets/widgets/widget.py:655\u001b[0m, in \u001b[0;36mWidget.set_state\u001b[0;34m(self, sync_data)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys:\n\u001b[1;32m    653\u001b[0m     from_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrait_metadata(name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom_json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    654\u001b[0m                                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trait_from_json)\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_trait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43msync_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/bayesian-models/lib/python3.10/site-packages/traitlets/traitlets.py:1764\u001b[0m, in \u001b[0;36mHasTraits.set_trait\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_trait(name):\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraitError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not have a trait named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1764\u001b[0m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/bayesian-models/lib/python3.10/site-packages/traitlets/traitlets.py:690\u001b[0m, in \u001b[0;36mTraitType.set\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: HasTraits, value: S) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/bayesian-models/lib/python3.10/site-packages/traitlets/traitlets.py:722\u001b[0m, in \u001b[0;36mTraitType._validate\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 722\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_cross_validation_lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    724\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cross_validate(obj, value)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/bayesian-models/lib/python3.10/site-packages/traitlets/traitlets.py:2693\u001b[0m, in \u001b[0;36mCInt.validate\u001b[0;34m(self, obj, value)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(value)\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 2693\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mcast(G, _validate_bounds(\u001b[38;5;28mself\u001b[39m, obj, value))\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/bayesian-models/lib/python3.10/site-packages/traitlets/traitlets.py:831\u001b[0m, in \u001b[0;36mTraitType.error\u001b[0;34m(self, obj, value, error, info)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    826\u001b[0m     e \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m trait expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, not \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    827\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    828\u001b[0m         info \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo(),\n\u001b[1;32m    829\u001b[0m         describe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m\"\u001b[39m, value),\n\u001b[1;32m    830\u001b[0m     )\n\u001b[0;32m--> 831\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TraitError(e)\n",
      "\u001b[0;31mTraitError\u001b[0m: The 'value' trait of an IntSlider instance expected an int, not the NoneType None."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Plotting function ---\n",
    "def plot_likelihood_views(n, k, theta_fixed):\n",
    "    theta_vals = np.linspace(0, 1, 500)\n",
    "    likelihood_theta = binom.pmf(k, n, theta_vals)\n",
    "    area_theta = np.trapz(likelihood_theta, theta_vals)\n",
    "\n",
    "    k_vals = np.arange(0, n + 1)\n",
    "    likelihood_data = binom.pmf(k_vals, n, theta_fixed)\n",
    "    area_data = np.sum(likelihood_data)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Left: likelihood as function of θ\n",
    "    axes[0].plot(theta_vals, likelihood_theta, color='black', lw=2)\n",
    "    axes[0].set_title(rf\"$\\theta \\mapsto p(k={k} \\mid \\theta, n={n})$\")\n",
    "    axes[0].set_xlabel(\"θ\")\n",
    "    axes[0].set_ylabel(r\"$p(k \\mid \\theta, n)$\")\n",
    "    axes[0].grid(True, linestyle='--', alpha=0.5)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].text(0.95, 0.95, f\"∫ dθ ≈ {area_theta:.3f}\", transform=axes[0].transAxes,\n",
    "                 ha='right', va='top', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'))\n",
    "\n",
    "    # Right: likelihood as function of data (k)\n",
    "    axes[1].bar(k_vals, likelihood_data, color='black', alpha=0.8)\n",
    "    axes[1].set_title(rf\"$k \\mapsto p(k \\mid \\theta={theta_fixed:.2f}, n={n})$\")\n",
    "    axes[1].set_xlabel(\"k (number of successes)\")\n",
    "    axes[1].set_ylabel(\"Probability\")\n",
    "    axes[1].grid(True, linestyle='--', alpha=0.5)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].text(0.95, 0.95, f\"∑ = {area_data:.3f}\", transform=axes[1].transAxes,\n",
    "                 ha='right', va='top', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Sliders ---\n",
    "n_slider = widgets.IntSlider(min=1, max=50, value=10, step=1, description=\"n (trials)\")\n",
    "k_slider = widgets.IntSlider(min=0, max=50, value=5, step=1, description=\"k (successes)\")\n",
    "theta_slider = widgets.FloatSlider(min=0.01, max=0.99, value=0.5, step=0.01, description=\"θ (fixed)\")\n",
    "\n",
    "# --- Enforce k ≤ n ---\n",
    "def update_k_max(*args):\n",
    "    k_slider.max = n_slider.value\n",
    "    if k_slider.value > k_slider.max:\n",
    "        k_slider.value = k_slider.max\n",
    "\n",
    "n_slider.observe(update_k_max, names='value')\n",
    "update_k_max()\n",
    "\n",
    "# --- Layout ---\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"Left: Likelihood as a function of θ\"),\n",
    "    widgets.HTML(\"Right: Likelihood as a function of data\"),\n",
    "    widgets.HBox([n_slider, k_slider, theta_slider])\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(plot_likelihood_views, {\n",
    "    'n': n_slider,\n",
    "    'k': k_slider,\n",
    "    'theta_fixed': theta_slider\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9267205e",
   "metadata": {},
   "source": [
    "**Bayesian credibility intervals.** A Bayesian credibility interval is a range of values that contains the true value of the parameter with a certain probability. \n",
    "It is similar to a confidence interval in frequentist statistics, but it is based on the posterior distribution of the parameter rather than the sampling distribution. Its interpretation is actually what most people think of when they hear the term \"confidence interval\". Its what they are looking for when they ask for a confidence interval. You can construct a Bayesian credibility interval by picking the probability you want to contain the true value of the parameter, and then finding the range of values that contains that probability. \n",
    "\n",
    "**Exercise.** Click on the code cell below and press play to run. \n",
    "\n",
    "- Play with the sliders to see how the credibility interval changes as you change the probability:\n",
    "\n",
    "- The most typical is the 95% credibility interval $BCI_{95}$, which contains the value of the parameter with 95% probability. \n",
    "\n",
    "- The 50% credibility interval $BCI_{50}$ would contains the value of the parameter with 50% probability. And so on.\n",
    "\n",
    "- How is this different from the frequentist confidence interval? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe9531a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e777cb8f4f4221a83198dd1c732c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='μ', max=5.0, min=-5.0), FloatSlider(value=1.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_gaussian_bci(mu=0.0, sigma=1.0, bci=95.0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_gaussian_bci(mu=0.0, sigma=1.0, bci=95.0):\n",
    "    # Fixed x and y ranges\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = norm.pdf(x, mu, sigma)\n",
    "\n",
    "    # Compute credible interval bounds\n",
    "    alpha = 1 - bci / 100\n",
    "    lower_bound = norm.ppf(alpha / 2, mu, sigma)\n",
    "    upper_bound = norm.ppf(1 - alpha / 2, mu, sigma)\n",
    "    y_bound = norm.pdf([lower_bound, upper_bound], mu, sigma).min()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot curve and shading\n",
    "    ax.plot(x, y, label=f'N({mu:.2f}, {sigma:.2f}²)', color='purple')\n",
    "    ax.fill_between(x, y, color='plum', alpha=0.2)\n",
    "\n",
    "    # Highlight credible interval\n",
    "    mask = (x >= lower_bound) & (x <= upper_bound)\n",
    "    ax.fill_between(x[mask], y[mask], color='mediumvioletred', alpha=0.6,\n",
    "                    label=f\"{bci:.0f}% BCI: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "\n",
    "    # Dashed lines at interval bounds\n",
    "    ax.hlines(y=y_bound, xmin=lower_bound, xmax=upper_bound,\n",
    "              color='black', linestyle='--', linewidth=1)\n",
    "    ax.vlines([lower_bound, upper_bound], ymin=0, ymax=y_bound,\n",
    "              color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Fixed axes\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(0, 0.5)  # Fixed y-range to accommodate all practical normal densities\n",
    "\n",
    "    # Labels and legend\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "# Interactive sliders\n",
    "interact(\n",
    "    plot_gaussian_bci,\n",
    "    mu=widgets.FloatSlider(min=-5, max=5, step=0.1, value=0.0, description=\"μ\"),\n",
    "    sigma=widgets.FloatSlider(min=0.1, max=3.0, step=0.1, value=1.0, description=\"σ\"),\n",
    "    bci=widgets.FloatSlider(min=50, max=99, step=1, value=95, description=\"BCI (%)\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d902012-4c10-4f8b-9ead-4fe6147bc4fe",
   "metadata": {},
   "source": [
    "**Summarising the posterior.** With a proper normalised posterior, we may want to summarise it with credible intervals. These are the Bayesian equivalent of confidence intervals. But better, obvs. \n",
    "\n",
    "**Exercise.** Play with the slider that finds the Xth percentile credible interval.\n",
    "\n",
    "- What happens to the interval when you drop the percentile lower?\n",
    "\n",
    "- Plot the MAP - Maximum a posteriori. What is it? \n",
    "\n",
    "- How would you define it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "104a074f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27996a82d4b4ae8ab39435abe2fc99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<b style='color:blue'>Prior sliders</b>\"), HBox(children=(IntSlider(value=2, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96c5dd90ccf4d99bd9f2ab36ceeac7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "from scipy.special import betaln\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# --- Plotting function ---\n",
    "def plot_bayes(k_prior, n_prior, k_like, n_like, ci_width, show_shading, show_mean, show_map, show_curves):\n",
    "    # Prior parameters\n",
    "    a_prior = k_prior + 1\n",
    "    b_prior = (n_prior - k_prior) + 1\n",
    "    y_prior = beta.pdf(x, a_prior, b_prior)\n",
    "\n",
    "    # Likelihood as Beta PDF (for visualization only)\n",
    "    a_like = k_like + 1\n",
    "    b_like = (n_like - k_like) + 1\n",
    "    y_like = beta.pdf(x, a_like, b_like)\n",
    "\n",
    "    # Posterior parameters (from conjugate Beta-Binomial update)\n",
    "    a_post = a_prior + k_like\n",
    "    b_post = b_prior + n_like - k_like\n",
    "    y_post = beta.pdf(x, a_post, b_post)\n",
    "\n",
    "    # Compute proper marginal likelihood\n",
    "    log_ml = betaln(k_like + a_prior, n_like - k_like + b_prior) - betaln(a_prior, b_prior)\n",
    "    marginal_likelihood = np.exp(log_ml)\n",
    "\n",
    "    # Compute Bayesian credible interval (BCI)\n",
    "    lower = beta.ppf((1 - ci_width / 100) / 2, a_post, b_post)\n",
    "    upper = beta.ppf(1 - (1 - ci_width / 100) / 2, a_post, b_post)\n",
    "\n",
    "    # Posterior Mean and MAP\n",
    "    posterior_mean = a_post / (a_post + b_post)\n",
    "    map_index = np.argmax(y_post)\n",
    "    posterior_map = x[map_index]\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    if show_curves:\n",
    "        ax.plot(x, y_prior, label=\"Prior\", color=\"blue\")\n",
    "        ax.plot(x, y_like, label=\"Likelihood (visualized)\", color=\"red\")\n",
    "\n",
    "    ml_text = f\"Posterior (marginal likelihood ≈ {marginal_likelihood:.3f})\"\n",
    "    ax.plot(x, y_post, label=ml_text, color=\"green\")\n",
    "\n",
    "    if show_shading:\n",
    "        ax.fill_between(x, y_post, where=(x >= lower) & (x <= upper), color='gray', alpha=0.3, label=f\"{ci_width}% BCI\")\n",
    "    else:\n",
    "        ax.axvline(lower, color='gray', linestyle='--', label=f\"{ci_width}% BCI\")\n",
    "        ax.axvline(upper, color='gray', linestyle='--')\n",
    "\n",
    "    if show_mean:\n",
    "        ax.axvline(posterior_mean, color='black', linestyle=':', label=\"Posterior Mean\")\n",
    "\n",
    "    if show_map:\n",
    "        ax.axvline(posterior_map, color='purple', linestyle='-.', label=\"Posterior MAP\")\n",
    "\n",
    "    ax.set_title(\"Bayesian Updating with BCI, Mean, and MAP\")\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Sliders\n",
    "k_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=1, description=\"k_prior\", style={'description_width': 'initial'})\n",
    "n_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=2, description=\"n_prior\", style={'description_width': 'initial'})\n",
    "\n",
    "k_like_slider = widgets.IntSlider(min=0, max=100, step=1, value=1, description=\"k_like\", style={'description_width': 'initial'})\n",
    "n_like_slider = widgets.IntSlider(min=0, max=100, step=1, value=2, description=\"n_like\", style={'description_width': 'initial'})\n",
    "\n",
    "ci_slider = widgets.IntSlider(min=50, max=99, step=1, value=95, description=\"BCI (%)\", style={'description_width': 'initial'})\n",
    "\n",
    "# Toggles (defaults set to False)\n",
    "shading_toggle = widgets.Checkbox(value=False, description='Shade BCI')\n",
    "mean_toggle = widgets.Checkbox(value=False, description='Show Posterior Mean')\n",
    "map_toggle = widgets.Checkbox(value=False, description='Show MAP')\n",
    "curves_toggle = widgets.Checkbox(value=False, description='Show Prior & Likelihood')\n",
    "\n",
    "# Reusable fix for k ≤ n\n",
    "def enforce_k_leq_n(k_slider, n_slider):\n",
    "    k_slider.max = n_slider.value\n",
    "    if k_slider.value > k_slider.max:\n",
    "        k_slider.value = k_slider.max\n",
    "\n",
    "# Observers\n",
    "def update_k_max_prior(*args):\n",
    "    enforce_k_leq_n(k_prior_slider, n_prior_slider)\n",
    "\n",
    "def update_k_max_like(*args):\n",
    "    enforce_k_leq_n(k_like_slider, n_like_slider)\n",
    "\n",
    "n_prior_slider.observe(update_k_max_prior, names='value')\n",
    "n_like_slider.observe(update_k_max_like, names='value')\n",
    "\n",
    "# Initial sync\n",
    "update_k_max_prior()\n",
    "update_k_max_like()\n",
    "\n",
    "# Layout\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<b style='color:blue'>Prior sliders</b>\"),\n",
    "    widgets.HBox([n_prior_slider, k_prior_slider]),\n",
    "    widgets.HTML(\"<b style='color:red'>Likelihood sliders</b>\"),\n",
    "    widgets.HBox([n_like_slider, k_like_slider]),\n",
    "    widgets.HTML(\"<b>Posterior Display Options</b>\"),\n",
    "    ci_slider,\n",
    "    shading_toggle,\n",
    "    mean_toggle,\n",
    "    map_toggle,\n",
    "    curves_toggle\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(plot_bayes, {\n",
    "    'k_prior': k_prior_slider,\n",
    "    'n_prior': n_prior_slider,\n",
    "    'k_like': k_like_slider,\n",
    "    'n_like': n_like_slider,\n",
    "    'ci_width': ci_slider,\n",
    "    'show_shading': shading_toggle,\n",
    "    'show_mean': mean_toggle,\n",
    "    'show_map': map_toggle,\n",
    "    'show_curves': curves_toggle\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf4e02f-4a3d-4275-8668-56707da36214",
   "metadata": {},
   "source": [
    "**Compute the posterior by updating the beta.** The beta distribution allows for a very simple way to compute the posterior just by adding n and k to the inputs to the Beta function. This is called computing the posterior analytically, and it is a special case of the *conjugate* prior. This simplicity is, alas, not always possible.\n",
    "\n",
    "**Advanced explanation for why this works.**\n",
    "You can skip this for the moment, its ok to assume that the equation works out this way for this specific case.  \n",
    "- Beta is the conjugate prior for the binomial likelihood\n",
    "- Prior: $p(\\theta) \\propto \\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}$\n",
    "- Likelihood: $p(D \\mid \\theta) \\propto \\theta^k (1 - \\theta)^{n - k}$\n",
    "- Multiply prior and likelihood: exponents add\n",
    "- Posterior: $p(\\theta \\mid D) \\sim \\text{Beta}(\\alpha + k,\\ \\beta + n - k)$\n",
    "\n",
    "**Exercise.** Click on the code cell below and press play to run.\n",
    "\n",
    "- Play with the sliders to see how the posterior changes as you change the number of successes and failures.\n",
    "\n",
    "- Notice that the posterior is another beta distribution, with parameters $n + k$ and $m + n - k$.\n",
    "\n",
    "- This makes it easy to compute the posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d38c24d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4e8ddc266547249a77def0189dab17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=56, description='n (trials)', min=1, style=SliderStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f3acce268a40f88c525d9f1ab524fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "def plot_special_case(n, k):\n",
    "    if k > n:\n",
    "        print(\"Error: k must be ≤ n.\")\n",
    "        return\n",
    "\n",
    "    # Prior: Beta(1, 1)\n",
    "    a_prior, b_prior = 1, 1\n",
    "    y_prior = beta.pdf(x, a_prior, b_prior)\n",
    "\n",
    "    # Posterior: Beta(1 + k, 1 + n - k)\n",
    "    a_post = 1 + k\n",
    "    b_post = 1 + (n - k)\n",
    "    y_post = beta.pdf(x, a_post, b_post)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot prior\n",
    "    ax.plot(x, y_prior, label=\"Prior: Beta(1, 1)\", color=\"blue\")\n",
    "    ax.fill_between(x, y_prior, color=\"skyblue\", alpha=0.4)\n",
    "\n",
    "    # Plot posterior\n",
    "    full_label = f\"Posterior: Beta(1 + k, 1 + (n − k)) = Beta({a_post}, {b_post})\"\n",
    "    ax.plot(x, y_post, label=full_label, color=\"green\")\n",
    "    ax.fill_between(x, y_post, color=\"lightgreen\", alpha=0.4)\n",
    "\n",
    "    # Adjust layout and axis\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.set_ylim(0, max(np.max(y_post), np.max(y_prior)) * 1.2)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Move legend outside to the right\n",
    "    fig.subplots_adjust(right=0.75)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Sliders\n",
    "n_slider = widgets.IntSlider(min=1, max=100, step=1, value=56, description=\"n (trials)\", style={'description_width': 'initial'})\n",
    "k_slider = widgets.IntSlider(min=0, max=56, step=1, value=43, description=\"k (correct)\", style={'description_width': 'initial'})\n",
    "\n",
    "# Auto-adjust k bounds\n",
    "def update_k_slider(*args):\n",
    "    k_slider.max = n_slider.value\n",
    "    if k_slider.value > k_slider.max:\n",
    "        k_slider.value = k_slider.max\n",
    "\n",
    "n_slider.observe(update_k_slider, names='value')\n",
    "update_k_slider()  # Initial sync\n",
    "\n",
    "# Layout\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([n_slider, k_slider])\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(plot_special_case, {'n': n_slider, 'k': k_slider})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437e64",
   "metadata": {},
   "source": [
    "**Sequential vs aggregated updating.**  Bayesian updating can be done in two ways. You can update your beliefs step by step as new data arrives (sequential updating), or you can combine all the data and update in one go (aggregated updating). With conjugate priors like the Beta distribution, both methods give the same result. Let’s see how this works in a simple example.\n",
    "\n",
    "We’ll start with a uniform prior: Beta(1, 1). You then observe two datasets:\n",
    "- First: 9 correct out of 10 (k = 9, n = 10)\n",
    "- Second: 3 correct out of 5 (k = 3, n = 5)\n",
    "\n",
    "There are two ways to compute the posterior:\n",
    "\n",
    "**Sequential updating**  \n",
    "   - Start with Beta(1, 1)  \n",
    "   - Update with the first dataset → Beta(10, 2)  \n",
    "   - Use that as the new prior, update with the second dataset → Beta(13, 4)\n",
    "\n",
    "**Aggregated updating**  \n",
    "   - Combine the data: k = 12, n = 15  \n",
    "   - Start with Beta(1, 1), update once → Beta(13, 4)\n",
    "\n",
    "\n",
    "**Exercise.** Use the sliders to try both methods and confirm they give the same result.\n",
    "\n",
    "- First, try the sequential approach:  \n",
    "  Set prior to Beta(1, 1), then enter k = 9, n = 10.  \n",
    "  Use the resulting posterior as the new prior, then enter k = 3, n = 5.\n",
    "\n",
    "- Then try the aggregated approach:  \n",
    "  Set prior to Beta(1, 1), then enter k = 12, n = 15.\n",
    "\n",
    "- Do you get the same posterior in both cases?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "385e7e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e2ff2163b249d4ab11fb38e42ccc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='🔵 <b>Prior</b>'), HBox(children=(IntSlider(value=20, description='n_prior'), IntSli…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db12d08f6a441d2bbeea78efd77700e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "def plot_update(n_prior, k_prior, n_new, k_new):\n",
    "    if k_prior > n_prior or k_new > n_new:\n",
    "        print(\"Error: k must be ≤ n.\")\n",
    "        return\n",
    "\n",
    "    # Prior = Beta(1 + k_prior, 1 + n_prior - k_prior)\n",
    "    a_prior = 1 + k_prior\n",
    "    b_prior = 1 + (n_prior - k_prior)\n",
    "    y_prior = beta.pdf(x, a_prior, b_prior)\n",
    "\n",
    "    # Posterior = Beta(1 + total_k, 1 + total_n - total_k)\n",
    "    total_k = k_prior + k_new\n",
    "    total_n = n_prior + n_new\n",
    "    a_post = 1 + total_k\n",
    "    b_post = 1 + (total_n - total_k)\n",
    "    y_post = beta.pdf(x, a_post, b_post)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    ax.plot(x, y_prior, label=f\"Prior: Beta(1 + {k_prior}, 1 + {n_prior - k_prior})\", color=\"blue\")\n",
    "    ax.fill_between(x, y_prior, color=\"skyblue\", alpha=0.4)\n",
    "\n",
    "    ax.plot(x, y_post, label=f\"Posterior: Beta(1 + {total_k}, 1 + {total_n - total_k})\", color=\"green\")\n",
    "    ax.fill_between(x, y_post, color=\"lightgreen\", alpha=0.4)\n",
    "\n",
    "    ax.set_xlabel(\"θ\")\n",
    "    ax.set_ylabel(\"Probability Density\")\n",
    "    ax.set_ylim(0, max(np.max(y_prior), np.max(y_post)) * 1.2)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Adjust spacing and move legend to right\n",
    "    fig.subplots_adjust(right=0.75)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Sliders\n",
    "n_prior_slider = widgets.IntSlider(min=0, max=100, step=1, value=20, description=\"n_prior\")\n",
    "k_prior_slider = widgets.IntSlider(min=0, max=20, step=1, value=10, description=\"k_prior\")\n",
    "\n",
    "n_new_slider = widgets.IntSlider(min=0, max=100, step=1, value=30, description=\"n_new\")\n",
    "k_new_slider = widgets.IntSlider(min=0, max=30, step=1, value=15, description=\"k_new\")\n",
    "\n",
    "# Auto-limit k sliders\n",
    "def update_k_prior_max(*args):\n",
    "    k_prior_slider.max = n_prior_slider.value\n",
    "    if k_prior_slider.value > k_prior_slider.max:\n",
    "        k_prior_slider.value = k_prior_slider.max\n",
    "\n",
    "def update_k_new_max(*args):\n",
    "    k_new_slider.max = n_new_slider.value\n",
    "    if k_new_slider.value > k_new_slider.max:\n",
    "        k_new_slider.value = k_new_slider.max\n",
    "\n",
    "n_prior_slider.observe(update_k_prior_max, names='value')\n",
    "n_new_slider.observe(update_k_new_max, names='value')\n",
    "\n",
    "# Initial sync\n",
    "update_k_prior_max()\n",
    "update_k_new_max()\n",
    "\n",
    "# Layout\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"🔵 <b>Prior</b>\"),\n",
    "    widgets.HBox([n_prior_slider, k_prior_slider]),\n",
    "    widgets.HTML(\"🟢 <b>New Data</b>\"),\n",
    "    widgets.HBox([n_new_slider, k_new_slider])\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(plot_update, {\n",
    "    'n_prior': n_prior_slider,\n",
    "    'k_prior': k_prior_slider,\n",
    "    'n_new': n_new_slider,\n",
    "    'k_new': k_new_slider\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678268e1",
   "metadata": {},
   "source": [
    "**MCMC.** To illustrate the power of MCMC, we demo with this interactive example. The demo runs JAGS via python. JAGS is a sampler that implements MCMC, though there are many samplers available. The model is set up via as specific language. It is everything inside \"model {...}\" in the code cell below. Here the model is a simple Bernoulli model, where we have a prior on theta, and we have a likelihood for observe k successes out of n trials.\n",
    "\n",
    "**Exercise.** Play with the data k and n and set the number of samples that sampler runs. Keep it low to begin with.\n",
    "\n",
    "-  Compare sampling to an analytical posterior for the same data. Click on the show analytic posteror button to overlay the analytical posterior. \n",
    "\n",
    "- How do you get the MCMC posterior to better match the analytical posterior? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29872727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3db650fafb34229946d1c36270e9ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=6, description='Successes (k):', max=20), IntSlider(value=10, description='Tria…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c10fefcda7d4e2fa8e34e1762b05309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from scipy.stats import beta\n",
    "\n",
    "# Ensure plots show inline\n",
    "%matplotlib inline\n",
    "\n",
    "def run_jags(k=6, n=10, samples=100, show_analytic=False):\n",
    "    if k > n:\n",
    "        print(\"k must be ≤ n\")\n",
    "        return\n",
    "\n",
    "    # Data setup\n",
    "    y = np.array([1]*k + [0]*(n-k))\n",
    "\n",
    "    # JAGS model string\n",
    "    model_code = \"\"\"\n",
    "    model {\n",
    "      for (i in 1:N) {\n",
    "        y[i] ~ dbern(theta)\n",
    "      }\n",
    "      theta ~ dbeta(1, 1)\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    data = {\"y\": y, \"N\": len(y)}\n",
    "    inits = [{\"theta\": 0.5}]\n",
    "\n",
    "    # Run JAGS model\n",
    "    model = pyjags.Model(code=model_code, data=data, init=inits, chains=1)\n",
    "    model.update(100)  # Burn-in\n",
    "    result = model.sample(samples, vars=[\"theta\"])\n",
    "    theta_samples = result[\"theta\"].reshape(-1)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # --- Trace plot ---\n",
    "    axs[0].plot(theta_samples)\n",
    "    axs[0].set_title(\"Trace of θ (Chain 1)\")\n",
    "    axs[0].set_xlabel(\"Sample\")\n",
    "    axs[0].set_ylabel(\"θ\")\n",
    "    axs[0].set_ylim(0, 1)\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # --- Horizontal histogram ---\n",
    "    axs[1].hist(theta_samples, bins=30, density=True, alpha=0.6, color='gray', orientation='horizontal', label='MCMC posterior')\n",
    "    axs[1].set_title(\"Posterior of θ\")\n",
    "    axs[1].set_xlabel(\"Density\")\n",
    "    axs[1].set_ylabel(\"θ\")\n",
    "    axs[1].set_ylim(0, 1)\n",
    "\n",
    "    # --- Line-over-histogram plot ---\n",
    "    axs[2].hist(theta_samples, bins=30, density=True, alpha=0.5, color='steelblue', label='MCMC posterior')\n",
    "\n",
    "    # Optional analytical overlay\n",
    "    if show_analytic:\n",
    "        alpha_post = 1 + k\n",
    "        beta_post = 1 + (n - k)\n",
    "        theta_vals = np.linspace(0, 1, 200)\n",
    "        analytical_posterior = beta.pdf(theta_vals, alpha_post, beta_post)\n",
    "        axs[1].plot(analytical_posterior, theta_vals, 'r-', lw=2, label='Analytical posterior')\n",
    "        axs[1].legend()\n",
    "\n",
    "        axs[2].plot(theta_vals, analytical_posterior, 'r-', lw=2, label='Analytical posterior')\n",
    "        axs[2].legend()\n",
    "\n",
    "    axs[2].set_title(f\"Posterior of θ (k={k}, n={n})\")\n",
    "    axs[2].set_xlabel(\"θ\")\n",
    "    axs[2].set_ylabel(\"Density\")\n",
    "    axs[2].set_xlim(0, 1)\n",
    "    axs[2].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Widgets\n",
    "k_slider = widgets.IntSlider(value=6, min=0, max=20, step=1, description='Successes (k):')\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=20, step=1, description='Trials (n):')\n",
    "sample_slider = widgets.IntSlider(value=100, min=100, max=10000, step=100, description='Samples:')\n",
    "\n",
    "analytic_toggle = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Show analytic posterior',\n",
    "    tooltip='Toggle analytical posterior overlay',\n",
    "    icon='line-chart'\n",
    ")\n",
    "\n",
    "# Link function to widgets\n",
    "ui = widgets.VBox([k_slider, n_slider, sample_slider, analytic_toggle])\n",
    "out = widgets.interactive_output(run_jags, {\n",
    "    'k': k_slider,\n",
    "    'n': n_slider,\n",
    "    'samples': sample_slider,\n",
    "    'show_analytic': analytic_toggle\n",
    "})\n",
    "\n",
    "# Display\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c35c64",
   "metadata": {},
   "source": [
    "**Binomial distribution.**  Here we can see the binomial distribution: the probability of getting `k` successes in `n` independent trials, where each trial has a success probability of `θ`. Use the sliders below to adjust `n` and `θ`, and watch how the shape of the distribution changes. The equation updates to show how the probability is computed based on the current values.\n",
    "\n",
    "**Exercise.** Play with the sliders and answer the following:\n",
    "\n",
    "- What happens when θ is close to 0 or close to 1?\n",
    "\n",
    "- How does the distribution change as you increase n?\n",
    "\n",
    "- When is the distribution symmetric, and when is it skewed?\n",
    "\n",
    "- What value of `k` seems most likely, and how does that relate to `θ × n`?\n",
    "\n",
    "Try predicting the outcome before you move the sliders — then use the plot to check your intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3700289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50d4e116abf47bf910754ce255921bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='θ', max=0.99, min=0.01, step=0.01), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(theta, n)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n",
    "# Define sliders\n",
    "theta_slider = widgets.FloatSlider(value=0.5, min=0.01, max=0.99, step=0.01, description='θ')\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=100, step=1, description='n')\n",
    "\n",
    "# Define function to update the plot and LaTeX\n",
    "def update_plot(theta, n):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Binomial support and PMF\n",
    "    k = np.arange(0, n+1)\n",
    "    pmf = [np.math.comb(n, ki) * theta**ki * (1 - theta)**(n - ki) for ki in k]\n",
    "    \n",
    "    # Display formula with current values\n",
    "    display(Math(f\"P(k\\\\mid\\\\theta={theta:.2f},\\\\ n={n}) = \\\\binom{{n}}{{k}} \\\\theta^k (1 - \\\\theta)^{{n - k}}\"))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(k, pmf, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Binomial Distribution: n={n}, θ={theta:.2f}')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('P(k | θ, n)')\n",
    "    plt.grid(True, axis='y', linestyle=':')\n",
    "    plt.show()\n",
    "\n",
    "# Interactive output\n",
    "widgets.interact(update_plot, theta=theta_slider, n=n_slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b385ebf",
   "metadata": {},
   "source": [
    "**MCMC convergaence checks.** This is the same model as the one we used in the MCMC demo. The model is a simple Bernoulli model, where we have a prior on theta, and we have a likelihood for observe k successes out of n trials. The model is set up via as specific language. It is everything inside \"model {...}\" in the code cell below. We run it here so that we can look at the model outputs and see if they look ok. \n",
    "\n",
    "**Exercise.** Play with the sliders, run the model, and check the convergence diagnostics.\n",
    "- Do the chains look like they have converged?\n",
    "- Do they look like \"hairy caterpillars\"?\n",
    "- Is the R-hat statistic close to 1?\n",
    "- Is the Pope Catholic?\n",
    "- Just checking you are awake. x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "795161db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f01c0fbaec64ea0abb896d1d07e369e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=6, description='Successes (k)', max=20), IntSlider(value=10, description='Trial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Math, Latex, clear_output\n",
    "\n",
    "# Define the JAGS model as a string\n",
    "model_code = \"\"\"\n",
    "model {\n",
    "  theta ~ dbeta(1,1)\n",
    "  k ~ dbin(theta, n)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# R-hat computation\n",
    "def compute_rhat(chains):\n",
    "    m = len(chains)\n",
    "    n = len(chains[0])\n",
    "    \n",
    "    chain_means = np.array([np.mean(c) for c in chains])\n",
    "    chain_vars = np.array([np.var(c, ddof=1) for c in chains])\n",
    "\n",
    "    B = n * np.var(chain_means, ddof=1)\n",
    "    W = np.mean(chain_vars)\n",
    "    var_hat = ((n - 1) / n) * W + (1 / n) * B\n",
    "    Rhat = np.sqrt(var_hat / W)\n",
    "\n",
    "    return Rhat, B, W, var_hat, chain_means, chain_vars\n",
    "\n",
    "# Main model runner\n",
    "def run_model(k, n, samples):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Running with k={k}, n={n}, samples={samples} per chain\\n\")\n",
    "    \n",
    "    chains = []\n",
    "    for i in range(4):\n",
    "        data = {'k': k, 'n': n}\n",
    "        model = pyjags.Model(code=model_code, data=data, chains=1, adapt=500)\n",
    "        model.update(1000)\n",
    "        result = model.sample(samples, vars=['theta'])\n",
    "        theta_samples = result['theta'].reshape(-1)\n",
    "        chains.append(theta_samples)\n",
    "        plt.plot(theta_samples, label=f\"Chain {i+1}\")\n",
    "\n",
    "    plt.title(\"Trace plots of θ for 4 chains\")\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"θ\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Compute R-hat\n",
    "    Rhat, B, W, var_hat, chain_means, chain_vars = compute_rhat(chains)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Per-chain statistics:\")\n",
    "    for i, (mean, var) in enumerate(zip(chain_means, chain_vars)):\n",
    "        print(f\"  Chain {i+1}: mean = {mean:.4f}, var = {var:.5f}\")\n",
    "\n",
    "    print(f\"\\nBetween-chain variance (B): {B:.5f}\")\n",
    "    print(f\"Within-chain variance (W): {W:.5f}\")\n",
    "    print(f\"Estimated variance (var_hat): {var_hat:.5f}\")\n",
    "    print(f\"R-hat: {Rhat:.4f}\\n\")\n",
    "\n",
    "    display(Math(r\"\"\"\n",
    "    \\hat{R} = \\sqrt{ \\frac{ \\left( \\frac{n-1}{n} \\right) W + \\left( \\frac{1}{n} \\right) B }{W} }\n",
    "    \"\"\"))\n",
    "\n",
    "# Interactive widgets\n",
    "k_slider = widgets.IntSlider(value=6, min=0, max=20, description='Successes (k)')\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=20, description='Trials (n)')\n",
    "samples_slider = widgets.IntSlider(value=1000, min=100, max=5000, step=100, description='Samples')\n",
    "\n",
    "run_button = widgets.Button(description=\"Run model\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_click(b):\n",
    "    with output:\n",
    "        run_model(k_slider.value, n_slider.value, samples_slider.value)\n",
    "\n",
    "run_button.on_click(on_click)\n",
    "display(widgets.VBox([k_slider, n_slider, samples_slider, run_button, output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270a4e61",
   "metadata": {},
   "source": [
    "**Inferring the difference between two rates.** In this model, we observe two processes — for example, two groups or experimental conditions — each with a number of successes out of a number of trials. We assume the underlying success rates are governed by parameters $\\theta_1$ and $\\theta_2$, with uniform Beta priors:\n",
    "\n",
    "$\\theta_1 \\sim \\text{Beta}(1, 1)$, $\\theta_2 \\sim \\text{Beta}(1, 1)$  \n",
    "$k_1 \\sim \\text{Binomial}(n_1, \\theta_1)$, $k_2 \\sim \\text{Binomial}(n_2, \\theta_2)$\n",
    "\n",
    "Our quantity of interest is the difference in success rates:\n",
    "\n",
    "$\\delta = \\theta_1 - \\theta_2$\n",
    "\n",
    "This tells us how much more (or less) likely success is in one group than the other.\n",
    "\n",
    "Use the sliders to adjust $k_1$, $n_1$, $k_2$, and $n_2$. The trace plots show MCMC samples of $\\delta$, and the histogram shows its posterior distribution. You can also overlay an approximate analytical solution for comparison.\n",
    "\n",
    "**Exercise.** Explore the model using the sliders and answer the following:\n",
    "\n",
    "- When does the posterior of $\\delta$ look symmetric? When is it skewed?\n",
    "- How does increasing $n_1$ or $n_2$ affect the width of the posterior?\n",
    "- What happens when $k_1 = k_2$ but $n_1 \\ne n_2$?\n",
    "- Use the checkbox to compare the MCMC posterior with the analytical approximation. How close are they?\n",
    "\n",
    "Try predicting what the posterior will look like before adjusting the sliders — then use the plot to check your intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "114766e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5353ccd924ae43ab97a58f076d6f220e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=15, description='k₁', max=30), IntSlider(value=20, description='n₁', max=40, mi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Math, clear_output\n",
    "\n",
    "# JAGS model code\n",
    "model_code = \"\"\"\n",
    "model {\n",
    "  k1 ~ dbin(theta1, n1)\n",
    "  k2 ~ dbin(theta2, n2)\n",
    "  theta1 ~ dbeta(1,1)\n",
    "  theta2 ~ dbeta(1,1)\n",
    "  delta <- theta1 - theta2\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# R-hat calculation\n",
    "def compute_rhat(chains):\n",
    "    m = len(chains)\n",
    "    n = len(chains[0])\n",
    "    chain_means = np.array([np.mean(c) for c in chains])\n",
    "    chain_vars = np.array([np.var(c, ddof=1) for c in chains])\n",
    "    B = n * np.var(chain_means, ddof=1)\n",
    "    W = np.mean(chain_vars)\n",
    "    var_hat = ((n - 1) / n) * W + (1 / n) * B\n",
    "    Rhat = np.sqrt(var_hat / W)\n",
    "    return Rhat, B, W, var_hat, chain_means, chain_vars\n",
    "\n",
    "# Main model function\n",
    "def run_model(k1, n1, k2, n2, samples):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Running with (k1={k1}, n1={n1}), (k2={k2}, n2={n2}), samples={samples} per chain\")\n",
    "\n",
    "    data = {\"k1\": k1, \"n1\": n1, \"k2\": k2, \"n2\": n2}\n",
    "    chains = []\n",
    "\n",
    "    for i in range(4):\n",
    "        model = pyjags.Model(code=model_code, data=data, chains=1, adapt=500)\n",
    "        model.update(1000)\n",
    "        result = model.sample(samples, vars=[\"delta\"])\n",
    "        delta_chain = result[\"delta\"].reshape(-1)\n",
    "        chains.append(delta_chain)\n",
    "\n",
    "    # Plot traces and histogram\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    grid = plt.GridSpec(1, 2, width_ratios=[3, 1], wspace=0.3)\n",
    "\n",
    "    ax_traces = fig.add_subplot(grid[0])\n",
    "    for i in range(4):\n",
    "        ax_traces.plot(chains[i], label=f\"Chain {i+1}\")\n",
    "    ax_traces.set_title(\"Trace plots of δ (4 chains)\")\n",
    "    ax_traces.set_xlabel(\"Sample\")\n",
    "    ax_traces.set_ylabel(\"δ = θ₁ − θ₂\")\n",
    "    ax_traces.axhline(0, color='black', linestyle='--', lw=1)\n",
    "    ax_traces.legend()\n",
    "    ax_traces.grid(True)\n",
    "\n",
    "    ax_hist = fig.add_subplot(grid[1])\n",
    "    all_samples = np.concatenate(chains)\n",
    "    ax_hist.hist(all_samples, bins=30, orientation='horizontal', density=True, color='gray', alpha=0.6, label='Posterior')\n",
    "    ax_hist.set_title(\"Posterior of δ\")\n",
    "    ax_hist.set_xlabel(\"Density\")\n",
    "    ax_hist.set_ylabel(\"δ\")\n",
    "    ax_hist.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # R-hat diagnostics\n",
    "    Rhat, B, W, var_hat, chain_means, chain_vars = compute_rhat(chains)\n",
    "\n",
    "    print(\"Per-chain statistics:\")\n",
    "    for i, (mean, var) in enumerate(zip(chain_means, chain_vars)):\n",
    "        print(f\"  Chain {i+1}: mean = {mean:.4f}, var = {var:.5f}\")\n",
    "\n",
    "    print(f\"\\nBetween-chain variance (B): {B:.5f}\")\n",
    "    print(f\"Within-chain variance (W): {W:.5f}\")\n",
    "    print(f\"Estimated variance (var_hat): {var_hat:.5f}\")\n",
    "    print(f\"R-hat: {Rhat:.4f}\\n\")\n",
    "\n",
    "    display(Math(r\"\"\"\n",
    "    \\hat{R} = \\sqrt{ \\frac{ \\left( \\frac{n-1}{n} \\right) W + \\left( \\frac{1}{n} \\right) B }{W} }\n",
    "    \"\"\"))\n",
    "\n",
    "# Sliders\n",
    "k1_slider = widgets.IntSlider(value=15, min=0, max=30, description='k₁')\n",
    "n1_slider = widgets.IntSlider(value=20, min=1, max=40, description='n₁')\n",
    "k2_slider = widgets.IntSlider(value=10, min=0, max=30, description='k₂')\n",
    "n2_slider = widgets.IntSlider(value=20, min=1, max=40, description='n₂')\n",
    "samples_slider = widgets.IntSlider(value=1000, min=100, max=5000, step=100, description='Samples')\n",
    "\n",
    "run_button = widgets.Button(description=\"Run model\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_click(b):\n",
    "    with output:\n",
    "        run_model(\n",
    "            k1_slider.value, n1_slider.value,\n",
    "            k2_slider.value, n2_slider.value,\n",
    "            samples_slider.value\n",
    "        )\n",
    "\n",
    "run_button.on_click(on_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    k1_slider, n1_slider, k2_slider, n2_slider,\n",
    "    samples_slider, run_button, output\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a02d73",
   "metadata": {},
   "source": [
    "**Inferring a common rate.**  In this model, we assume a single underlying common rate $\\theta$ governs two independent processes: one produces $k_1$ successes out of $n_1$ trials, and the other produces $k_2$ out of $n_2$. We use a uniform Beta prior for $\\theta$ and observe both outcomes to update our belief about its value. This model is useful when we believe the same latent process drives two different datasets. Later we can turn this into a question whether the data is modelled better by a single rate (e.g. ability), or two different rates.\n",
    "\n",
    "**Exercise.**  Use the sliders to explore how the posterior of $\\theta$ responds to different inputs:\n",
    "\n",
    "- What happens when $k_1$ and $k_2$ are both small or both large?\n",
    "- How does increasing $n_1$ or $n_2$ while keeping $k_1$ and $k_2$ fixed affect the posterior? \n",
    "- What if one process supports a high $\\theta$ and the other supports a low one?\n",
    "- How can you change the sliders such that process 1 or 2 has more influence over the posterior? Why does this work?\n",
    "\n",
    "Try guessing what the posterior will look like before you run the code — then use the plots to test your expectations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf64bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677235620b45427d9c8d815febc895d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=10, description='n₁', max=30, min=1), IntSlider(value=6, description='k₁', max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# JAGS model code\n",
    "model_code = \"\"\"\n",
    "model {\n",
    "  k1 ~ dbin(theta, n1)\n",
    "  k2 ~ dbin(theta, n2)\n",
    "  theta ~ dbeta(1,1)\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Function to run the model\n",
    "def run_model(k1, k2, n1, n2, samples):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Running with k1={k1}, n1={n1}, k2={k2}, n2={n2}, samples={samples}\")\n",
    "    \n",
    "    data = {\"k1\": k1, \"k2\": k2, \"n1\": n1, \"n2\": n2}\n",
    "    model = pyjags.Model(code=model_code, data=data, chains=1, adapt=500)\n",
    "    model.update(1000)\n",
    "    samples_dict = model.sample(samples, vars=[\"theta\"])\n",
    "    theta_chain = samples_dict[\"theta\"].reshape(-1)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw={\"width_ratios\": [3, 1]})\n",
    "    \n",
    "    # Trace plot\n",
    "    axs[0].plot(theta_chain)\n",
    "    axs[0].set_title(\"Trace plot of θ\")\n",
    "    axs[0].set_xlabel(\"Iteration\")\n",
    "    axs[0].set_ylabel(\"θ\")\n",
    "    axs[0].set_ylim(0, 1)\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Horizontal histogram\n",
    "    axs[1].hist(theta_chain, bins=30, orientation=\"horizontal\", density=True, color=\"gray\", edgecolor=\"black\")\n",
    "    axs[1].set_title(\"Posterior of θ\")\n",
    "    axs[1].set_xlabel(\"Density\")\n",
    "    axs[1].set_ylabel(\"θ\")\n",
    "    axs[1].set_ylim(0, 1)\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sliders\n",
    "n1_slider = widgets.IntSlider(value=10, min=1, max=30, description=\"n₁\")\n",
    "k1_slider = widgets.IntSlider(value=6, min=0, max=n1_slider.value, description=\"k₁\")\n",
    "\n",
    "n2_slider = widgets.IntSlider(value=10, min=1, max=30, description=\"n₂\")\n",
    "k2_slider = widgets.IntSlider(value=4, min=0, max=n2_slider.value, description=\"k₂\")\n",
    "\n",
    "samples_slider = widgets.IntSlider(value=1000, min=100, max=5000, step=100, description=\"Samples\")\n",
    "run_button = widgets.Button(description=\"Run model\")\n",
    "output = widgets.Output()\n",
    "\n",
    "# Link k1 max to n1\n",
    "def update_k1_max(*args):\n",
    "    k1_slider.max = n1_slider.value\n",
    "    if k1_slider.value > k1_slider.max:\n",
    "        k1_slider.value = k1_slider.max\n",
    "\n",
    "def update_k2_max(*args):\n",
    "    k2_slider.max = n2_slider.value\n",
    "    if k2_slider.value > k2_slider.max:\n",
    "        k2_slider.value = k2_slider.max\n",
    "\n",
    "n1_slider.observe(update_k1_max, names='value')\n",
    "n2_slider.observe(update_k2_max, names='value')\n",
    "\n",
    "# Run button action\n",
    "def on_click(b):\n",
    "    with output:\n",
    "        run_model(\n",
    "            k1_slider.value, k2_slider.value,\n",
    "            n1_slider.value, n2_slider.value,\n",
    "            samples_slider.value\n",
    "        )\n",
    "\n",
    "run_button.on_click(on_click)\n",
    "\n",
    "# Display everything\n",
    "display(widgets.VBox([\n",
    "    n1_slider, k1_slider,\n",
    "    n2_slider, k2_slider,\n",
    "    samples_slider,\n",
    "    run_button,\n",
    "    output\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161829a3",
   "metadata": {},
   "source": [
    "**Simulate guessers and tryers** This code allows you simulate the mix of guessers and tryers along with the ability of the tryers. The code is set up to simulate a test with 1000 trials, and you can adjust the number of guessers and tryers using the sliders. The ability of the tryers is also adjustable, and you can see how this affects the distribution of scores.\n",
    "\n",
    "**Exercise.** Play around with the sliders to see how the distribution of scores changes as you adjust the number of guessers and tryers, and the ability of the tryers. \n",
    "\n",
    "- what happens when the number of trials is low? \n",
    "\n",
    "- is it harder or easier to tell the difference between guessers and tryers?\n",
    "\n",
    "- what happens when you change the ability of the tryers? why? \n",
    "\n",
    "- how would this inform your experimental design?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ab659ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8f87a1da7f43d6811181ae13063d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>1. Simulate data:</b>'), IntSlider(value=5, description='Guessers', max=30), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Store for simulated data\n",
    "sim_data = {}\n",
    "sim_output = widgets.Output()\n",
    "\n",
    "# --- SIMULATION ---\n",
    "def simulate_data(num_guessers, num_tryers, phi, n):\n",
    "    sim_output.clear_output(wait=True)\n",
    "\n",
    "    # Clamp phi to valid Beta range\n",
    "    phi = np.clip(phi, 0.01, 0.99)\n",
    "\n",
    "    p = num_guessers + num_tryers\n",
    "    z_true = np.array([0]*num_guessers + [1]*num_tryers)\n",
    "    np.random.shuffle(z_true)\n",
    "    theta = np.where(z_true == 0, 0.5, np.random.beta(phi*10, (1-phi)*10, size=p))\n",
    "    k = np.random.binomial(n, theta)\n",
    "\n",
    "    sim_data.update({\"z_true\": z_true, \"k\": k, \"p\": p, \"n\": n, \"theta\": theta})\n",
    "\n",
    "    with sim_output:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.bar(np.arange(p), k, color='steelblue', edgecolor='black')\n",
    "        plt.title(f\"Simulated correct responses (n = {n})\")\n",
    "        plt.xlabel(\"Participant\")\n",
    "        plt.ylabel(\"Correct responses\")\n",
    "        plt.ylim(0, n)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"True z values (0 = guesser, 1 = tryer):\")\n",
    "        print(z_true)\n",
    "        print(\"Simulated theta values:\")\n",
    "        print(np.round(theta, 3))\n",
    "\n",
    "# --- WIDGETS ---\n",
    "guess_slider = widgets.IntSlider(value=5, min=0, max=30, description=\"Guessers\")\n",
    "tryer_slider = widgets.IntSlider(value=15, min=0, max=30, description=\"Tryers\")\n",
    "phi_slider = widgets.FloatSlider(value=0.9, min=0.01, max=0.99, step=0.01, description=\"Mean Ability φ\")\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=100, step=1, description=\"Trials (n)\")\n",
    "\n",
    "simulate_button = widgets.Button(description=\"Simulate\")\n",
    "\n",
    "simulate_button.on_click(lambda b: simulate_data(\n",
    "    guess_slider.value, tryer_slider.value, phi_slider.value, n_slider.value\n",
    "))\n",
    "\n",
    "# --- DISPLAY UI ---\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<b>1. Simulate data:</b>\"),\n",
    "    guess_slider,\n",
    "    tryer_slider,\n",
    "    phi_slider,\n",
    "    n_slider,\n",
    "    simulate_button,\n",
    "    sim_output\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5a02f",
   "metadata": {},
   "source": [
    "**Simulate a single subject then infer guesser or tryer.** This code allows you to simulate a single subject's performance on the test, and then infer whether they are a guesser or a tryer based on their score. The model assumes that the subject is either a guesser or a tryer, and uses the observed score to update the prior beliefs about their type. \n",
    "\n",
    "**Exercise.** \n",
    "- Play around with the sliders to see how the posterior probabilities of being a guesser or a tryer change as you make the guesser and the tryer closer, or further apart. \n",
    "- Play around with the amount of data you have from the experiment. Does this make it easier to tell the difference between guessers and tryers when they are closer together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c73563e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67eb73a396f44a968629b9fa22aec7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>1. Simulate a single subject:</b>'), Dropdown(description='True z:', options=(('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Output areas\n",
    "sim_output = widgets.Output()\n",
    "infer_output = widgets.Output()\n",
    "\n",
    "# Store simulated data\n",
    "sim_data = {}\n",
    "\n",
    "# --- SIMULATE ONE SUBJECT ---\n",
    "def simulate_one_subject(z_true, phi, n):\n",
    "    sim_output.clear_output(wait=True)\n",
    "    \n",
    "    theta = 0.5 if z_true == 0 else np.random.beta(phi * 10, (1 - phi) * 10)\n",
    "    k = np.random.binomial(n, theta)\n",
    "    \n",
    "    sim_data.update({\"z_true\": z_true, \"k\": k, \"n\": n})\n",
    "\n",
    "    with sim_output:\n",
    "        print(f\"True z: {z_true} ({'guesser' if z_true == 0 else 'tryer'})\")\n",
    "        print(f\"Generated θ: {theta:.3f}\")\n",
    "        print(f\"Observed correct responses: k = {k} out of n = {n}\")\n",
    "\n",
    "# --- INFER ONE SUBJECT ---\n",
    "def infer_one_subject(samples):\n",
    "    infer_output.clear_output(wait=True)\n",
    "    \n",
    "    if not sim_data:\n",
    "        with infer_output:\n",
    "            print(\"⚠️ Please simulate data first.\")\n",
    "        return\n",
    "\n",
    "    k = sim_data[\"k\"]\n",
    "    n = sim_data[\"n\"]\n",
    "    z_true = sim_data[\"z_true\"]\n",
    "\n",
    "    model_code = \"\"\"\n",
    "    model {\n",
    "      z ~ dcat(pi[1:2])\n",
    "      pi[1] <- 0.5\n",
    "      pi[2] <- 0.5\n",
    "\n",
    "      theta[1] <- 0.5\n",
    "      theta[2] ~ dbeta(1, 1)\n",
    "\n",
    "      k ~ dbin(theta[z], n)\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    data = {\"k\": k, \"n\": n}\n",
    "\n",
    "    model = pyjags.Model(code=model_code, data=data, chains=1, adapt=500)\n",
    "    model.update(1000)\n",
    "    result = model.sample(samples, vars=[\"z\"])\n",
    "    z_chain = result[\"z\"].reshape(-1)\n",
    "\n",
    "    with infer_output:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        plt.hist(z_chain, bins=[0.5, 1.5, 2.5], rwidth=0.7, align='mid', color='gray')\n",
    "        plt.xticks([1, 2], ['z=1 (guesser)', 'z=2 (tryer)'])\n",
    "        plt.title(\"Posterior distribution of z\")\n",
    "        plt.xlabel(\"Model\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        p1 = np.mean(z_chain == 1)\n",
    "        p2 = np.mean(z_chain == 2)\n",
    "        print(f\"Posterior p(z = 1): {p1:.3f}\")\n",
    "        print(f\"Posterior p(z = 2): {p2:.3f}\")\n",
    "        print(f\"True z: {z_true} ({'guesser' if z_true == 0 else 'tryer'})\")\n",
    "\n",
    "# --- WIDGETS ---\n",
    "z_true_slider = widgets.Dropdown(options=[(\"Guesser (z=0)\", 0), (\"Tryer (z=1)\", 1)], value=0, description=\"True z:\")\n",
    "phi_slider = widgets.FloatSlider(value=0.9, min=0.5, max=1.0, step=0.01, description=\"Ability φ:\")\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=100, step=1, description=\"Trials (n):\")\n",
    "samples_slider = widgets.IntSlider(value=1000, min=500, max=5000, step=100, description=\"Samples:\")\n",
    "\n",
    "simulate_button = widgets.Button(description=\"Simulate 1 subject\")\n",
    "infer_button = widgets.Button(description=\"Run Inference\")\n",
    "\n",
    "simulate_button.on_click(lambda b: simulate_one_subject(\n",
    "    z_true_slider.value, phi_slider.value, n_slider.value\n",
    "))\n",
    "\n",
    "infer_button.on_click(lambda b: infer_one_subject(samples_slider.value))\n",
    "\n",
    "# --- DISPLAY UI ---\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<b>1. Simulate a single subject:</b>\"),\n",
    "    z_true_slider,\n",
    "    phi_slider,\n",
    "    n_slider,\n",
    "    simulate_button,\n",
    "    sim_output,\n",
    "    widgets.HTML(\"<b>2. Run inference for that subject:</b>\"),\n",
    "    samples_slider,\n",
    "    infer_button,\n",
    "    infer_output\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18009128",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "**Latent mixture models for model comparison.** In the above example we are inferring what type of person a subject is based on their performance. This is really just a type of model comparison. We are comparing two models: one where the subject is a guesser, and one where the subject is a tryer. The model comparison is done by computing the posterior probability of each model given the data. Here we make it more explicit that we are comparing two models. You can set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2828d08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc268a842e8420681953d3929bda627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Compare Null vs. Alternative Model</h3>'), HTML(value='<b>Data</b>'), VBox(chil…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyjags\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "# Output display\n",
    "output = widgets.Output()\n",
    "\n",
    "# --- RUN THE MODEL COMPARISON ---\n",
    "def run_model_comparison(k, n, theta_null, alpha, beta_param, samples):\n",
    "    output.clear_output(wait=True)\n",
    "\n",
    "    model_code = \"\"\"\n",
    "    model {\n",
    "      z ~ dcat(pi[1:2])\n",
    "      pi[1] <- 0.5\n",
    "      pi[2] <- 0.5\n",
    "\n",
    "      theta[1] <- theta_null\n",
    "      theta[2] ~ dbeta(alpha, beta)\n",
    "\n",
    "      k ~ dbin(theta[z], n)\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    data = {\n",
    "        \"k\": k,\n",
    "        \"n\": n,\n",
    "        \"theta_null\": theta_null,\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\": beta_param\n",
    "    }\n",
    "\n",
    "    model = pyjags.Model(code=model_code, data=data, chains=1, adapt=500)\n",
    "    model.update(1000)\n",
    "    samples_dict = model.sample(samples, vars=[\"z\"])\n",
    "    z_samples = samples_dict[\"z\"].reshape(-1)\n",
    "\n",
    "    # Posterior stats\n",
    "    p_null = np.mean(z_samples == 1)\n",
    "    p_alt = np.mean(z_samples == 2)\n",
    "    bf = p_null / p_alt if p_alt > 0 else np.inf\n",
    "\n",
    "    with output:\n",
    "        # DATA summary\n",
    "        print(\"## Data\")\n",
    "        print(f\"Observed successes (k): {k}\")\n",
    "        print(f\"Number of trials (n):   {n}\")\n",
    "        print()\n",
    "\n",
    "        # PRIORS\n",
    "        print(\"## Model Priors\")\n",
    "        print(f\"Null model: θ = {theta_null:.2f}\")\n",
    "        print(f\"Alternative model: θ ~ Beta({alpha}, {beta_param})\")\n",
    "        print()\n",
    "\n",
    "        # Plot priors and posterior of z\n",
    "        theta_vals = np.linspace(0, 1, 300)\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.axvline(theta_null, color='blue', linestyle='--', label=f\"θ_null = {theta_null:.2f}\")\n",
    "        plt.plot(theta_vals, beta_dist.pdf(theta_vals, alpha, beta_param),\n",
    "                 label=f\"Beta({alpha}, {beta_param})\", color='red')\n",
    "        plt.title(\"Model Priors for θ\")\n",
    "        plt.xlabel(\"θ\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(z_samples, bins=[0.5, 1.5, 2.5], align='mid', rwidth=0.6, color='gray')\n",
    "        plt.xticks([1, 2], [\"Null (θ fixed)\", \"Alt (θ ~ Beta)\"])\n",
    "        plt.title(\"Posterior of z\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Posterior summaries\n",
    "        print(\"## Posterior Model Probabilities\")\n",
    "        print(f\"p(z = 1 | data)  = {p_null:.3f} (Model Null)\")\n",
    "        print(f\"p(z = 2 | data)  = {p_alt:.3f} (Model Alt)\")\n",
    "\n",
    "        if np.isinf(bf):\n",
    "            print(\"Bayes Factor BF_null/alt = ∞ (Model Alt never sampled)\")\n",
    "        else:\n",
    "            print(f\"Bayes Factor BF_null/alt = {bf:.3f}\")\n",
    "\n",
    "# --- WIDGETS ---\n",
    "# Group 1: Data\n",
    "k_slider = widgets.IntSlider(value=30, min=0, max=100, description=\"Successes k:\")\n",
    "n_slider = widgets.IntSlider(value=50, min=1, max=200, step=1, description=\"Trials n:\")\n",
    "data_box = widgets.VBox([k_slider, n_slider])\n",
    "\n",
    "# Group 2: Null model\n",
    "theta_null_slider = widgets.FloatSlider(value=0.5, min=0, max=1, step=0.01, description=\"θ_null:\")\n",
    "null_box = widgets.VBox([theta_null_slider])\n",
    "\n",
    "# Group 3: Alternative model\n",
    "alpha_slider = widgets.FloatSlider(value=1, min=0.1, max=10, step=0.1, description=\"Beta α:\")\n",
    "beta_slider = widgets.FloatSlider(value=1, min=0.1, max=10, step=0.1, description=\"Beta β:\")\n",
    "alt_box = widgets.VBox([alpha_slider, beta_slider])\n",
    "\n",
    "# Group 4: Sampling\n",
    "samples_slider = widgets.IntSlider(value=1000, min=500, max=5000, step=100, description=\"Samples:\")\n",
    "sample_box = widgets.VBox([samples_slider])\n",
    "\n",
    "# Button\n",
    "run_button = widgets.Button(description=\"Run Model Comparison\")\n",
    "\n",
    "def on_click(b):\n",
    "    run_model_comparison(\n",
    "        k_slider.value,\n",
    "        n_slider.value,\n",
    "        theta_null_slider.value,\n",
    "        alpha_slider.value,\n",
    "        beta_slider.value,\n",
    "        samples_slider.value\n",
    "    )\n",
    "\n",
    "run_button.on_click(on_click)\n",
    "\n",
    "# Display UI\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Compare Null vs. Alternative Model</h3>\"),\n",
    "    widgets.HTML(\"<b>Data</b>\"),\n",
    "    data_box,\n",
    "    widgets.HTML(\"<b>Null Model (θ = constant)</b>\"),\n",
    "    null_box,\n",
    "    widgets.HTML(\"<b>Alternative Model (θ ~ Beta)</b>\"),\n",
    "    alt_box,\n",
    "    widgets.HTML(\"<b>Sampling</b>\"),\n",
    "    sample_box,\n",
    "    run_button,\n",
    "    output\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6870ce2",
   "metadata": {},
   "source": [
    "**Compare octopusses via marginal likelihood.** This illustration shows how two different predictive models—one broad and vague (Alice), one narrow and focused (Bob)—assign different probability mass to a fixed data point (the submarine). The model that predicts the actual outcome more precisely (without wasting too much probability elsewhere) achieves a higher marginal likelihood. We approximate this using a grid and visualize the predictions alongside a log-scale ratio of their marginal likelihoods.\n",
    "\n",
    "**Exercise.**  \n",
    "- Try changing Bob’s prediction location or spread. When does he outperform Alice?  \n",
    "- Try switching Alice’s hemisphere. When does she beat Bob?  \n",
    "- What happens to the marginal likelihood ratio as Bob’s prediction gets more precise?  \n",
    "- Meet Chris the squid. He's a fun guy but he's very vague. He thinks the submarine is \"you know, probably just in the water somewhere.\" What would his marginal likelihood be? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98d48ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4925d106d4154bca894484d0c5b2d9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Label(value='Alice’s Predictions'), RadioButtons(description='Hem…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import FloatSlider, RadioButtons, VBox, HBox, Label, interactive_output\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Grid setup\n",
    "grid_res = 100\n",
    "x = np.linspace(0, 1, grid_res)\n",
    "y = np.linspace(0, 1, grid_res)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "grid_points = np.dstack([X, Y])\n",
    "\n",
    "# Submarine location\n",
    "true_x, true_y = 0.65, 0.8\n",
    "ix = (np.abs(x - true_x)).argmin()\n",
    "iy = (np.abs(y - true_y)).argmin()\n",
    "\n",
    "def compute_bob_marginal_likelihood(mu_x, mu_y, sigma):\n",
    "    rv = multivariate_normal(mean=[mu_x, mu_y], cov=sigma**2 * np.eye(2))\n",
    "    pred = rv.pdf(grid_points)\n",
    "    pred /= pred.sum()\n",
    "    return pred, pred[iy, ix]\n",
    "\n",
    "def compute_alice_marginal_likelihood(hemisphere):\n",
    "    mask = Y > 0.5 if hemisphere == 'North' else Y < 0.5\n",
    "    pred = np.zeros_like(X)\n",
    "    pred[mask] = 1.0\n",
    "    pred /= pred.sum()\n",
    "    return pred, pred[iy, ix]\n",
    "\n",
    "def plot_predictions(mu_x=0.5, mu_y=0.75, sigma=0.1, hemisphere='North'):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax1 = plt.subplot2grid((3, 2), (0, 0))\n",
    "    ax2 = plt.subplot2grid((3, 2), (0, 1))\n",
    "    ax3 = plt.subplot2grid((3, 2), (1, 0), colspan=2)\n",
    "\n",
    "    alice_pred, alice_ml = compute_alice_marginal_likelihood(hemisphere)\n",
    "    bob_pred, bob_ml = compute_bob_marginal_likelihood(mu_x, mu_y, sigma)\n",
    "\n",
    "    # Alice's prediction\n",
    "    ax1.imshow(alice_pred, extent=[0, 1, 0, 1], origin='lower', cmap='Reds', alpha=0.3)\n",
    "    ax1.scatter(true_x, true_y, marker='x', color='black', s=100)\n",
    "    ax1.set_title(\"Alice's prediction\")\n",
    "    ax1.text(0.02, 0.95, f\"Marginal likelihood: {alice_ml:.5f}\", transform=ax1.transAxes)\n",
    "    ax1.set_xlabel(\"Longitude\")\n",
    "    ax1.set_ylabel(\"Latitude\")\n",
    "    ax1.set_aspect('equal')\n",
    "\n",
    "    # Bob's prediction\n",
    "    ax2.imshow(bob_pred, extent=[0, 1, 0, 1], origin='lower', cmap='Reds', alpha=0.6)\n",
    "    ax2.scatter(true_x, true_y, marker='x', color='black', s=100)\n",
    "    ax2.set_title(\"Bob's prediction\")\n",
    "    ax2.text(0.02, 0.95, f\"Marginal likelihood: {bob_ml:.5f}\", transform=ax2.transAxes)\n",
    "    ax2.set_xlabel(\"Longitude\")\n",
    "    ax2.set_ylabel(\"Latitude\")\n",
    "    ax2.set_aspect('equal')\n",
    "\n",
    "    # Ratio plot (vertical and narrow)\n",
    "    ax3.set_title(\"Ratio of marginal likelihoods (Bob / Alice)\")\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.set_ylim(1/100, 100)\n",
    "    ax3.set_xlim(0.495, 0.505)\n",
    "    ax3.set_yticks([1/100, 1/30, 1/3, 1, 3, 10, 30, 100])\n",
    "    ax3.set_yticklabels([\"1/100\", \"1/30\", \"1/3\", \"1\", \"3\", \"10\", \"30\", \"100\"])\n",
    "    ax3.set_xticks([])\n",
    "    ax3.set_ylabel(\"Bayes factor (log scale)\")\n",
    "\n",
    "    if alice_ml > 0:\n",
    "        ratio = np.clip(bob_ml / alice_ml, 1/100, 100)\n",
    "        ax3.plot([0.5], [ratio], 'ro', markersize=12)\n",
    "    else:\n",
    "        ax3.text(0.5, 1, \"undefined\", ha='center', va='top')\n",
    "\n",
    "    # Arrows near left margin\n",
    "    ax3.annotate(\"Bob's model better\",\n",
    "                 xy=(0.496, 100), xytext=(0.496, 30),\n",
    "                 arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                 ha='left', va='top', fontsize=10)\n",
    "\n",
    "    ax3.annotate(\"Alice's model better\",\n",
    "                 xy=(0.496, 1/100), xytext=(0.496, 1/30),\n",
    "                 arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                 ha='left', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Widgets\n",
    "alice_controls = VBox([\n",
    "    Label(\"Alice’s Predictions\"),\n",
    "    RadioButtons(options=[\"North\", \"South\"], value=\"North\", description=\"Hemisphere:\")\n",
    "])\n",
    "bob_controls = VBox([\n",
    "    Label(\"Bob’s Predictions\"),\n",
    "    FloatSlider(value=0.67, min=0, max=1, step=0.01, description=\"X (lon)\"),\n",
    "    FloatSlider(value=0.75, min=0, max=1, step=0.01, description=\"Y (lat)\"),\n",
    "    FloatSlider(value=0.1, min=0.01, max=0.3, step=0.01, description=\"Sigma\")\n",
    "])\n",
    "\n",
    "output = interactive_output(\n",
    "    plot_predictions,\n",
    "    {\n",
    "        \"mu_x\": bob_controls.children[1],\n",
    "        \"mu_y\": bob_controls.children[2],\n",
    "        \"sigma\": bob_controls.children[3],\n",
    "        \"hemisphere\": alice_controls.children[1],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(VBox([HBox([alice_controls, bob_controls]), output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859230b",
   "metadata": {},
   "source": [
    "**Bayes factor scale interpretation** The Bayes factor is a measure of the strength of evidence in favor of one model over another. It is defined as the ratio of the marginal likelihoods of two models, and it can be used to compare the relative fit of different models to the same data.\n",
    "\n",
    "**Exercise.** Play around with the sliders to see how the Bayes factor changes as you adjust the marginal likelihood of the two models.\n",
    "- what happens when you switch from $BF_12 to BF_21?\n",
    "- what happens to the Bayes factor when you both models have same marginal likelihood?\n",
    "- what happens if both models are really good?\n",
    "- is it possible to have extreme bayes factors and for both models to be bad? what does this mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd62d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06679f2002649dc92c8389926573c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='<span style=\"color:blue\">p(D|M₁):</span>'), FloatSlider(value=0.5, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2965fb46f688499ebf9a96ff5f37f6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def plot_bayes_factor_vertical(m1_likelihood, m2_likelihood):\n",
    "    bf12 = m1_likelihood / m2_likelihood\n",
    "    bf21 = m2_likelihood / m1_likelihood\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 6))  # Wider to make room for left-aligned labels\n",
    "\n",
    "    # Define Jeffreys scale positions and labels\n",
    "    y_vals = np.log10([1/1000, 1/100, 1/30, 1/10, 1/3, 1, 3, 10, 30, 100, 1000])\n",
    "    yticks = np.log10([1/100, 1/30, 1/10, 1/3, 1, 3, 10, 30, 100])\n",
    "    yticklabels = ['1/100', '1/30', '1/10', '1/3', '1', '3', '10', '30', '100']\n",
    "    descriptions = [\n",
    "        \"Extreme evidence for $M_2$\", \"Very strong evidence for $M_2$\",\n",
    "        \"Strong evidence for $M_2$\", \"Moderate evidence for $M_2$\",\n",
    "        \"Anecdotal evidence for $M_2$\", \"No preference\",\n",
    "        \"Anecdotal evidence for $M_1$\", \"Moderate evidence for $M_1$\",\n",
    "        \"Strong evidence for $M_1$\", \"Very strong evidence for $M_1$\",\n",
    "        \"Extreme evidence for $M_1$\"\n",
    "    ]\n",
    "\n",
    "    # Shading bands\n",
    "    for i in range(len(y_vals) - 1):\n",
    "        ax.axhspan(y_vals[i], y_vals[i+1], color='lightgray', alpha=0.3)\n",
    "\n",
    "    # Plot Bayes Factors\n",
    "    ax.plot(0, np.log10(bf12), 'bo', label=f'$BF_{{12}}$ = {bf12:.2f}')\n",
    "    ax.plot(0, np.log10(bf21), 'ro', label=f'$BF_{{21}}$ = {bf21:.2f}')\n",
    "\n",
    "    # Plot formatting\n",
    "    ax.set_ylim(np.log10(1/1000), np.log10(1000))\n",
    "    ax.set_xlim(-0.5, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_title(\"Bayes Factors on Jeffreys Scale\")\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    # Add full-left labels using fig.text (outside axes)\n",
    "    for i in range(len(y_vals) - 1):\n",
    "        ypos = (y_vals[i] + y_vals[i+1]) / 2\n",
    "        fig.text(0.02, (ypos - ax.get_ylim()[0]) / (ax.get_ylim()[1] - ax.get_ylim()[0]),\n",
    "                 descriptions[i], ha='right', va='center', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\033[34mp(D|M₁): {m1_likelihood:.3f}    \\033[31mp(D|M₂): {m2_likelihood:.3f}\")\n",
    "\n",
    "# Color-coded slider labels\n",
    "m1_label = widgets.HTML('<span style=\"color:blue\">p(D|M₁):</span>')\n",
    "m2_label = widgets.HTML('<span style=\"color:red\">p(D|M₂):</span>')\n",
    "m1_slider = widgets.FloatSlider(value=0.5, min=0.001, max=0.999, step=0.001)\n",
    "m2_slider = widgets.FloatSlider(value=0.5, min=0.001, max=0.999, step=0.001)\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([m1_label, m1_slider]),\n",
    "    widgets.HBox([m2_label, m2_slider])\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(plot_bayes_factor_vertical, {\n",
    "    \"m1_likelihood\": m1_slider,\n",
    "    \"m2_likelihood\": m2_slider\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55693405",
   "metadata": {},
   "source": [
    "**Bayes factor calculation step by step** This code allows you to calculate the Bayes factor step by step. You can adjust the prior and likelihood for each model, and see how this affects the Bayes factor. The code also shows the marginal likelihood for each model, and how this is used to calculate the Bayes factor.\n",
    "\n",
    "**Exercise.** Play around with the sliders to see how the Bayes factor changes as you adjust the data. For a theta of 0.5 for the guessing model can you make the data so that the guessing model wins?\n",
    "\n",
    "- adjust the data to make it BF in favour of M1. \n",
    "\n",
    "- how many different ways can you do this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "115f6c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821e1b9d0f6c42c2b62e33f1a865c686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h4>Data</h4>'), IntSlider(value=10, description='Trials (n):', max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8aabb2728b4ecdb97e5e9c0c69ac2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import beta, binom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Core computation and display logic ---\n",
    "def compute_and_display(k=9, n=10, theta_fixed=0.5):\n",
    "    if k > n:\n",
    "        display(Markdown(\"**⚠️ Error:** $k$ (successes) cannot be greater than $n$ (trials).\"))\n",
    "        return\n",
    "\n",
    "    # Marginal likelihoods\n",
    "    marginal_M1 = 1 / (n + 1)\n",
    "    marginal_M2 = binom.pmf(k, n, theta_fixed)\n",
    "    BF_12 = marginal_M1 / marginal_M2 if marginal_M2 > 0 else np.inf\n",
    "\n",
    "    # --- Data section ---\n",
    "    display(Markdown(f\"\"\"\n",
    "## Data  \n",
    "- Number of trials: $n = {n}$  \n",
    "- Number of successes: $k = {k}$  \n",
    "\"\"\"))\n",
    "\n",
    "    # --- Models section ---\n",
    "    display(Markdown(f\"\"\"\n",
    "## Models  \n",
    "- **$M_1$**: Unknown ability → $\\\\theta \\\\sim \\\\mathrm{{Uniform}}(0,1)$  \n",
    "- **$M_2$**: Guessing → $\\\\theta = {theta_fixed:.2f}$\n",
    "\"\"\"))\n",
    "\n",
    "    # --- Priors section ---\n",
    "    theta = np.linspace(0, 1, 500)\n",
    "    prior_M1 = np.ones_like(theta)\n",
    "    fig, ax = plt.subplots(figsize=(6, 2.5))\n",
    "    ax.plot(theta, prior_M1, label=r\"$M_1$: $\\theta \\sim \\mathrm{Uniform}(0,1)$\", color='blue')\n",
    "    ax.axvline(theta_fixed, color='red', linestyle='--', linewidth=2, label=rf\"$M_2$: $\\theta = {theta_fixed:.2f}$\")\n",
    "    ax.set_title(\"Priors\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_xlabel(r\"$\\theta$\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_yticks([])\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Marginal Likelihoods section ---\n",
    "    display(Markdown(f\"\"\"\n",
    "## Marginal Likelihoods  \n",
    "\n",
    "- $p(D \\\\mid M_1) = \\\\frac{{1}}{{n+1}} = \\\\frac{{1}}{{{n}+1}} = {marginal_M1:.4f}$  \n",
    "\n",
    "- $p(D \\\\mid M_2) = \\\\binom{{{n}}}{{{k}}} ({theta_fixed:.2f})^{{{k}}} (1 - {theta_fixed:.2f})^{{{n-k}}} = {marginal_M2:.4f}$  \n",
    "\"\"\"))\n",
    "\n",
    "    # --- Posterior section ---\n",
    "    display(Markdown(\"## Posterior\"))\n",
    "    posterior_M1 = beta.pdf(theta, k + 1, n - k + 1)\n",
    "    fig, ax = plt.subplots(figsize=(6, 2.5))\n",
    "    ax.plot(theta, posterior_M1, label=fr\"$M_1$: Beta($\\alpha$={k+1}, $\\beta$={n-k+1})\", color='blue')\n",
    "    ax.axvline(theta_fixed, color='red', linestyle='--', linewidth=2, label=rf\"$M_2$: $\\theta = {theta_fixed:.2f}$\")\n",
    "    ax.set_title(\"Posterior for $M_1$ and fixed $\\theta$ under $M_2$\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_xlabel(r\"$\\theta$\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Bayes Factor section ---\n",
    "    display(Markdown(f\"\"\"\n",
    "## Bayes Factor  \n",
    "\n",
    "- $BF_{{12}} = \\\\frac{{p(D \\mid M_1)}}{{p(D \\mid M_2)}} = \\\\frac{{{marginal_M1:.4f}}}{{{marginal_M2:.4f}}} = {BF_12:.2f}$  \n",
    "\n",
    "**Interpretation:**  \n",
    "The data is about {BF_12:.1f}× more likely under $M_1$ than under $M_2$.\n",
    "\"\"\"))\n",
    "\n",
    "# --- Interactive widget setup ---\n",
    "def interactive_bf(n, k, theta_fixed):\n",
    "    compute_and_display(k=min(k, n), n=n, theta_fixed=theta_fixed)\n",
    "\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=50, description='Trials (n):')\n",
    "k_slider = widgets.IntSlider(value=9, min=0, max=50, description='Successes (k):')\n",
    "theta_slider = widgets.FloatSlider(value=0.5, min=0.01, max=0.99, step=0.01, description='θ (M₂):')\n",
    "\n",
    "def update_k_slider_range(*args):\n",
    "    k_slider.max = n_slider.value\n",
    "\n",
    "n_slider.observe(update_k_slider_range, names='value')\n",
    "\n",
    "# Layout: Sliders grouped by section\n",
    "data_section = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>Data</h4>\"),\n",
    "    n_slider,\n",
    "    k_slider\n",
    "])\n",
    "\n",
    "model_section = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>Models</h4>\"),\n",
    "    theta_slider\n",
    "])\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    data_section,\n",
    "    model_section\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(interactive_bf, {\n",
    "    'n': n_slider,\n",
    "    'k': k_slider,\n",
    "    'theta_fixed': theta_slider\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5c252",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5106748",
   "metadata": {},
   "source": [
    "**Posterior odds calculation.** This code allows you to calculate the posterior odds for two models given the prior odds and the Bayes factor. The posterior odds are the odds of one model being true given the data, and they can be calculated using Bayes' theorem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21fecac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f4e72b2d2648f5bcefa57491709720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Data:</h3>'), HBox(children=(IntSlider(value=5, description='Successes (k):', m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22798c6d055a4d8782fee1c08daca0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta, binom\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- Widgets ---\n",
    "k_slider = widgets.IntSlider(value=5, min=0, max=20, step=1, description='Successes (k):')\n",
    "n_slider = widgets.IntSlider(value=10, min=1, max=20, step=1, description='Trials (n):')\n",
    "\n",
    "alpha1_slider = widgets.FloatSlider(value=1, min=0.1, max=10, step=0.1, description='α₁ (M1):')\n",
    "beta1_slider = widgets.FloatSlider(value=1, min=0.1, max=10, step=0.1, description='β₁ (M1):')\n",
    "\n",
    "alpha2_slider = widgets.FloatSlider(value=2, min=0.1, max=10, step=0.1, description='α₂ (M2):')\n",
    "beta2_slider = widgets.FloatSlider(value=2, min=0.1, max=10, step=0.1, description='β₂ (M2):')\n",
    "\n",
    "prior_odds_slider = widgets.FloatSlider(value=1, min=0.1, max=10, step=0.1, description='Prior Odds (M1:M2)')\n",
    "show_sampling_toggle = widgets.ToggleButton(value=False, description='Show sampling data', icon='flask')\n",
    "samples_slider = widgets.IntSlider(value=1000, min=100, max=5000, step=100, description='Samples:')\n",
    "\n",
    "# --- Core logic ---\n",
    "def update(k, n, alpha1, beta1, alpha2, beta2, prior_odds, show_sampling):\n",
    "    if k > n:\n",
    "        k = n\n",
    "        k_slider.value = n\n",
    "\n",
    "    theta = np.linspace(0, 1, 500)\n",
    "    likelihood = binom.pmf(k, n, theta)\n",
    "\n",
    "    prior1 = beta.pdf(theta, alpha1, beta1)\n",
    "    prior2 = beta.pdf(theta, alpha2, beta2)\n",
    "\n",
    "    unnorm_post1 = likelihood * prior1\n",
    "    unnorm_post2 = likelihood * prior2\n",
    "\n",
    "    post1 = unnorm_post1 / np.trapz(unnorm_post1, theta)\n",
    "    post2 = unnorm_post2 / np.trapz(unnorm_post2, theta)\n",
    "\n",
    "    marginal1 = np.trapz(unnorm_post1, theta)\n",
    "    marginal2 = np.trapz(unnorm_post2, theta)\n",
    "\n",
    "    bayes_factor = marginal1 / marginal2 if marginal2 > 0 else np.inf\n",
    "    posterior_odds = prior_odds * bayes_factor\n",
    "\n",
    "    p1 = prior_odds / (1 + prior_odds)\n",
    "    p2 = 1 / (1 + prior_odds)\n",
    "    post1_prob = posterior_odds / (1 + posterior_odds)\n",
    "    post2_prob = 1 / (1 + posterior_odds)\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(12, 10), sharex=True)\n",
    "    fig.suptitle(f\"Bayesian Model Comparison (k={k}, n={n})\", fontsize=16)\n",
    "\n",
    "    axs[0, 0].plot(theta, prior1, label='Prior', color='blue')\n",
    "    axs[0, 1].plot(theta, prior2, label='Prior', color='red')\n",
    "    axs[1, 0].plot(theta, likelihood, label='Likelihood', color='black')\n",
    "    axs[1, 1].plot(theta, likelihood, label='Likelihood', color='black')\n",
    "    axs[2, 0].plot(theta, post1, label='Posterior', color='blue')\n",
    "    axs[2, 1].plot(theta, post2, label='Posterior', color='red')\n",
    "\n",
    "    if show_sampling:\n",
    "        samples = samples_slider.value\n",
    "        sampled_prior1 = np.random.beta(alpha1, beta1, samples)\n",
    "        sampled_prior2 = np.random.beta(alpha2, beta2, samples)\n",
    "        sampled_post1 = np.random.beta(alpha1 + k, beta1 + n - k, samples)\n",
    "        sampled_post2 = np.random.beta(alpha2 + k, beta2 + n - k, samples)\n",
    "\n",
    "        axs[0, 0].hist(sampled_prior1, bins=50, density=True, alpha=0.3, color='blue')\n",
    "        axs[0, 1].hist(sampled_prior2, bins=50, density=True, alpha=0.3, color='red')\n",
    "        axs[2, 0].hist(sampled_post1, bins=50, density=True, alpha=0.3, color='blue')\n",
    "        axs[2, 1].hist(sampled_post2, bins=50, density=True, alpha=0.3, color='red')\n",
    "\n",
    "    for ax_row in axs:\n",
    "        for ax in ax_row:\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(bottom=0)\n",
    "            ax.grid(True)\n",
    "\n",
    "    axs[0, 0].set_title(\"Model 1: Prior\")\n",
    "    axs[0, 1].set_title(\"Model 2: Prior\")\n",
    "    axs[1, 0].set_title(\"Model 1: Likelihood\")\n",
    "    axs[1, 1].set_title(\"Model 2: Likelihood\")\n",
    "    axs[2, 0].set_title(\"Model 1: Posterior\")\n",
    "    axs[2, 1].set_title(\"Model 2: Posterior\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # Posterior model probabilities pie chart\n",
    "    fig, ax = plt.subplots(figsize=(5, 3))\n",
    "    ax.pie([post1_prob, post2_prob], labels=['Model 1', 'Model 2'], autopct='%.1f%%',\n",
    "           colors=['blue', 'red'], startangle=90)\n",
    "    ax.set_title(\"Posterior Model Probabilities\")\n",
    "    plt.subplots_adjust(top=0.8, bottom=0.2)\n",
    "    plt.show()\n",
    "\n",
    "    # Equation table using plain text\n",
    "    html_table = f\"\"\"\n",
    "    <h4>Posterior Odds Calculation</h4>\n",
    "    <table style=\"width:100%; text-align:center; border-collapse: collapse;\">\n",
    "      <tr style=\"border-bottom:1px solid #ccc;\">\n",
    "        <th></th>\n",
    "        <th>Prior Odds</th>\n",
    "        <th>Bayes Factor</th>\n",
    "        <th>Posterior Odds</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Equation</b></td>\n",
    "        <td>p(M1)/p(M2)</td>\n",
    "        <td>p(D|M1)/p(D|M2)</td>\n",
    "        <td>p(M1|D)/p(M2|D)</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td><b>Substitution</b></td>\n",
    "        <td>{p1:.2f} / {p2:.2f}</td>\n",
    "        <td>{marginal1:.2f} / {marginal2:.2f}</td>\n",
    "        <td>{post1_prob:.2f} / {post2_prob:.2f}</td>\n",
    "      </tr>\n",
    "      <tr style=\"border-top:1px solid #ccc;\">\n",
    "        <td><b>Value</b></td>\n",
    "        <td>{prior_odds:.2f}</td>\n",
    "        <td>{bayes_factor:.2f}</td>\n",
    "        <td>{posterior_odds:.2f}</td>\n",
    "      </tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    display(HTML(html_table))\n",
    "\n",
    "# --- UI setup ---\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Data:</h3>\"),\n",
    "    widgets.HBox([k_slider, n_slider]),\n",
    "    widgets.HTML(\"<h3>Model 1 Prior (θ ~ Beta):</h3>\"),\n",
    "    widgets.HBox([alpha1_slider, beta1_slider]),\n",
    "    widgets.HTML(\"<h3>Model 2 Prior (θ ~ Beta):</h3>\"),\n",
    "    widgets.HBox([alpha2_slider, beta2_slider]),\n",
    "    widgets.HTML(\"<h3>Model Comparison:</h3>\"),\n",
    "    prior_odds_slider,\n",
    "    show_sampling_toggle,\n",
    "    samples_slider\n",
    "])\n",
    "\n",
    "out = widgets.interactive_output(update, {\n",
    "    \"k\": k_slider,\n",
    "    \"n\": n_slider,\n",
    "    \"alpha1\": alpha1_slider,\n",
    "    \"beta1\": beta1_slider,\n",
    "    \"alpha2\": alpha2_slider,\n",
    "    \"beta2\": beta2_slider,\n",
    "    \"prior_odds\": prior_odds_slider,\n",
    "    \"show_sampling\": show_sampling_toggle\n",
    "})\n",
    "\n",
    "display(ui, out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
