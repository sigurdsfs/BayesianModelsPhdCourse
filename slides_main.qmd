--- 
title: Bayesian Models of Brains, Minds, & Behaviors
subtitle:  Hvidovre Hospital / Copenhagen / May 2025 
author:  Ollie Hulme 
format:
  revealjs:
    code-fold: true
    incremental: true
    toc: true  # Enables a sidebar with slide titles
    toc-depth: 1
    toc-title: "Lecture Topics"  # Custom title for TOC
    slide-number: c/t
    execute:
      enabled: true  # Ensures code execution
    navigation-mode: vertical  # Allows 2D navigation (optional)
    chalkboard:
      buttons: true
    preview-links: auto
    css: styles.css
    #footer: "[insert binder link]"
jupyter: python3  # Ensure Quarto uses Jupyter instead of R
params:
  title_acronym: "bayesMod"  # Store for potential reuse
resources:
  - bayesModels.pdf
---

# Affiliations

![](images/DRCMR_regionH_KU_logo.png)  


# Preamble
::: incremental
- Course materials
- Course schedule
- Social 😊
:::

## Course materials
::: incremental
- All slides & interactive exercises run on binder → [binder link]
- ☝️ Lets you run code without needing to install anything. Simples. 😌 
- Also downloadable on github [github.com/ollie-hulme/BayesianModels](https://github.com/ollie-hulme/BayesianModels)
:::

## Course schedule
::: incremental
- *insert schedule*
:::

## WhatsApp social group
::: incremental
- Here you can talk to each other
- Answer each others questions
- Stay in touch ❤️
- Plan friday bar 😄
:::

# Introduction
::: incremental
- Aims 
- Expectations
- Bayesian preliminaries
:::

## Overarching aim
::: incremental
-  To use the tools of probability theory to scientifically understand brains minds annd behavior
:::

## Specific aims
::: incremental
- Hands-on / interactive / discursive
- KISS: Keep It Simple Stoopid 😜
- Stop describing 🥱 → start explaining 😌
- Emphasis on the models of mind & behavior
- Paired with simple methods for connecting to neural data 🧠
- Intuitive, playful, enlightening, soul-nourishing, life-affirming, fun!, useful! ✨
:::

## Our expectations of you
::: incremental
- Life is too to short to let things pass that you dont understand. 
- Your life matters. Your understanding matters. Actively take part.
- You are entitled to understand this. 
- Don't listen, fight it. 🥊
- Ask questions, interupt. 🙋‍♂️
- Be sceptical. You cant offend me. 🤨
::: 

## Do say:
::: incremental
- "I'm probably being stupid..." 🤪
- "I might have missed this..."
- "i dont understand why..."
- "do you have an intuition for why..."
- "i'm confused..." 🤔
- If shy, say it in the break, write in whatsApp
:::

## Don't:
::: incremental
- Nod and pretend you get something. 🙅‍♂️ 
- Assume you are the only one confused. 😵‍💫😵‍💫😵‍💫
:::

## What is a Bayesian model?
::: incremental
- Bayesian models → probabilistic models
- Uncertainty is fundamental to all real-world phenomena
  - Brains / minds / bodies need probabilistic models 
- Uncertainty is fundamental to all data about brains / minds / bodies 
  - Scientists need probabilistic models 
:::

## What can Bayesian models model?
::: incremental
- Anything. Literally. 🤯
- Here we apply it to brains/minds/behavior, with emphasis on experimental data. 
- Particular emphasis on: 
  - cognitive models which can be inferred from behavior 
  - integration into analyses of neuroimaging data
- The heavy lifting is all on the cogntive modelling, but principles apply to any phenomena of interest.
- These modelling principles apply to anything. I promise.    
:::

## The esesence of Bayesianism?
::: incremental
- Bayesianism just reduces to bein consistent with your beliefs... 
- ...and how you update them according to evidence.
- Being consistent with probabilitistic beliefs is just logic.
- So really, deep down, this course is just about being logical.
- It's not a theory that could be empirically falsified, its not Newtonian physics, or evolution.
- Its mathematically true.  
:::

## Spoiler alert: Bayes wasn't that much of a genius
::: incremental
- Bayes wasn't a Newton, Darwin, or Einstein.
- He just rearranged an equation and came up with an interesting application. 
- Its not his domain of theory, he just was the first to notice this and suggest an application. 
:::

# How to think like a Bayesian
'Google slides: <a href="https://docs.google.com/presentation/d/1wEUf06-69HDm93K-SIimnBpMw4ken5g6j6rDQ5xTsoU/edit?usp=sharing
">[link]</a>'

# The basics of Bayesian analysis
::: incremental
- Principles
- Hidden vs. observable
- Beliefs and evidence  
- Estimation methods
- Why Bayes?
:::

## General principles
::: incremental
-  current belief → evidence → new belief
-   prior belief  → evidence → posterior belief
::: 

## Example
::: incremental
- a cognitive task of 10 trials of equal difficulty
- we want to estimate cognitive ability $\theta$ which is the rate of correct performance
- $\theta$ is hidden (latent), we cannot observe it directly, only its effect on the number of trials correct
- before we obtain data we have uncertainty over $\theta$
- we do know $0 < \theta <1$ where 0 is no performance, and 1 is perfect performance
::: 

## Hidden variables
::: incremental
- modeling hidden variables is central to cognitive modelling 
- it is at the heart of explanation beyond description
- description would just plot performance score as this is observed.
- but if you want to understand what underlies it, you need to model hidden variables.
- psychology / neuroscience is always ultimately about hidden variables
:::

## Why the difference between hidden and observed
::: incremental
- number of trials correct $k$ is an observable consequence of ability which is hidden
- they are not the same
- the same ability can give different $\k$ depending on chance
- often our questions and theories are about the hidden not the observed. 
:::

## Prior belief over $\theta$
::: incremental
- if we are agnostic about $\theta$ we can assign uniform probability distribution for $p(\theta)$
- *uniform dist*
- we know nothing so we assign equal probability to all possible values of $\theta$
- we are maximally uncertain
:::

## Representing beliefs as probabiity distributions
::: incremental
- Probability distributions are a natural way to represent beliefs
- They express our what we believe (.e.g is theta high or low) 
- ...and our uncertainy (how spread out is the distribution)
- *interactive beta dist*
- play with sliders to represent different prior beliefs
:::

## Update prior with evidence from data
::: incremental
- Data: k = 9/10 trials correct
- $p(\theta) \rightarrow p(k|\theta) \rightarrow p(\theta|data)$  
- prior $\rightarrow$ likelihood $\rightarrow$ posterior 
:::

## Posterior distribution
::: incremental
- prior was *uniform dist*
- posterior $p(\theta|data)$
- is *plot beta(9,1)*
- we are now more certain than before
- the evidence has reduced our uncertainty
:::


## Bayes rule
::: incremental
- Bayes rule expresses how to update priors into posterior according to data
- $p(\theta | \text{data}) = \frac{p(\text{data} | \theta) \, p(\theta)}{p(\text{data})}$
- $\text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{marginal likelihood}}$
:::

## Marginal likelihood
::: incremental
- $p(D)$ this is the probability of the data. 
- Its a single number that ensures the posterior sums to 1 and is therefore a probability distribution
- It doesnt depend on $\theta$
:::

## Alternative expression of Bayes
::: incremental
- $p(\theta | \text{data}) \propto p(\text{data} | \theta) \times p(\theta)$
- $\text{Posterior} \propto \text{Likelihood} \times \text{Prior}$
:::

## Bayesian credibility intervals express uncertainy
::: incremental
- The posterior can be interpreted as a probability distribution
- if 50% of distribution is above say 0.55, then there is $p(0.5)>\theta$ 
- Bayesian credible intervals e.g. $\text{BCI}_{95\%} [0.4,0.8]$ meaans that 95% probability the true value is between those limits. 
- This is what we want classical confidence intervals to be. 
- *insert drawing of how to make BCI*
:::

## Sequential updating
::: incremental
- suppose we collect more data
- 5 more trials and 3 of these are correct
- we previously had 9 out of 10 correct
- we just add the data together so now its 12 / 15
- unlike frequentist statistics the order of data collection doesnt matter
- *plot new posterior*
:::

## Sequential updating
::: incremental
- it makes no difference if
- compute posterior for first data, then update again
- prior → 9/10 → posterior → prior → 3/ 5 → posterior
- or just compute all in one go
- prior → 12/15 → posterior
- same final posterior. simples.
:::

## Analytic solution
::: incremental
- For certain cases you can calculate posteriors "analytically" via equations
- e.g. in this example we represent our uniform prior as a beta distribution
- $p(\theta)~beta(1,1)$
- to update to the posterior is simple
- $p(\theta|k,n)~beta(1+k,1+(n-k))$
- thus for every possible observation we know how to update our posterior
::: 

## Interactive beta distribution
::: incremental
- *plot beta with sliders for k and n*
- play around with the data to see how it updates the uniform prior to any posterior
- what values of k and n, make the posterior most uncertain?
- which values make posterior most certain it theta is low? high? intermediate?
::: 

## Conjugate
::: incremental
- in nice cases like this, it is only possible because the prior and posterior belong to the same family of distributions
- they are "conjugate"
- sadly, in most cases this is not possible 😞 
:::

## So what do we do in other cases?
::: incremental
- dont fear MCMC is here!
- what?
- Monte Carlo Markov Chain.  
- Its a method for sampling from the posterior and it works in almost all cases.
- It's a little bit magic. 
- This course focuses on MCMC because it works in all cases and its simple to use. 
:::

## Comparing analytic vs. numerical (sampling)
::: incremental
- *plot analytic posterior*
- *plot MCMC*
- *screencap from book?*
:::

## How does MCMC work?
::: incremental
- There are two paths forward. 
- Red pill - you like to know the truth because truth is power. Therefore you can follow this video *insert link* on how it works
- Blue pill - you take for granted that it works and not knowing exactly how has no material impact on your modelling. 
:::

## Advantages of Bayesian modelling
::: incremental
- Flexibility - Bayesian models can be built to accomodate the complexity or simplicity of any data. They can deal with contamination, confounds, hierarchical data, missing data.
- Principled - Uncertainty is always fully represented, and accounted for, no useful information is discarded. 
- Intuitive - Generally Bayes will align with your intuition, much better than other approaches. Sometimes you will need to update your intuition, but thats ok. 
- Easy - It is easy, once you know how. 
:::

# Modelling a binary process 
::: incremental
- Start simple → binary processes
- Most tasks are binary processes 😲
:::

## Getting started
::: incremental
- Our first example was about inferring a rate for a binary process
- Binary process since either correct or incorrect
- Inferring a rate, since we were inferring the underlying ability as a probability $\theta$
- Lots of tasks can be represented as binary processes
:::

## Cognitive tasks that are binary processes
::: incremental
- Task switching
- Go-no-go
- Stop signal
- etc. 
:::

## From binary outcomes to rate
::: incremental
- Binary outcomes typically k correct out of n
- Expressed as a rate = k / n
- This is the observed rate
- But what is the underlying hidden rate $/theta$
- $k \sim \text{Binomial}(\theta, n)$
:::