<!DOCTYPE html>
<html lang="en"><head>
<script src="BayesianModels_slides_files/libs/clipboard/clipboard.min.js"></script>
<script src="BayesianModels_slides_files/libs/quarto-html/tabby.min.js"></script>
<script src="BayesianModels_slides_files/libs/quarto-html/popper.min.js"></script>
<script src="BayesianModels_slides_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="BayesianModels_slides_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="BayesianModels_slides_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="BayesianModels_slides_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.42">

  <title>bayesianmodels_slides</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="BayesianModels_slides_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="BayesianModels_slides_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="BayesianModels_slides_files/libs/revealjs/dist/theme/quarto-2f366650f320edcfcf53d73c80250a32.css">
  <link rel="stylesheet" href="BayesianModels_styles.css">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section class="slide level2">

<!-- #region Title slide -->
</section>
<section class="slide level2">

<div style="display: flex; flex-direction: column; align-items: center; height: 100vh; padding-top: 60px; text-align: center;">
<div style="font-size:1.6em; font-weight:bold; margin-bottom:10px; margin-top:10px;">
<pre><code>Bayesian Models of Brains, Minds, &amp; Behaviors </code></pre>
</div>
<div style="font-size:1.1em; margin-bottom:15px;">
<pre><code>DRCMR · Copenhagen · May 2025 </code></pre>
</div>
<div style="font-size:0.9em; max-width: 80%; margin: 0 auto;">
<pre><code>Ollie Hulme · David Meder · Janine Bühler · Melissa Larsen
    Amin Kangavari · Simon Steinkamp · Naiara Demnitz</code></pre>
</div>
<p><img src="images/DRCMR_regionH_KU_logo.png" alt="DRCMR logo" style="height:80px; margin-top:15px;"></p>
</div>
</section>
<section class="slide level2">

<!-- #endregion -->
<!-- #region Lecture#1 Preamble -->
</section>
<section>
<section id="lecture-1-preamble-housekeeping" class="title-slide slide level1 center">
<h1>Lecture 1: Preamble &amp; housekeeping</h1>

</section>
<section id="roadmap" class="slide level2">
<h2>Roadmap</h2>
<ul>
<li class="fragment">What this course is about</li>
<li class="fragment">Materials: GitHub, Binder, Book…<br>
</li>
<li class="fragment">Schedule, social, and how the week works<br>
</li>
<li class="fragment">Group project<br>
</li>
<li class="fragment">Vibe &amp; expectations</li>
</ul>
</section>
<section id="how-the-week-will-go" class="slide level2">
<h2>How the week will go</h2>
<ul>
<li class="fragment">A mix of lectures, interactive demos, case studies, discussion &amp; group work<br>
</li>
<li class="fragment">You’ll build and present a research project with your group<br>
</li>
<li class="fragment">Lectures &amp; demos lay the conceptual groundwork</li>
<li class="fragment">You build the research project sequentially as you learn more<br>
</li>
<li class="fragment">Morning - lectures and demos</li>
<li class="fragment">Afternoons - self-paced demos and/or group work</li>
</ul>
</section>
<section id="materials" class="slide level2">
<h2>Materials</h2>
<ul>
<li class="fragment">GitHub: slides, code, schedule, everything…<br>
</li>
<li class="fragment">Binder: run interactive notebooks in browser, no install<br>
</li>
<li class="fragment">Book: key reference, especially early in the week<br>
</li>
<li class="fragment">Important links: GitHub <code>README.md</code></li>
</ul>
</section>
<section id="github" class="slide level2">
<h2>GitHub</h2>
<ul>
<li class="fragment"><p>Main hub for slides, code, notebooks, and schedule etc.<br>
</p></li>
<li class="fragment"><p>link in footer 👇<br>
<img src="/images/github.png" width="500px"></p>
<h2 id="binder">Binder</h2></li>
<li class="fragment"><p>Launch notebooks in your browser<br>
</p></li>
<li class="fragment"><p>No installation needed<br>
</p></li>
<li class="fragment"><p>Be patient — it takes time to load<br>
</p></li>
<li class="fragment"><p>Keep your Binder tab open</p></li>
<li class="fragment"><p>It can time out so best to use continuously in longer sessions<br>
<img src="/images/binder.png" width="300px"></p></li>
</ul>
</section>
<section id="book" class="slide level2">
<h2>Book</h2>
<ul>
<li class="fragment">Core reference for models &amp; theory</li>
<li class="fragment">This course roughly goes through it in order<br>
</li>
<li class="fragment">Especially useful Mon–Wed<br>
<img src="/images/book.jpeg" width="300px"></li>
</ul>
</section>
<section id="schedule" class="slide level2">
<h2>Schedule</h2>
<ul>
<li class="fragment">Each day blends concepts, demos, &amp; group work</li>
</ul>
</section>
<section id="schedule-1" class="slide level2">
<h2>Schedule</h2>
<ul>
<li class="fragment">Up to date on GitHub</li>
<li class="fragment">Note different room on wed &amp; thurs<br>
</li>
<li class="fragment"><img src="/images/schedule.png" width="1000px"></li>
</ul>
</section>
<section id="course-overview" class="slide level2">
<h2>Course overview</h2>
<ul>
<li class="fragment"><strong>Basic modelling</strong> → Get started (Mon-Tues)</li>
<li class="fragment"><strong>Intermediate modelling</strong> → Go deeper (Wed)</li>
<li class="fragment"><strong>Neural data integration</strong> → Link models to brain (Thurs)</li>
<li class="fragment"><strong>Group project</strong> → Model, analyze, present (Fri)</li>
</ul>
</section>
<section id="course-schematic" class="slide level2">
<h2>Course schematic</h2>
<ul>
<li class="fragment"><img data-src="images/course_schema.jpg" style="width:100.0%"><br>
</li>
<li class="fragment">Your group project will follow this workflow</li>
</ul>
</section>
<section id="group-project" class="slide level2">
<h2>Group project</h2>
<ul>
<li class="fragment">Form a small group<br>
</li>
<li class="fragment">Pick a cognitive question<br>
</li>
<li class="fragment">Design an experiment (behavioral + neural)<br>
</li>
<li class="fragment">Build models to test your hypotheses<br>
</li>
<li class="fragment">Present your work on Friday (~15 min)</li>
<li class="fragment">See google doc on the Github README</li>
</ul>
</section>
<section id="supervision-support" class="slide level2">
<h2>Supervision &amp; Support</h2>
<ul>
<li class="fragment"><strong>Janine</strong>: talk to her if you are shy to ask in class<br>
</li>
<li class="fragment"><strong>Ollie &amp; David:</strong> logistics, organisation, schedule<br>
</li>
<li class="fragment"><strong>Simon:</strong> anything technical, Binder, GitHub, Python<br>
</li>
<li class="fragment"><strong>Group work:</strong> you will have many supervisors depending on the day</li>
</ul>
</section>
<section id="social" class="slide level2">
<h2>Social</h2>
<ul>
<li class="fragment">Join the WhatsApp group (link on GitHub)<br>
</li>
<li class="fragment">There you can ask informal questions, comment, idle thoughts</li>
<li class="fragment">Friday bar to celebrate</li>
</ul>
</section>
<section id="our-expectations-of-you" class="slide level2">
<h2>Our expectations of you</h2>
<ul>
<li class="fragment">Ask questions — don’t nod and fake it<br>
</li>
<li class="fragment">Disrupt — curiosity is good<br>
</li>
<li class="fragment">You deserve to understand this</li>
</ul>
</section>
<section id="things-we-love-to-hear" class="slide level2">
<h2>Things we love to hear</h2>
<ul>
<li class="fragment">“I might have missed this, but…”<br>
</li>
<li class="fragment">“Can I ask a stupid question?”<br>
</li>
<li class="fragment">“Do you have an intuition for why…”<br>
</li>
<li class="fragment">“I’m confused” 🤔</li>
</ul>
</section>
<section id="things-not-to-do-to-yourself" class="slide level2">
<h2>Things not to do to yourself</h2>
<ul>
<li class="fragment">Don’t pretend to get it<br>
</li>
<li class="fragment">Don’t assume you’re the only one confused<br>
</li>
<li class="fragment">Don’t sit in silence out of self-doubt</li>
</ul>
</section>
<section id="overarching-aim" class="slide level2">
<h2>Overarching aim</h2>
<ul>
<li class="fragment">Use probability theory to explain minds, brains, and behavior<br>
</li>
<li class="fragment">Upgrade your scientific reasoning<br>
</li>
<li class="fragment">Simple: intuitive, powerful tools<br>
</li>
<li class="fragment">Universal: same ideas apply across science<br>
</li>
<li class="fragment">From description → explanation</li>
</ul>
</section>
<section id="specific-aims" class="slide level2">
<h2>Specific aims</h2>
<ul>
<li class="fragment">Build and test cognitive models<br>
</li>
<li class="fragment">Link cognitive models to neural data<br>
</li>
<li class="fragment">Be hands-on, interactive, exploratory</li>
</ul>
</section>
<section id="upcoming-lectures" class="slide level2">
<h2>Upcoming lectures</h2>
<ul>
<li class="fragment">Basics of Bayesian thinking<br>
</li>
<li class="fragment">Modeling cognition as a binary process<br>
</li>
<li class="fragment">Modeling mixtures of processes<br>
</li>
<li class="fragment">Selecting models<br>
</li>
<li class="fragment">Bayes factors and posterior odds<br>
<!-- #endregion--></li>
</ul>
<!-- #region Lecture#2 Basics of Bayesian analysis-->
</section></section>
<section>
<section id="lecture-2-bayesian-basics" class="title-slide slide level1 center">
<h1>Lecture 2: Bayesian basics</h1>

</section>
<section id="roadmap-1" class="slide level2">
<h2>Roadmap</h2>
<ul>
<li class="fragment">The spirit of Bayesian thinking</li>
<li class="fragment">What is Bayesian modeling?</li>
<li class="fragment">Principles of Bayesian inference</li>
<li class="fragment">Observable vs.&nbsp;latent variables</li>
<li class="fragment">Beliefs and evidence</li>
<li class="fragment">Estimation methods</li>
<li class="fragment">Why Bayesian methods?</li>
</ul>
</section>
<section id="the-spirit-of-bayesian-thinking" class="slide level2">
<h2>The spirit of Bayesian thinking</h2>
<ul>
<li class="fragment"><br></li>
<li class="fragment"><em>“Probability theory is nothing but common sense reduced to calculation.”</em> — Laplace (1814)<br>
</li>
<li class="fragment"><br></li>
<li class="fragment"><em>“The rules of probability are the rules of consistent reasoning.”</em> — Jaynes (2003)<br>
</li>
<li class="fragment"><br></li>
<li class="fragment"><em>“Bayesian methods are not a special brand of inference; they are the only logically consistent rules for inference that are known.”</em> — Jaynes (2003)</li>
</ul>
<div class="footer">
<p>Laplace (1814); Jaynes (2003)</p>
</div>
</section>
<section id="bayesianism-as-the-calculus-of-common-sense" class="slide level2">
<h2>Bayesianism as the calculus of common sense</h2>
<ul>
<li class="fragment"><br><br></li>
<li class="fragment"><em>Bayesianism is just probability theory applied to inference.</em> — Jaynes (2003)</li>
</ul>
<div class="footer">
<p>Jaynes (2003)</p>
</div>
</section>
<section id="probability-as-rational-consistency" class="slide level2">
<h2>Probability as rational consistency</h2>
<ul>
<li class="fragment">Bayesian modeling follows the rules of probability.</li>
<li class="fragment">Probability is logic.</li>
<li class="fragment">Logic is consistency.</li>
<li class="fragment">Consistency is rationality.</li>
<li class="fragment">And rationality is just thinking clearly.</li>
</ul>
</section>
<section id="so-what-is-bayesian-modeling-of-minds-brains-and-behavior" class="slide level2">
<h2>So what is “Bayesian Modeling of Minds, Brains, and Behavior”?</h2>
<ul>
<li class="fragment">This course is about <strong>thinking clearly about minds, brains, and behavior</strong>.</li>
<li class="fragment">It’s about testing theories rationally, using the evidence provided by data.</li>
<li class="fragment">Bayesian modeling offers a principled, rational way to update beliefs based on evidence.</li>
<li class="fragment">Ta-da!</li>
</ul>
</section>
<section id="bayesian-updating-in-a-nutshell" class="slide level2">
<h2>Bayesian Updating in a Nutshell</h2>
<ul>
<li class="fragment">Prior belief → Evidence → Posterior belief</li>
<li class="fragment">Thats it.</li>
</ul>
</section>
<section id="a-cognitive-task" class="slide level2">
<h2>A cognitive task</h2>
<ul>
<li class="fragment">The go-nogo task</li>
<li class="fragment"><img src="/images/gonogo.png" width="800px"></li>
<li class="fragment">You respond (Go) to frequent stimuli…</li>
<li class="fragment">…and withhold response (No-Go) to infrequent ones.</li>
<li class="fragment">Measures response inhibition, impulse control, and attention.</li>
<li class="fragment">Commonly used in motor neuroscience / ADHD / addiction / tests of frontal lobe function…</li>
</ul>
</section>
<section id="estimating-ability-from-behavioral-data" class="slide level2">
<h2>Estimating ability from behavioral data</h2>
<ul>
<li class="fragment">10 trials of equal difficulty</li>
<li class="fragment">Binary outcomes, correct or incorrect</li>
<li class="fragment">We want to estimate ability <span class="math inline">\(\theta\)</span> from behavior</li>
<li class="fragment"><span class="math inline">\(\theta\)</span> is <em>latent</em> which means it is hidden</li>
<li class="fragment">Data are <em>observed</em> which means we can see it</li>
<li class="fragment">e.g., correct responses <span class="math inline">\(k = 8\)</span> out of <span class="math inline">\(n = 10\)</span></li>
<li class="fragment"><em>We will use the same symbols &amp; letters throughout</em></li>
</ul>
</section>
<section id="latent-vs.-observed" class="slide level2">
<h2>Latent vs.&nbsp;observed</h2>
<ul>
<li class="fragment"><img src="/images/latent_observable.png" width="1000px"></li>
</ul>
</section>
<section id="why-latent-variables" class="slide level2">
<h2>Why latent variables?</h2>
<ul>
<li class="fragment">We want to explain, not just describe</li>
<li class="fragment"><em>Descriptive</em>: e.g.&nbsp;“What did the subject score?”</li>
<li class="fragment"><em>Explanatory</em>: e.g.&nbsp;“What ability caused that score?”</li>
</ul>
</section>
<section id="scientitific-questions-pertain-to-the-latent" class="slide level2">
<h2>Scientitific questions pertain to the latent</h2>
<ul>
<li class="fragment">Do parkinsons patients differ in <em>risk taking</em> on and off medication?</li>
<li class="fragment">Does serotonin change <em>empathy</em>?</li>
<li class="fragment">Does alpha waves cause <em>memory consolidation</em>?</li>
<li class="fragment">Does ozempic improve <em>cognitive flexibility</em>?</li>
</ul>
</section>
<section id="science-cares-about-the-latent" class="slide level2">
<h2>Science cares about the latent</h2>
<ul>
<li class="fragment">We observe data, but we want to infer about latent variables</li>
<li class="fragment">Cognitive &amp; brain sciences are ultimately about latent variables</li>
<li class="fragment">Bayesian modeling connects observables to latent processes</li>
</ul>
</section>
<section id="back-to-go-nogo" class="slide level2">
<h2>Back to go-nogo</h2>
<ul>
<li class="fragment"><strong>Observed</strong>: behavior → number correct <span class="math inline">\(k/n\)</span></li>
<li class="fragment"><strong>Latent</strong>: ability → <span class="math inline">\(\theta\)</span></li>
<li class="fragment">There is always uncertainty</li>
<li class="fragment">The same ability can result in different behavior</li>
<li class="fragment">Different ability can result in the same behavior</li>
<li class="fragment">Bayesian inference accounts for this uncertainty</li>
</ul>
</section>
<section id="beliefs-as-distributions" class="slide level2">
<h2>Beliefs as Distributions</h2>
<ul>
<li class="fragment">Probability distributions encode <strong>beliefs</strong></li>
<li class="fragment">The <strong>center</strong> = most likely value</li>
<li class="fragment">The <strong>spread</strong> = uncertainty</li>
<li class="fragment"><code>notebooks/probability_distributions.ipynb</code></li>
<li class="fragment">see “Beliefs as distributions”</li>
</ul>
</section>
<section id="probability-mass-functions-for-discrete-variables" class="slide level2">
<h2>Probability mass functions for discrete variables</h2>
<ul>
<li class="fragment"><img src="/images/probability_mass.jpg" width="350px" style="display:inline"></li>
<li class="fragment">Total mass sums to 1: <span class="math inline">\(\sum_x p(x) = 1\)</span><br>
</li>
<li class="fragment">Range sums: <span class="math inline">\(p(2 \leq x \leq 4) = 0.4\)</span><br>
</li>
<li class="fragment">Odds: <span class="math inline">\(\frac{p(5)}{p(7)} = 7\)</span></li>
</ul>
</section>
<section id="probability-density-functions-for-continuous-variables" class="slide level2">
<h2>Probability density functions for continuous variables</h2>
<ul>
<li class="fragment"><img src="/images/probability_density.jpg" width="350px"></li>
<li class="fragment">All probability density integrates to 1</li>
<li class="fragment">(area under the curve is 1)</li>
<li class="fragment">Densities can exceed 1</li>
<li class="fragment">Ratios make sense: The value “5.5” is 5 times less likely than “0.7”</li>
</ul>
</section>
<section id="probability-density-functions-for-continuous-variables-1" class="slide level2">
<h2>Probability Density Functions for continuous variables</h2>
<ul>
<li class="fragment"><img src="/images/probability_density.jpg" width="350px" style="display:inline"></li>
<li class="fragment">Total area under curve: <span class="math inline">\(\int p(x)\,dx = 1\)</span><br>
</li>
<li class="fragment">Densities can &gt; 1, but only area matters.<br>
</li>
<li class="fragment">Likelihood ratios make sense: e.g., <span class="math inline">\(p(5.5)/p(0.7) = 1/5\)</span></li>
</ul>
</section>
<section id="interpreting-probability-distributions" class="slide level2">
<h2>Interpreting probability distributions</h2>
<ul>
<li class="fragment">It’s important to read and reason with probabilty distributions.</li>
<li class="fragment"><code>notebooks/probability_distributions.ipynb</code></li>
<li class="fragment">see “Interpreting probability distributions”</li>
</ul>
</section>
<section id="bayes-rule" class="slide level2">
<h2>Bayes’ rule</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment">Posterior = <span class="math inline">\(\frac{\color{red}{\text{Likelihood}}  \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment"><span class="math inline">\(\theta\)</span> is a parameter, here “ability” on go-nogo task</li>
<li class="fragment"><span class="math inline">\(D\)</span> is data, here it is the correct performance, <span class="math inline">\(k\)</span> successes out of <span class="math inline">\(n\)</span> trials</li>
<li class="fragment">This tells us how our beliefs about ability are updated by the evidence provided by the data.</li>
</ul>
</section>
<section id="prior" class="slide level2">
<h2>Prior</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment"><strong><span class="math inline">\(\color{blue}{\text{Prior}}\)</span></strong> is what we believe about <span class="math inline">\(\theta\)</span> before seeing the data.</li>
<li class="fragment">It reflects our prior assumptions or knowledge.</li>
</ul>
</section>
<section id="likelihood" class="slide level2">
<h2>Likelihood</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}\)</span></li>
<li class="fragment"><strong><span class="math inline">\(\color{red}{\text{Likelihood}}\)</span></strong> is the probability of data <span class="math inline">\(D\)</span> given a value of <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">It tells us how well each <span class="math inline">\(\theta\)</span> explains the data</li>
<li class="fragment">It also tells us how to update our beliefs about each value of <span class="math inline">\(\theta\)</span></li>
<li class="fragment">Higher likelihood → stronger belief in <span class="math inline">\(\theta\)</span></li>
<li class="fragment">Lower likelihood → weaker belief in <span class="math inline">\(\theta\)</span></li>
</ul>
</section>
<section id="marginal-likelihood" class="slide level2">
<h2>Marginal likelihood</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment"><strong><span class="math inline">\(\color{green}{\text{Marginal likelihood}}\)</span></strong> is the total probability of the data averaged over all possible values of <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">It represents how good the model is at predicting the data.</li>
<li class="fragment">(It also normalizes the posterior so it is a valid probability distribution)</li>
</ul>
</section>
<section id="posterior" class="slide level2">
<h2>Posterior</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment"><strong>Posterior</strong> is what we believe about <span class="math inline">\(\theta\)</span> <em>after</em> seeing the data.</li>
<li class="fragment">It’s the prior that has been updated by the evidence provided by the data.</li>
</ul>
</section>
<section id="recap-in-context-of-go-nogo" class="slide level2">
<h2>Recap in context of go-nogo</h2>
<ul>
<li class="fragment">The <em>prior</em> is our prior belief about ability <span class="math inline">\(p(\theta)\)</span></li>
<li class="fragment">The <em>likelihood</em> is how likely the behavior is under each ability <span class="math inline">\(p(data|\theta)\)</span></li>
<li class="fragment">The <em>posterior</em> is our new belief about ability after observing the behavior <span class="math inline">\(p(\theta|data)\)</span></li>
<li class="fragment">The <em>marginal likelihood</em> is how good in general this model predicts the behaivor <span class="math inline">\(p(data)\)</span></li>
</ul>
</section>
<section id="updating-beliefs-with-data" class="slide level2">
<h2>Updating beliefs with data</h2>
<ul>
<li class="fragment">We start with a <strong>prior</strong> belief <span class="math inline">\(p(\theta)\)</span></li>
<li class="fragment">Observe <strong>data</strong>: e.g.&nbsp;( k = 9 ) correct out of ( n = 10 )</li>
<li class="fragment">Bayes updates the belief:</li>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span></li>
</ul>
</section>
<section id="intuition-behind-belief-updating" class="slide level2">
<h2>Intuition behind belief updating</h2>
<ul>
<li class="fragment">The more likely the data is for a given <span class="math inline">\(\theta\)</span><br>
</li>
<li class="fragment">The more we believe in that value of <span class="math inline">\(\theta\)</span> after seeing the data.</li>
<li class="fragment">The values of <span class="math inline">\(\theta\)</span> that are better supported by the data, are more believed in after experiencing the data</li>
<li class="fragment">We have updated our beliefs according to the data</li>
</ul>
</section>
<section id="proportional-form-of-bayes-rule" class="slide level2">
<h2>Proportional form of Bayes rule</h2>
<ul>
<li class="fragment">Since:<br>
<span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment">The <strong><span class="math inline">\(\color{green}{\text{Marginal likelihood}}\)</span></strong> <span class="math inline">\(\color{green}{p(D)}\)</span> doesn’t depend on <span class="math inline">\(\theta\)</span></li>
<li class="fragment"><br></li>
<li class="fragment">So we can rewrite as:<br>
</li>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) \propto \color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}\)</span></li>
<li class="fragment"><span class="math inline">\(\text{Posterior} \propto \color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}\)</span></li>
<li class="fragment">“<em>Posterior is proportional to likelihood times prior</em>””</li>
</ul>
</section>
<section id="prior-beliefs-for-theta" class="slide level2">
<h2>Prior beliefs for theta</h2>
<ul>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
<li class="fragment">see “Prior beliefs for theta”</li>
</ul>
</section>
<section id="multiplying-prior-and-likelihood" class="slide level2">
<h2>Multiplying prior and likelihood</h2>
<ul>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
<li class="fragment">see “Multiplying prior and likelihood”</li>
</ul>
</section>
<section id="is-the-likelihood-a-probability-distribution" class="slide level2">
<h2>Is the likelihood a probability distribution?</h2>
<ul>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
<li class="fragment">see “Is the likelihood a probability distribution?”<br>
</li>
<li class="fragment">It is if you plot it appropriately.</li>
<li class="fragment">e.g <span class="math inline">\(p(data|theta=0.5)\)</span></li>
<li class="fragment">It is if you plot it over the data rather than theta</li>
</ul>
</section>
<section id="summarising-the-posterior" class="slide level2">
<h2>Summarising the posterior</h2>
<ul>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
<li class="fragment">see “Bayesian credibility intervals”<br>
</li>
<li class="fragment">see “Summarising the posterior”</li>
</ul>
</section>
<section id="compute-the-posterior-for-the-beta" class="slide level2">
<h2>Compute the posterior for the beta</h2>
<ul>
<li class="fragment">Here is an easy way to calculate the posterior</li>
<li class="fragment">We start with a flat prior: <span class="math inline">\(p(\theta) \sim \text{Beta}(1,1)\)</span></li>
<li class="fragment">Observe some data - <span class="math inline">\(k\)</span> = correct, <span class="math inline">\(n\)</span> = total trials</li>
<li class="fragment">Posterior is then: <span class="math inline">\(p(\theta \mid D) \sim \text{Beta}(1 + k, 1 + n - k)\)</span></li>
<li class="fragment">Simple, tractable update rule for binary outcomes</li>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
<li class="fragment">see “Computing posterior for the beta”</li>
</ul>
</section>
<section id="sequential-updating-of-posterior" class="slide level2">
<h2>Sequential updating of posterior</h2>
<ul>
<li class="fragment">Bayesian inference is consistent across steps</li>
<li class="fragment">One-step:
<ul>
<li class="fragment">Prior → Combined data1 &amp; data2 → Posterior</li>
</ul></li>
<li class="fragment">Two-step:
<ul>
<li class="fragment">Prior → Data1 → Intermediate Posterior → Data2 → Final Posterior</li>
</ul></li>
<li class="fragment">Final result is identical</li>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
<li class="fragment">see “Sequential updating”</li>
</ul>
</section>
<section id="why-sequential-updating-of-posterior-matters" class="slide level2">
<h2>Why sequential updating of posterior matters</h2>
<ul>
<li class="fragment">You can peak at your data, it’s ok!</li>
<li class="fragment">Enables inference as data rolls in.</li>
<li class="fragment"><em>Optional stopping</em>: stop early, or extend data collection</li>
<li class="fragment"><em>Efficient</em>: time, money, resources</li>
<li class="fragment"><em>Ethical</em>: minimises animals, humans, unnecessary treatment etc.</li>
</ul>
</section>
<section id="optional-stopping" class="slide level2">
<h2>Optional stopping</h2>
<ul>
<li class="fragment"><ul>
<li class="fragment"><img data-src="images/optional_stopping.png" style="width:100.0%"></li>
</ul></li>
</ul>
</section>
<section id="frequentist-methods-dont-allow-this" class="slide level2">
<h2>Frequentist methods don’t allow this</h2>
<ul>
<li class="fragment">Frequentist inference assumes a fixed sample size and plan</li>
<li class="fragment">Stopping early or collecting more data invalidates p-values</li>
<li class="fragment"><em>Counterfactual policies:</em> what you would have done, if the data had turned out different impacts on your p-values.</li>
<li class="fragment">Not widely understood.</li>
</ul>
</section>
<section id="conjugate-priors" class="slide level2">
<h2>Conjugate priors</h2>
<ul>
<li class="fragment">If Prior and posterior from the same distribution family → <em>conjugate</em><br>
</li>
<li class="fragment"><br></li>
<li class="fragment">For example:</li>
<li class="fragment"><span class="math inline">\(p(\theta): \text{Beta}(\alpha, \beta)\)</span></li>
<li class="fragment"><span class="math inline">\(p(\theta|data): \text{Beta}(\alpha + k, \beta + n - k)\)</span></li>
<li class="fragment"><br></li>
<li class="fragment">The posterior can be computed by plugging data directly into an equation</li>
<li class="fragment">Conjugacy allows for <em>analytic updates</em></li>
</ul>
</section>
<section id="when-conjugacy-isnt-possible-sampling" class="slide level2">
<h2>When conjugacy isnt possible: Sampling</h2>
<ul>
<li class="fragment">Conjugacy is relatively rare in real world cases</li>
<li class="fragment">In cases where conjugacy is not available sampling solutions are possible</li>
<li class="fragment">Commonly MCMC - Markov Chain Monte Carlo</li>
<li class="fragment">Works even when no closed-form solution</li>
<li class="fragment">Approximates the posterior via sampling</li>
</ul>
</section>
<section id="analytic-vs.-sampling" class="slide level2">
<h2>Analytic vs.&nbsp;Sampling</h2>
<ul>
<li class="fragment"><strong>Analytic</strong>: Exact, requires conjugacy, rare</li>
<li class="fragment"><strong>MCMC</strong>: Approximate, doesnt require conjugacy, more flexible, common</li>
</ul>
</section>
<section id="mcmc-in-practice" class="slide level2">
<h2>MCMC in Practice</h2>
<ul>
<li class="fragment"><strong>Red pill</strong>: Learn how MCMC really works 🤓
<ul>
<li class="fragment"><a href="https://chat.openai.com/share/887b4e2d-2689-4e2d-b9ae-246b71d5782e">Intuitive guide to MCMC internals</a><br>
</li>
<li class="fragment"><a href="https://chat.openai.com/share/55cb6a00-1718-4327-8d51-3902a2064a11">Metropolis-Hastings explained simply</a><br>
</li>
<li class="fragment">These methods will keep evolving — expect newer algorithms, faster sampling, and better approximations.</li>
</ul></li>
<li class="fragment"><strong>Blue pill</strong>: Trust the method and use it (but know how to spot when it’s broken) 😄
<ul>
<li class="fragment">We’ll demonstrate and visualise MCMC in practice.</li>
</ul></li>
</ul>
</section>
<section id="mcmc-demo" class="slide level2">
<h2>MCMC demo</h2>
<ul>
<li class="fragment">An easy way to sample the posterior.</li>
<li class="fragment">With enough samples you can get as close to the true posterior as you want.</li>
<li class="fragment">Works for conjugacy cases too.</li>
<li class="fragment">Go to “MCMC”</li>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
</ul>
</section>
<section id="why-bayesian-methods" class="slide level2">
<h2>Why Bayesian Methods?</h2>
<ul>
<li class="fragment"><strong>Principled reasoning</strong>: Probabilistic logic = consistent thinking<br>
</li>
<li class="fragment"><strong>Uncertainty-aware</strong>: Fully models uncertainty<br>
</li>
<li class="fragment"><strong>Latent variables</strong>: Model hidden causes, not just outcomes<br>
</li>
<li class="fragment"><strong>Sequential updating</strong>: Updating is the same whether data is sequential or all-in-one-go.<br>
</li>
<li class="fragment"><strong>Explanations</strong>: From describing data to explaining via theory<br>
</li>
<li class="fragment"><strong>Flexibility</strong>: Hierarchical, generative, extensible</li>
<li class="fragment"><strong>Simple</strong>: Same principle always. Learn it once. Apply it forever.</li>
</ul>
<!-- #endregion-->
<!-- #region Lecture#3 Modelling a binary process-->
</section></section>
<section>
<section id="lecture-3-modeling-a-binary-process" class="title-slide slide level1 center">
<h1>Lecture 3: Modeling a Binary Process</h1>

</section>
<section id="roadmap-2" class="slide level2">
<h2>Roadmap</h2>
<ul>
<li class="fragment">Cogntive tasks with binary outcomes<br>
</li>
<li class="fragment">From observed data to hidden probabilities<br>
</li>
<li class="fragment">Graphical models and their notation<br>
</li>
<li class="fragment">Bayesian inference via sampling in JAGS<br>
</li>
<li class="fragment">Convergence diagnostics and posterior checks</li>
</ul>
</section>
<section id="modeling-a-binary-process" class="slide level2">
<h2>Modeling a Binary Process</h2>
<ul>
<li class="fragment">Start simple: focus on binary outcomes<br>
</li>
<li class="fragment">e.g., <em>Success/Failure</em>, <em>Correct/Incorrect</em>, <em>Yes/No</em></li>
<li class="fragment">Common examples:</li>
<li class="fragment"><em>coin flips, true/false questions, detection tasks, motor responses</em></li>
<li class="fragment">In our go/no-go example, each of the <span class="math inline">\(n\)</span> trials results in either <span class="math inline">\(k\)</span> successes<br>
</li>
<li class="fragment">Binary processes are foundational for modeling cognition</li>
</ul>
</section>
<section id="getting-started" class="slide level2">
<h2>Getting Started</h2>
<ul>
<li class="fragment">Our goal is to infer ability in a go-no-go task</li>
<li class="fragment">We estimate a rate — the hidden probability <span class="math inline">\(\theta\)</span> that a response is correct</li>
<li class="fragment">We represent our uncertainty about <span class="math inline">\(\theta\)</span> as a probability distribution</li>
<li class="fragment">Many cognitive tasks can be modeled this way</li>
</ul>
</section>
<section id="binary-tasks-in-cognitive-science" class="slide level2">
<h2>Binary Tasks in Cognitive Science</h2>
<ul>
<li class="fragment">Go/no-go, stop-signal, 2AFC, task switching<br>
</li>
<li class="fragment">Recognition memory, Stroop, Flanker, oddball detection<br>
</li>
<li class="fragment">Visual search, discrimination tasks, and so on.</li>
</ul>
</section>
<section id="from-binary-outcomes-to-a-hidden-rate" class="slide level2">
<h2>From Binary Outcomes to a Hidden Rate</h2>
<ul>
<li class="fragment">Observe <span class="math inline">\(k\)</span> successes in <span class="math inline">\(n\)</span> trials → compute <span class="math inline">\(\frac{k}{n}\)</span></li>
<li class="fragment">But our interest is in the <em>underlying</em> success rate <span class="math inline">\(\theta\)</span></li>
<li class="fragment">Model: <span class="math inline">\(k \sim \text{Binomial}(\theta, n)\)</span></li>
<li class="fragment"><span class="math inline">\(p(k \mid \theta, n) = \binom{n}{k} \theta^k (1 - \theta)^{n - k}\)</span></li>
<li class="fragment">For a given <span class="math inline">\(\theta\)</span> and <span class="math inline">\(n\)</span>, this is the probability distribtion for <span class="math inline">\(k\)</span></li>
<li class="fragment"><span style="color:lightgray;">Assumes independent, identically distributed (i.i.d.) trials — no history effects</span></li>
</ul>
</section>
<section id="try-it-yourself" class="slide level2">
<h2>Try it Yourself</h2>
<ul>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
<li class="fragment">see “Binomial distribution”</li>
</ul>
</section>
<section id="graphical-models" class="slide level2">
<h2>Graphical Models</h2>
<ul>
<li class="fragment">Graphical models represent probabilistic structure visually</li>
<li class="fragment">Nodes represent variables; edges represent dependencies</li>
<li class="fragment">Child nodes are conditionally dependent on parent nodes</li>
<li class="fragment">This is a simple model of the go-nogo task:</li>
<li class="fragment"><img data-src="images/graphical%20model.png" style="width:30.0%"></li>
</ul>
</section>
<section id="graphical-notation" class="slide level2">
<h2>Graphical Notation</h2>
<ul>
<li class="fragment"><strong>Circular</strong> nodes: continuous variables<br>
</li>
<li class="fragment"><strong>Square</strong> nodes: discrete variables<br>
</li>
<li class="fragment"><br></li>
<li class="fragment"><strong>Shaded</strong> nodes: observed<br>
</li>
<li class="fragment"><strong>Unshaded</strong> nodes: hidden<br>
</li>
<li class="fragment"><br></li>
<li class="fragment"><strong>Single border</strong>: stochastic variable<br>
</li>
<li class="fragment"><strong>Double border</strong>: deterministic relationship derived from others</li>
</ul>
</section>
<section id="graphical-notation-reference" class="slide level2">
<h2>Graphical Notation Reference</h2>
<ul>
<li class="fragment"><img data-src="images/graphical%20lexicon.jpeg" style="width:70.0%"></li>
</ul>
</section>
<section id="graphical-model-quiz" class="slide level2">
<h2>Graphical Model Quiz</h2>
<ul>
<li class="fragment"><img data-src="images/graphical%20model%20test.jpeg" style="width:30.0%"></li>
<li class="fragment">Upper node: What type of variable is this?<br>
</li>
<li class="fragment"><strong>Answer:</strong> Continuous, stochastic, and observed</li>
<li class="fragment">Lower node: What type of variable is this?<br>
</li>
<li class="fragment"><strong>Answer:</strong> Discrete, deterministic, and unobserved</li>
</ul>
</section>
<section id="sampling-via-jags" class="slide level2">
<h2>Sampling via JAGS</h2>
<div class="sourceCode" id="cb4" data-code-line-numbers="0|1|2|3|4|5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href=""></a>model {              <span class="co"># Define the model</span></span>
<span id="cb4-2"><a href=""></a>  theta <span class="op">~</span> dbeta(<span class="dv">1</span>,<span class="dv">1</span>) <span class="co"># Prior: theta follows a uniform beta distribution</span></span>
<span id="cb4-3"><a href=""></a>  k <span class="op">~</span> dbin(theta,n)  <span class="co"># Likelihood: k follows a binomial distribution</span></span>
<span id="cb4-4"><a href=""></a>                     <span class="co"># with parameters theta and n</span></span>
<span id="cb4-5"><a href=""></a>}                    <span class="co"># End of model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li class="fragment"><img data-src="images/graphical%20model.png" style="width:30.0%"></li>
</ul>
</section>
<section id="interpreting-the-graphical-model-for-the-code" class="slide level2">
<h2>Interpreting the graphical model for the code</h2>
<ul>
<li class="fragment"><img data-src="images/graphical%20model.png" style="width:30.0%"></li>
<li class="fragment">Theta is latent, and continuous</li>
<li class="fragment">n is observed and discrete</li>
<li class="fragment">k is observed and discrete</li>
<li class="fragment">Both feed n and k feed in to the likelihood to generate k</li>
</ul>
</section>
<section id="r-hat-as-a-convergence-check" class="slide level2">
<h2>R-hat as a convergence check</h2>
<ul>
<li class="fragment">It’s important to check that the sampling has converged to the stationary distribution.</li>
<li class="fragment">One heuristic is the R-hat statistic:<br>
<span class="math inline">\(\hat{R} = \frac{\text{var}(\text{within-chain})}{\text{var}(\text{across-chain})}\)</span></li>
<li class="fragment">Rule of thumb: <span class="math inline">\(\hat{R}\)</span> should be between 1.00 and 1.01 for convergence.</li>
</ul>
</section>
<section id="inspecting-the-chains" class="slide level2">
<h2>Inspecting the chains</h2>
<ul>
<li class="fragment">the chains of samples should look like <em>hairy catapillars</em></li>
<li class="fragment">like this:</li>
<li class="fragment"><img data-src="images/hairy_caterpillar.png" style="width:60.0%"></li>
</ul>
</section>
<section id="try-it-yourself-1" class="slide level2">
<h2>Try it yourself</h2>
<ul>
<li class="fragment">go to “MCMC convergence checks**</li>
<li class="fragment">📂 <code>notebooks/probability distributions.ipynb</code></li>
</ul>
</section>
<section id="difference-between-two-rates" class="slide level2">
<h2>Difference between two rates</h2>
<ul>
<li class="fragment">Suppose we observe two processes, each producing successes out of trials:</li>
<li class="fragment">Process 1: <span class="math inline">\(k_1\)</span> successes out of <span class="math inline">\(n_1\)</span> trials<br>
</li>
<li class="fragment">Process 2: <span class="math inline">\(k_2\)</span> successes out of <span class="math inline">\(n_2\)</span> trials</li>
<li class="fragment">We assume each is governed by a different underlying rate: <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>.</li>
</ul>
</section>
<section id="estimating-the-difference" class="slide level2">
<h2>Estimating the difference</h2>
<ul>
<li class="fragment">We want to model each rate with a posterior Beta distribution, and we are interested in the difference:</li>
<li class="fragment"><span class="math inline">\(\delta = \theta_1 - \theta_2\)</span></li>
<li class="fragment">This tells us how much more (or less) likely success is in one group compared to the other.</li>
</ul>
</section>
<section id="examples-and-intuition" class="slide level2">
<h2>Examples and intuition</h2>
<ul>
<li class="fragment">Examples:</li>
<li class="fragment">📈 Effect of a drug on performance (<span class="math inline">\(\theta_1\)</span> = treated, <span class="math inline">\(\theta_2\)</span> = control)</li>
<li class="fragment">👶 Performance difference between age groups</li>
<li class="fragment">🧪 Comparison of two algorithms on success rate</li>
<li class="fragment">A positive <span class="math inline">\(\delta\)</span> means group 1 is better; a negative <span class="math inline">\(\delta\)</span> means group 2 is better.</li>
</ul>
</section>
<section id="graphical-model-for-inferring-differences" class="slide level2">
<h2>Graphical model for inferring differences</h2>
<ul>
<li class="fragment"><img data-src="images/fig3.3.jpg" style="width:100.0%"></li>
<li class="fragment">Why is delta double boundary?</li>
<li class="fragment">Because it is completely determined by the two thetas</li>
</ul>
</section>
<section id="jags-code-for-infferring-differences-in-rates" class="slide level2">
<h2>JAGS code for infferring differences in rates</h2>
<div class="sourceCode" id="cb5" data-code-line-numbers="0|1|2|3|4|5|6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href=""></a>model{ </span>
<span id="cb5-2"><a href=""></a>  k1 <span class="op">~</span> dbin(theta1,n1)</span>
<span id="cb5-3"><a href=""></a>  k2 <span class="op">~</span> dbin(theta2,n2)</span>
<span id="cb5-4"><a href=""></a>  theta1 <span class="op">~</span> dbeta(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb5-5"><a href=""></a>  theta2 <span class="op">~</span> dbeta(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb5-6"><a href=""></a>  delta <span class="op">&lt;-</span>theta1<span class="op">-</span>theta2</span>
<span id="cb5-7"><a href=""></a>}                   <span class="co"># End of model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="try-it-yourself-2" class="slide level2">
<h2>Try it yourself</h2>
<ul>
<li class="fragment">go to “Inferring the difference between two rates”<br>
</li>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
</ul>
</section>
<section id="interpret-the-posterior" class="slide level2">
<h2>Interpret the posterior</h2>
<ul>
<li class="fragment">What is the approximate probability that the difference in rates (<span class="math inline">\(\delta\)</span>) is below 0?</li>
<li class="fragment"><img data-src="images/posterior_delta.png" style="width:45.0%"></li>
</ul>
</section>
<section id="inferring-a-common-rate" class="slide level2">
<h2>Inferring a common rate</h2>
<ul>
<li class="fragment">In some cases we want to infer a common rate for 2 different processes</li>
<li class="fragment">e.g.&nbsp;<em>same subject &amp; task, two different sessions</em></li>
<li class="fragment">e.g.&nbsp;<em>same group, different subjects</em></li>
<li class="fragment">e.g.&nbsp;<em>same subject, different tasks</em></li>
<li class="fragment">Here we would model a single <span class="math inline">\(\theta\)</span></li>
<li class="fragment"><img data-src="./images/common_rate_model.jpg"></li>
</ul>
</section>
<section id="same-model-with-plate-notation" class="slide level2">
<h2>Same model with plate notation</h2>
<ul>
<li class="fragment"><img data-src="./images/common_rate_model_plate.jpg"></li>
<li class="fragment">note only one theta, but multiple processes indexed by i</li>
</ul>
</section>
<section id="jags-code-for-inferring-a-common-rate" class="slide level2">
<h2>JAGS code for inferring a common rate</h2>
<div class="sourceCode" id="cb6" data-code-line-numbers="0|1|2|3|4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href=""></a> model{ </span>
<span id="cb6-2"><a href=""></a>  k1 <span class="op">~</span> dbin(theta,n1)</span>
<span id="cb6-3"><a href=""></a>  k2 <span class="op">~</span> dbin(theta,n2)</span>
<span id="cb6-4"><a href=""></a>  theta <span class="op">~</span> dbeta(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb6-5"><a href=""></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li class="fragment"><img data-src="./images/common_rate_model.jpg" style="width:100.0%"></li>
<li class="fragment">Only one <span class="math inline">\(\theta\)</span> is modelling the two sets of data <span class="math inline">\(k1\)</span> and <span class="math inline">\(k2\)</span></li>
</ul>
</section>
<section id="try-it-out" class="slide level2">
<h2>Try it out</h2>
<ul>
<li class="fragment">go to “Inferring a common rate”<br>
</li>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
</ul>
</section>
<section id="predictions" class="slide level2">
<h2>Predictions</h2>
<ul>
<li class="fragment">In Bayesian modeling, everything is about prediction.</li>
<li class="fragment">There are two fundamental axes:
<ol type="1">
<li class="fragment">Are we predicting <strong>parameters</strong> or <strong>data</strong>?</li>
<li class="fragment">Are we predicting <strong>before</strong> or <strong>after</strong> observing data?</li>
</ol></li>
</ul>
</section>
<section id="different-types-of-prediction" class="slide level2">
<h2>Different types of prediction</h2>
<table class="caption-top">
<colgroup>
<col style="width: 14%">
<col style="width: 36%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Before data</strong></th>
<th><strong>After data</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Parameters</strong></td>
<td>Prior distribution: <span class="math inline">\(p(\theta)\)</span></td>
<td>Posterior distribution: <span class="math inline">\(p(\theta \mid d_{\text{obs}})\)</span></td>
</tr>
<tr class="even">
<td><strong>Data</strong></td>
<td>Prior predictive distribution: <span class="math inline">\(p(d_{\text{new}})\)</span></td>
<td>Posterior predictive distribution: <span class="math inline">\(p(d_{\text{new}} \mid d_{\text{obs}})\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="predicting-parameters" class="slide level2">
<h2>Predicting parameters</h2>
<ul>
<li class="fragment">The <strong>prior distribution</strong> <span class="math inline">\(p(\theta)\)</span> is our prediction about the parameter before seeing data.</li>
<li class="fragment">The <strong>posterior distribution</strong> <span class="math inline">\(p(\theta \mid d_\text{obs})\)</span> is our updated prediction after observing data.</li>
</ul>
</section>
<section id="predicting-data." class="slide level2">
<h2>Predicting data.</h2>
<ul>
<li class="fragment">The <strong>prior predictive distribution</strong> <span class="math inline">\(p(d_{\text{new}})\)</span> tells us what data we expect based on our prior belief about <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">The <strong>posterior predictive distribution</strong> <span class="math inline">\(p(d_{\text{new}} \mid d_{\text{obs}})\)</span> predicts new data based on our updated belief.</li>
</ul>
</section>
<section id="todays-posterior-is-tomorrows-prior" class="slide level2">
<h2>Todays posterior is tomorrows prior</h2>
<ul>
<li class="fragment">This means that any posterior can always become a prior for a future prediction, and so on.</li>
</ul>
</section>
<section id="prior-and-posterior-prediction" class="slide level2">
<h2>Prior and posterior prediction</h2>
<div class="sourceCode" id="cb7" data-code-line-numbers="1-3|5-6|8-10|12-13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href=""></a>model {</span>
<span id="cb7-2"><a href=""></a>  <span class="co"># Prior distribution</span></span>
<span id="cb7-3"><a href=""></a>  thetaprior <span class="op">~</span> dbeta(<span class="dv">1</span>,<span class="dv">1</span>) </span>
<span id="cb7-4"><a href=""></a>  </span>
<span id="cb7-5"><a href=""></a>  <span class="co"># Prior predictive distribtion</span></span>
<span id="cb7-6"><a href=""></a>  priorpredk <span class="op">~</span> dbin(thetapior,n) </span>
<span id="cb7-7"><a href=""></a>  </span>
<span id="cb7-8"><a href=""></a>  <span class="co"># Posterior distribution </span></span>
<span id="cb7-9"><a href=""></a>  theta <span class="op">~</span> dbeta(<span class="dv">1</span>,<span class="dv">1</span>) <span class="co"># theta becomes the posterior distribution of   </span></span>
<span id="cb7-10"><a href=""></a>  k <span class="op">~</span> dbin(theta,n) <span class="co">#likelihood for updating prior to posterior</span></span>
<span id="cb7-11"><a href=""></a>  </span>
<span id="cb7-12"><a href=""></a>  <span class="co"># Posterior predictive distribution</span></span>
<span id="cb7-13"><a href=""></a>  postpredk <span class="op">~</span> dbin(theta,n) <span class="co"># posterior predictive distibution</span></span>
<span id="cb7-14"><a href=""></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="samples-of-the-four-distributions" class="slide level2">
<h2>Samples of the four distributions</h2>
<ul>
<li class="fragment"><img data-src="./images/prior_post_predictions.jpg" style="width:60.0%"></li>
<li class="fragment">Prior and posterior distributions are in the space of the parameter</li>
<li class="fragment">Prior and posterior predictive distributions are in space of the data k out of n trials.</li>
</ul>
</section>
<section id="comparing-data-to-the-posterior-predictive-distribution" class="slide level2">
<h2>Comparing data to the posterior predictive distribution</h2>
<ul>
<li class="fragment"><ul>
<li class="fragment"><img data-src="./images/common_rate_model.jpg" style="width:100.0%"></li>
</ul></li>
<li class="fragment">If we estimate the model along with its predictive distributions</li>
<li class="fragment">We can see how this compares to the actual data</li>
</ul>
</section>
<section id="descriptive-adequacy" class="slide level2">
<h2>Descriptive adequacy</h2>
<ul>
<li class="fragment">…means how well does the model describe the data</li>
<li class="fragment">Posterior for this data: k1 = 0, n1 =10 &amp; k2=10, n2 = 10</li>
<li class="fragment"><img data-src="images/posterior_common.jpg" style="width:50.0%"></li>
<li class="fragment">Looks ok, but does it have descriptive adequacy?</li>
</ul>
</section>
<section id="check-posterior-predictive-distribution-against-data" class="slide level2">
<h2>Check posterior predictive distribution against data</h2>
<ul>
<li class="fragment"><img data-src="images/posterior_predictive_common.jpg" style="width:40.0%"></li>
<li class="fragment">X marks the observed data, square size indicates probability</li>
<li class="fragment">The model has poor <em>descriptive adequacy</em>. Why?</li>
<li class="fragment">Its a common rate model, so it predicts the same rate, but the data clearly is better modelled with different rates.</li>
</ul>
</section>
<section id="prediction-forward-and-backward-in-time" class="slide level2">
<h2>Prediction forward and backward in time</h2>
<ul>
<li class="fragment">Prediction usually targets the future — but can also fill in the past<br>
</li>
<li class="fragment">When data are missing, we infer what might have happened<br>
</li>
<li class="fragment">Cognitive models use observed data to uncover hidden causes of past behavior<br>
</li>
<li class="fragment">Inference helps us predict both outcomes and hidden history</li>
<li class="fragment">Bayesian inference works backwards and forwards in time <!-- #endregion --></li>
</ul>
<!-- #region Lecture#4 Latent Mixture models      -->
</section></section>
<section>
<section id="latent-mixture-models" class="title-slide slide level1 center">
<h1>Latent mixture models</h1>
<ul>
<li class="fragment">What are they?</li>
<li class="fragment">How to use them to model mixtures of cognitive processes or traits</li>
<li class="fragment">How to use them to model compare models</li>
</ul>
</section>
<section id="latent-mixture-models-1" class="slide level2">
<h2>Latent mixture models</h2>
<ul>
<li class="fragment">…allow you to model data as coming from a <em>mixture</em> of <em>latent</em> processes.</li>
<li class="fragment">This could be a mixture of cognitive processes, e.g.&nbsp;<em>guessing and trying, attending and not attending, remembering and forgetting.</em></li>
<li class="fragment">Or mixture of states or traits, e.g.&nbsp;<em>depresssed vs.&nbsp;healthy, parkinsons vs.&nbsp;healthy, sleepy vs.&nbsp;awake</em><br>
</li>
<li class="fragment">An indicator variable estimates which mixture of processes generated the data.</li>
<li class="fragment">You can also use these mixture models to compare different models</li>
</ul>
</section>
<section id="example-latent-mixture-model-of-cognitive-test" class="slide level2">
<h2>Example: Latent mixture model of cognitive test</h2>
<ul>
<li class="fragment"><em>“Tryers”</em> have an ability that determines their rate of correct responses.</li>
<li class="fragment"><em>“Guessers”</em> score at chance level (e.g.&nbsp;50%).</li>
<li class="fragment">Each participant belongs to one of the two groups.</li>
<li class="fragment">An <em>indicator</em> variable <span class="math inline">\(z_i\)</span> models <em>guesser</em> or <em>tryer</em>.</li>
</ul>
</section>
<section id="latent-mixture-model-vs.-simpler-model" class="slide level2">
<h2>Latent mixture model vs.&nbsp;simpler model</h2>
<ul>
<li class="fragment"><img data-src="images/lmm.jpg" style="width:50.0%"></li>
<li class="fragment"><img data-src="images/%20binomial%20rate%20model.jpg" style="width:50.0%"></li>
<li class="fragment">Its the same model, <span class="math inline">\(z\)</span> just allows a mix of two different processes that can set the rate <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">Simples.</li>
</ul>
</section>
<section id="latent-mixture-graphical-model" class="slide level2">
<h2>Latent mixture graphical model</h2>
<ul>
<li class="fragment"><img data-src="images/lmm.jpg" style="width:50.0%"></li>
<li class="fragment"><span class="math inline">\(z_i\)</span> is an indicator variable for participant <span class="math inline">\(i\)</span>.</li>
<li class="fragment"><span class="math inline">\(z_i = 0\)</span> → guesser; <span class="math inline">\(z_i = 1\)</span> → tryer.</li>
<li class="fragment"><span class="math inline">\(\psi = 0.5\)</span> is the known chance performance level.</li>
<li class="fragment"><span class="math inline">\(\phi\)</span> is the tryer’s ability (same as <span class="math inline">\(\theta\)</span>).</li>
<li class="fragment">Posterior of <span class="math inline">\(z\)</span> tells us the probability of each subject being either a guessers vs.&nbsp;a tryer.</li>
</ul>
</section>
<section id="jags-code-for-latent-mixture" class="slide level2">
<h2>JAGS code for latent mixture</h2>
<div class="sourceCode" id="cb8" data-code-line-numbers="1-3|4-5|6-8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href=""></a>model {</span>
<span id="cb8-2"><a href=""></a>  <span class="cf">for</span> (i <span class="kw">in</span> <span class="dv">1</span>:p) {</span>
<span id="cb8-3"><a href=""></a>    z[i] <span class="op">~</span> dbern(<span class="fl">0.5</span>)</span>
<span id="cb8-4"><a href=""></a>  }</span>
<span id="cb8-5"><a href=""></a></span>
<span id="cb8-6"><a href=""></a>  psi <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb8-7"><a href=""></a>  phi <span class="op">~</span> dbeta(<span class="dv">1</span>, <span class="dv">1</span>) I(<span class="fl">0.5</span>, <span class="dv">1</span>)</span>
<span id="cb8-8"><a href=""></a></span>
<span id="cb8-9"><a href=""></a>  <span class="cf">for</span> (i <span class="kw">in</span> <span class="dv">1</span>:p) {</span>
<span id="cb8-10"><a href=""></a>    theta[i] <span class="op">&lt;-</span> equals(z[i], <span class="dv">0</span>) <span class="op">*</span> psi <span class="op">+</span> equals(z[i], <span class="dv">1</span>) <span class="op">*</span> phi</span>
<span id="cb8-11"><a href=""></a>    k[i] <span class="op">~</span> dbin(theta[i], n)</span>
<span id="cb8-12"><a href=""></a>  }</span>
<span id="cb8-13"><a href=""></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="interactive-demo" class="slide level2">
<h2>Interactive demo</h2>
<ul>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
<li class="fragment">Let’s simulate data first</li>
<li class="fragment">go to “Simulate guessers and tryers”<br>
</li>
<li class="fragment"><br></li>
<li class="fragment">Then we can run inference on each subject</li>
<li class="fragment">go to “Infer guessers and tryers”</li>
</ul>
<!-- #endregion                             -->
<!-- #region Lecture#5 Model selection            -->
</section></section>
<section>
<section id="model-selection" class="title-slide slide level1 center">
<h1>Model selection</h1>
<ul>
<li class="fragment">Why we need to compare models</li>
<li class="fragment">Simplicity vs.&nbsp;complexity</li>
<li class="fragment">Marginal likelihood as a comparison tool</li>
<li class="fragment">Predictive performance and prior bets</li>
<li class="fragment">Bayesian model comparison intuition</li>
</ul>
</section>
<section id="why-compare-models" class="slide level2">
<h2>Why compare models?</h2>
<ul>
<li class="fragment">Exept for the last section, we’ve mainly focused on single models</li>
<li class="fragment">Science advances by comparing competing explanations</li>
<li class="fragment">“This theory is good” → compared to what?</li>
<li class="fragment">We want to know which theory explains the data <em>better</em></li>
<li class="fragment">This requires comparing models</li>
</ul>
</section>
<section id="simplicity-vs.-complexity" class="slide level2">
<h2>Simplicity vs.&nbsp;complexity</h2>
<ul>
<li class="fragment">“Explain phenomena by the simplest hypothesis that works” — Ptolemy</li>
<li class="fragment">“Avoid unnecessary plurality” — Occam’s razor</li>
<li class="fragment">“Complexity must pay for itself” — Hinton</li>
<li class="fragment">“Minimize free energy” — Friston</li>
<li class="fragment">These ideas all reflect the same principle: balance fit and complexity</li>
</ul>
</section>
<section id="the-bayesian-solution" class="slide level2">
<h2>The Bayesian solution</h2>
<ul>
<li class="fragment">Many methods try to balance fit and complexity</li>
<li class="fragment">Bayesian methods do it naturally</li>
<li class="fragment">Bayes gives us a single number for model quality: marginal likelihood</li>
<li class="fragment">It rewards accuracy…</li>
<li class="fragment">…and penalizes wasted complexity</li>
</ul>
</section>
<section id="conditioning-on-a-model" class="slide level2">
<h2>Conditioning on a model</h2>
<ul>
<li class="fragment">Bayes’ rule always assumes <em>a model</em><br>
</li>
<li class="fragment">But we can easily imagine different models, with different priors or structures</li>
<li class="fragment">Science is filled with competing explanations and models after all<br>
</li>
<li class="fragment">We now ask: which model better predicts the data?<br>
</li>
<li class="fragment">This leads us to the <em>marginal likelihood</em><br>
</li>
<li class="fragment">It’s the probability of the data, <em>given the model</em></li>
</ul>
</section>
<section id="marginal-likelihood-1" class="slide level2">
<h2>Marginal likelihood</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(D \mid M)\)</span> = average predictive performance of model <span class="math inline">\(M\)</span></li>
<li class="fragment">It’s a single number:</li>
<li class="fragment"><em>How well the model predicted the data, on average</em></li>
<li class="fragment">It accounts for <em>all</em> parameter values, weighted by their prior</li>
</ul>
</section>
<section id="analogy-to-betting" class="slide level2">
<h2>Analogy to betting</h2>
<ul>
<li class="fragment">Think of it like your model is betting on which parameter values best predict the data</li>
<li class="fragment">The better your bets, the higher your model’s score</li>
<li class="fragment">The prior is the placing of the bets, and the marginal likelihood is how good those bets paid off.</li>
</ul>
</section>
<section id="marginal-likelihood-in-words" class="slide level2">
<h2>Marginal likelihood in words</h2>
<ul>
<li class="fragment">How probable was the data under this model <span class="math inline">\(M\)</span>?</li>
<li class="fragment">Did the model concentrate its predictions where the data actually were?</li>
<li class="fragment">Priors spread out predictive mass</li>
<li class="fragment">Bad priors waste predictions on wrong areas</li>
<li class="fragment">Good priors focus predictions where the data land</li>
</ul>
</section>
<section id="octopus-example" class="slide level2">
<h2>Octopus example</h2>
<ul>
<li class="fragment">There are two <del>octopi</del> octopusses.</li>
<li class="fragment">Both claim to be paranormal.</li>
<li class="fragment">It’s 1970, so they are both working for the CIA</li>
<li class="fragment"><img data-src="images/octopuses.jpg" style="width:50.0%"></li>
</ul>
</section>
<section id="octopus-predictions" class="slide level2">
<h2>Octopus predictions</h2>
<ul>
<li class="fragment">Where is the Russian sub?</li>
<li class="fragment">Alice: northern hemisphere</li>
<li class="fragment">Bob: Baltic Sea</li>
<li class="fragment">Data: Off the coast of Stockholm</li>
<li class="fragment">Alice was vaguely right</li>
<li class="fragment">Bob was more precisely right → higher marginal likelihood</li>
</ul>
</section>
<section id="how-is-marginal-likelihood-calculated" class="slide level2">
<h2>How is marginal likelihood calculated?</h2>
<ul>
<li class="fragment">It’s the expected likelihood under the predictions of the prior:</li>
<li class="fragment"><span class="math inline">\(p(D \mid M) = \int p(D \mid \theta, M)\, p(\theta \mid M)\, d\theta\)</span></li>
<li class="fragment">For discrete parameters:</li>
<li class="fragment"><span class="math inline">\(p(D \mid M) = \sum p(D \mid \xi_i)\, p(\xi_i)\)</span></li>
<li class="fragment">It’s a weighted average across all parameter settings</li>
</ul>
</section>
<section id="try-it-out-1" class="slide level2">
<h2>Try it out</h2>
<ul>
<li class="fragment">Lets compare alice and bob</li>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
<li class="fragment">“Compare octopusses via marginal likelihood”</li>
</ul>
</section>
<section id="worked-example" class="slide level2">
<h2>Worked example</h2>
<ul>
<li class="fragment">Suppose a model has three parameter values: <span class="math inline">\(\theta = \{0, 0.5, 1\}\)</span></li>
<li class="fragment">Prior probabilities: <span class="math inline">\(p(\theta_1) = \color{blue}{0.6}\)</span>, <span class="math inline">\(p(\theta_2) = \color{blue}{0.3}\)</span>, <span class="math inline">\(p(\theta_3) = \color{blue}{0.1}\)</span></li>
<li class="fragment">Likelihoods: <span class="math inline">\(p(D \mid \theta_1) = \color{red}{0.1}\)</span>, <span class="math inline">\(p(D \mid \theta_2) = \color{red}{0.4}\)</span>, <span class="math inline">\(p(D \mid \theta_3) = \color{red}{0.6}\)</span></li>
<li class="fragment">Marginal likelihood:<br>
<span class="math inline">\(p(D) = \color{blue}{0.6} \cdot \color{red}{0.1} + \color{blue}{0.3} \cdot \color{red}{0.4} + \color{blue}{0.1} \cdot \color{red}{0.6}\)</span></li>
<li class="fragment">Step by step:<br>
<span class="math inline">\(= \color{gray}{0.06} + \color{gray}{0.12} + \color{gray}{0.06} = \boxed{0.24}\)</span></li>
</ul>
</section>
<section id="complexity-and-spread" class="slide level2">
<h2>Complexity and spread</h2>
<ul>
<li class="fragment">More complex models spread their predictions widely</li>
<li class="fragment">This lowers the average likelihood</li>
<li class="fragment">Even if they include the truth, they may assign low probability to it</li>
<li class="fragment">Marginal likelihood punishes this</li>
<li class="fragment">Broad priors = wasted predictions = lower score = lower marginal likelihood</li>
</ul>
</section>
<section id="complexity-is-not-just-parameter-count" class="slide level2">
<h2>Complexity is not just parameter count</h2>
<ul>
<li class="fragment">A model with <em>many</em> narrow priors can be simple</li>
<li class="fragment">A model with <em>one</em> vague prior can be complex</li>
<li class="fragment">Complexity = how broadly a model spreads its predictions</li>
<li class="fragment">Narrow predictive distributions = simpler models</li>
<li class="fragment">Wide, uncertain predictions = complex models</li>
</ul>
</section>
<section id="misconception---sidenote" class="slide level2">
<h2>Misconception - sidenote</h2>
<ul>
<li class="fragment"><span style="color:gray">AIC, BIC penalize complexity by counting parameters</span><br>
</li>
<li class="fragment"><span style="color:gray">But parameter count ≠ true complexity</span><br>
</li>
<li class="fragment"><span style="color:gray">Complexity = how broadly a model spreads its predictions</span></li>
<li class="fragment"><span style="color:gray">Bayesian marginal likelihood captures this automatically</span></li>
</ul>
</section>
<section id="example-prior-vagueness" class="slide level2">
<h2>Example: prior vagueness</h2>
<ul>
<li class="fragment"><span class="math inline">\(\theta \sim \text{Uniform}(0,1)\)</span> → vague → complex</li>
<li class="fragment"><span class="math inline">\(\theta \sim \text{Uniform}(0.5,1)\)</span> → tighter → simpler</li>
<li class="fragment">Both models have one parameter</li>
<li class="fragment">But they differ in how much of the prediction space they cover</li>
<li class="fragment">Complexity depends on how much ground a model tries to cover</li>
</ul>
</section>
<section id="example-prior-vagueness-1" class="slide level2">
<h2>Example: prior vagueness</h2>
<ul>
<li class="fragment"><img data-src="images/complexity.png" style="width:100.0%"></li>
</ul>
</section>
<section id="why-marginal-likelihood-matters" class="slide level2">
<h2>Why marginal likelihood matters</h2>
<ul>
<li class="fragment">It’s the most important quantity in Bayesian model comparison</li>
<li class="fragment">It unifies inference in brain, behavior, science</li>
<li class="fragment">Maximizing it means best average predictive performance</li>
<li class="fragment">It is the quantity that arguably everything else is trying to approximate: variational Bayes, free energy minimisation, ELBO, predictive coding, predictive processing</li>
</ul>
</section>
<section id="why-marginal-likelihood-matters-1" class="slide level2">
<h2>Why marginal likelihood matters</h2>
<ul>
<li class="fragment"><span style="color:gray">Can even argue it is a <em>unique universal maximandum</em> for all physical and adaptive systems</span></li>
<li class="fragment"><span style="color:gray">Woah man, that’s like, deep.</span></li>
<li class="fragment">😵‍💫</li>
</ul>
<!-- #endregion                             -->
<!-- #region Lecture#6 Bayes factors              -->
</section></section>
<section>
<section id="the-bayes-factor" class="title-slide slide level1 center">
<h1>The Bayes factor</h1>
<ul>
<li class="fragment">Compared to what?<br>
</li>
<li class="fragment">From marginal likelihood to relative evidence<br>
</li>
<li class="fragment">Interpreting Bayes factors<br>
</li>
<li class="fragment">Worked example: guessing vs.&nbsp;non-guessing<br>
</li>
<li class="fragment">Pitfalls and philosophical notes</li>
</ul>
</section>
<section id="why-compare-models-1" class="slide level2">
<h2>Why compare models?</h2>
<ul>
<li class="fragment">A model with high marginal likelihood is good — but only <em>relative</em> to alternatives<br>
</li>
<li class="fragment">Absolute goodness is rarely meaningful on its own<br>
</li>
<li class="fragment">The key question is: compared to what?<br>
</li>
<li class="fragment">We want relative evidence — which model explains the data better<br>
</li>
<li class="fragment">The Bayes factor gives us that answer</li>
</ul>
</section>
<section id="compared-to-what" class="slide level2">
<h2>Compared to what?</h2>
<ul>
<li class="fragment"><img data-src="images/oompa.jpg" style="width:100.0%"></li>
</ul>
</section>
<section id="set-a-compared-to-what-alarm-in-your-brain" class="slide level2">
<h2>Set a compared-to-what alarm in your brain</h2>
<ul>
<li class="fragment">Listen out for one-sided superiority / inferiority claims</li>
<li class="fragment"><em>“this theory explains…”</em></li>
<li class="fragment"><em>“this model predicts the data poorly…”</em></li>
<li class="fragment"><em>“this data is unlikely under the null hypothesis…”</em></li>
<li class="fragment"><strong>Ding!</strong> → Compared to what?</li>
</ul>
</section>
<section id="bayes-factor-definition" class="slide level2">
<h2>Bayes factor definition</h2>
<ul>
<li class="fragment">Marginal likelihood: <em>average predictive performance of a model</em><br>
</li>
<li class="fragment">Bayes factor compares this between two models<br>
</li>
<li class="fragment">Defined as the ratio of marginal likelihoods:<br>
<span class="math inline">\(BF_{\color{blue}{1} \color{red}{2}} = \frac{p(D \mid \color{blue}{M_1})}{p(D \mid \color{red}{M_2})}\)</span><br>
</li>
<li class="fragment">Quantifies how much more likely the data is under <span class="math inline">\(\color{blue}{M_1}\)</span> than <span class="math inline">\(\color{red}{M_2}\)</span></li>
</ul>
</section>
<section id="we-already-plotted-bayes-factors" class="slide level2">
<h2>We already plotted Bayes factors</h2>
<ul>
<li class="fragment">Sneakly i didnt tell you they were bayes factors.</li>
<li class="fragment"><img data-src="images/ratio_marginal_likelihoods.png"></li>
</ul>
</section>
<section id="interpreting-the-bayes-factor" class="slide level2">
<h2>Interpreting the Bayes factor</h2>
<ul>
<li class="fragment"><span class="math inline">\(BF_{12} &gt; 1\)</span> → data favors <span class="math inline">\(M_1\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(BF_{12} &lt; 1\)</span> → data favors <span class="math inline">\(M_2\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(BF_{12} = 5\)</span> → data is 5× more likely under <span class="math inline">\(M_1\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(BF_{12} = \frac{1}{5}\)</span> → data is 5× more likely under <span class="math inline">\(M_2\)</span><br>
</li>
<li class="fragment">Strength of evidence depends on how far from 1 the ratio is</li>
</ul>
</section>
<section id="jeffreys-scale" class="slide level2">
<h2>Jeffreys’ scale</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th><span class="math inline">\(BF_{12}\)</span></th>
<th>Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>&gt;100</td>
<td>Extreme evidence for <span class="math inline">\(M_1\)</span></td>
</tr>
<tr class="even">
<td>30–100</td>
<td>Very strong evidence for <span class="math inline">\(M_1\)</span></td>
</tr>
<tr class="odd">
<td>10–30</td>
<td>Strong evidence for <span class="math inline">\(M_1\)</span></td>
</tr>
<tr class="even">
<td>3–10</td>
<td>Moderate evidence for <span class="math inline">\(M_1\)</span></td>
</tr>
<tr class="odd">
<td>1–3</td>
<td>Anecdotal evidence for <span class="math inline">\(M_1\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td>No preference</td>
</tr>
<tr class="odd">
<td>1/3–1</td>
<td>Anecdotal evidence for <span class="math inline">\(M_2\)</span></td>
</tr>
<tr class="even">
<td>1/10–1/3</td>
<td>Moderate evidence for <span class="math inline">\(M_2\)</span></td>
</tr>
<tr class="odd">
<td>1/30–1/10</td>
<td>Strong evidence for <span class="math inline">\(M_2\)</span></td>
</tr>
<tr class="even">
<td>1/100–1/30</td>
<td>Very strong evidence for <span class="math inline">\(M_2\)</span></td>
</tr>
<tr class="odd">
<td>&lt;1/100</td>
<td>Extreme evidence for <span class="math inline">\(M_2\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="try-it-out-2" class="slide level2">
<h2>Try it out</h2>
<ul>
<li class="fragment">see “Bayes factor scale interpretation”</li>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
</ul>
</section>
<section id="example-guessing-vs.-non-guessing" class="slide level2">
<h2>Example: guessing vs.&nbsp;non-guessing</h2>
<ul>
<li class="fragment">9 out of 10 trials correct → <span class="math inline">\(k = 9\)</span>, <span class="math inline">\(n = 10\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(M_1\)</span>: unknown ability → <span class="math inline">\(\theta \sim \text{Uniform}(0,1)\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(M_2\)</span>: guessing → <span class="math inline">\(\theta = 0.5\)</span><br>
</li>
<li class="fragment">Compute marginal likelihood under each model<br>
</li>
<li class="fragment">Compare with a Bayes factor</li>
</ul>
</section>
<section id="bayes-factor-calculation" class="slide level2">
<h2>Bayes factor calculation</h2>
<ul>
<li class="fragment">For <span class="math inline">\(M_1\)</span>: <span class="math inline">\(p(D \mid M_1) = \frac{1}{1+n} = \frac{1}{11} \approx 0.0909\)</span><br>
</li>
<li class="fragment">For <span class="math inline">\(M_2\)</span>: <span class="math inline">\(p(D \mid M_2) = \binom{10}{9} (0.5)^{10} = 10 \cdot 0.000976 = 0.0098\)</span><br>
</li>
<li class="fragment">Bayes factor:<br>
<span class="math inline">\(BF_{12} = \frac{0.0909}{0.0098} \approx 9.3\)</span><br>
</li>
<li class="fragment">Data is about 9× more likely under <span class="math inline">\(M_1\)</span> than <span class="math inline">\(M_2\)</span></li>
</ul>
</section>
<section id="try-it-out-3" class="slide level2">
<h2>Try it out</h2>
<ul>
<li class="fragment">see “Bayes factor calculation step by step”</li>
<li class="fragment"><code>notebooks/probability distributions.ipynb</code></li>
</ul>
</section>
<section id="flipping-the-bf" class="slide level2">
<h2>Flipping the BF</h2>
<ul>
<li class="fragment">If <span class="math inline">\(BF_{12} &lt; 1\)</span>…</li>
<li class="fragment">…take the reciprocal: <span class="math inline">\(BF_{21} = \frac{1}{BF_{12}}\)</span><br>
</li>
<li class="fragment">Keeps interpretation intuitive: how many times more likely is the data?<br>
</li>
<li class="fragment">Example: <span class="math inline">\(BF_{12} = 0.2\)</span> → <span class="math inline">\(BF_{21} = 5\)</span><br>
</li>
<li class="fragment">Now we say: data is 5 × more likely under <span class="math inline">\(M_2\)</span><br>
</li>
<li class="fragment">Same info, more digestible</li>
</ul>
</section>
<section id="bayes-vs.-fisher" class="slide level2">
<h2>Bayes vs.&nbsp;Fisher</h2>
<ul>
<li class="fragment">Bayes compares models<br>
</li>
<li class="fragment">Fisherian methods tests a single null hypothesis<br>
</li>
<li class="fragment">p-values ask: “how unlikely is this data under <span class="math inline">\(H_0\)</span>?”<br>
</li>
<li class="fragment">Bayes factors ask: “which model better explains the data?”<br>
</li>
<li class="fragment">Evidence is always comparative: <span class="math inline">\(p(\text{data} \mid \text{A})\)</span> vs.&nbsp;<span class="math inline">\(p(\text{data} \mid \text{B})\)</span></li>
</ul>
</section>
<section id="critiques" class="slide level2">
<h2>Critiques</h2>
<ul>
<li class="fragment"><p><strong>Criticism:</strong> <em>Bayes factors are sensitive to prior choice</em><br>
</p></li>
<li class="fragment"><p><strong>Answer:</strong> This is a <strong>feature</strong> — priors are part of the model.</p></li>
<li class="fragment"><p><strong>Criticism:</strong> <em>But I don’t want my conclusions to depend so much on the prior</em><br>
</p></li>
<li class="fragment"><p><strong>Answer:</strong> Then use <strong>sensitivity analysis</strong> to check robustness.</p></li>
<li class="fragment"><p><strong>Criticism:</strong> <em>BFs be high if one bad model is much worse than another</em><br>
</p></li>
<li class="fragment"><p><strong>Answer:</strong> True. Check <strong>descriptive adequacy</strong> — look at <strong>posterior predictive distributions</strong>, simulate data, or compare out-of-sample predictions.</p></li>
</ul>
</section>
<section id="the-arc-of-civilisation" class="slide level2">
<h2>The arc of civilisation</h2>
<ul>
<li class="fragment"><p>Oppposable thumbs, fire, the wheel, writing, zero, the printing press, Newtonian physics, germ theory, the steam engine, the combustion engine, the Moon landing, In Rainbows, the internet, CRISPR, Bayes factors</p></li>
<li class="fragment"><p><sub><span style="color:gray"><em>I’m being satirical (kinda).</em></span></sub></p></li>
</ul>
<!-- #endregion                             -->
<!-- #region Lecture#7 Model probabilities -->
</section></section>
<section>
<section id="lecture-7-model-probabilities" class="title-slide slide level1 center">
<h1>Lecture 7 Model probabilities</h1>

</section>
<section id="roadmap-3" class="slide level2">
<h2>Roadmap</h2>
<ul>
<li class="fragment">Model probabilities</li>
<li class="fragment">Prior model probability</li>
<li class="fragment">Marginal likelihood</li>
<li class="fragment">Posterior model probability</li>
<li class="fragment">Odds and model comparison</li>
<li class="fragment">From prior odds to posterior odds</li>
<li class="fragment">Conceptual and computational challenges</li>
</ul>
</section>
<section id="model-probabilities" class="slide level2">
<h2>Model probabilities</h2>
<ul>
<li class="fragment">Often we want to know which model to believe in<br>
</li>
<li class="fragment">As Bayesians, we assign prior probabilities to models<br>
</li>
<li class="fragment">Then update these into posterior model probabilities<br>
</li>
<li class="fragment">Updating is based on how well each model predicts the data</li>
</ul>
</section>
<section id="prior-model-probability" class="slide level2">
<h2>Prior model probability</h2>
<ul>
<li class="fragment">Assign prior probabilities to models: <span class="math inline">\(p(m_1)\)</span>, <span class="math inline">\(p(m_2)\)</span> …<br>
</li>
<li class="fragment">Reflects belief before seeing any data<br>
</li>
<li class="fragment">Must sum to 1 across the model space <span class="math inline">\(M\)</span><br>
</li>
<li class="fragment">Can be informed by prior knowledge or data…<br>
</li>
<li class="fragment">…or set uniformly if agnostic</li>
</ul>
</section>
<section id="marginal-likelihood-2" class="slide level2">
<h2>Marginal likelihood</h2>
<ul>
<li class="fragment">Measures average predictive performance of a model<br>
</li>
<li class="fragment">e.g.&nbsp;for a model <span class="math inline">\(m_1\)</span>:<br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid m_1) = \int p(D \mid \theta, m_1)\,p(\theta \mid m_1)\,d\theta\)</span><br>
</li>
<li class="fragment">Rewards models that predict the data well<br>
</li>
<li class="fragment">Penalizes models that spread probability too widely</li>
</ul>
</section>
<section id="posterior-model-probability" class="slide level2">
<h2>Posterior model probability</h2>
<ul>
<li class="fragment">Probability of model after seeing data<br>
</li>
<li class="fragment">Bayes’ rule for models:<br>
</li>
<li class="fragment"><span class="math inline">\(p(m_1 \mid D) = \frac{p(D \mid m_1)\,p(m_1)}{p(D \mid M)}\)</span><br>
</li>
<li class="fragment"><br></li>
<li class="fragment">Note: The denominator is the marginal likelihood of the model space:<br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid M) = \sum_{m \in M} p(D \mid m)\,p(m)\)</span></li>
</ul>
</section>
<section id="posterior-model-probability-1" class="slide level2">
<h2>Posterior model probability</h2>
<ul>
<li class="fragment">Probability of model after seeing data<br>
</li>
<li class="fragment">Bayes’ rule for models:<br>
</li>
<li class="fragment"><br></li>
<li class="fragment"><span class="math inline">\(p(m_1 \mid D) = \frac{p(D \mid m_1)\,p(m_1)}{p(D \mid M)}\)</span><br>
</li>
<li class="fragment"><br></li>
<li class="fragment"><span style="color:gray;"> Note: The denominator is the marginal likelihood of the model space <span class="math inline">\(M\)</span>:<br>
<span class="math display">\[
p(D \mid M) = \sum_{m \in M} p(D \mid m)\,p(m)
\]</span> </span></li>
</ul>
</section>
<section id="odds" class="slide level2">
<h2>Odds</h2>
<ul>
<li class="fragment">Odds express the ratio of one probability relative to another<br>
</li>
<li class="fragment">If probability of success is p = 0.75</li>
<li class="fragment">…and probability of failure is then p = 0.25 then:<br>
<span class="math inline">\(\text{odds} = \frac{0.75}{.25} = 3\)</span><br>
</li>
<li class="fragment">“Three to one” odds means success is three times more likely than failure</li>
</ul>
</section>
<section id="prior-odds" class="slide level2">
<h2>Prior odds</h2>
<ul>
<li class="fragment">Compares plausibility of two models before seeing data<br>
</li>
<li class="fragment">Defined as: <span class="math inline">\(\text{Prior odds} = \frac{p(m_1)}{p(m_2)}\)</span><br>
</li>
<li class="fragment">Encodes how much you believe in <span class="math inline">\(m_1\)</span> relative to <span class="math inline">\(m_2\)</span> a priori</li>
</ul>
</section>
<section id="posterior-odds" class="slide level2">
<h2>Posterior odds</h2>
<ul>
<li class="fragment">Compares plausibility of two models after seeing data<br>
</li>
<li class="fragment">Defined as: <span class="math inline">\(\text{Posterior odds} = \frac{p(m_1 \mid D)}{p(m_2 \mid D)}\)</span><br>
</li>
<li class="fragment">Reflects relative belief in one model over another after the data</li>
</ul>
</section>
<section id="from-prior-odds-to-posterior-odds" class="slide level2">
<h2>From prior odds to posterior odds</h2>
<ul>
<li class="fragment">Bayes’ rule for model comparison (in odds form):</li>
<li class="fragment"><span class="math display">\[
\frac{p(m_1)}{p(m_2)} \times \frac{p(D \mid m_1)}{p(D \mid m_2)} = \frac{p(m_1 \mid D)}{p(m_2 \mid D)}
\]</span><br>
</li>
<li class="fragment">Prior odds × Bayes factor = Posterior odds</li>
</ul>
</section>
<section id="simples" class="slide level2">
<h2>Simples!</h2>
<ul>
<li class="fragment"><img data-src="images/simples.jpeg" style="width:2000.0%"></li>
</ul>
</section>
<section id="posterior-odds-example" class="slide level2">
<h2>Posterior odds example</h2>
<ul>
<li class="fragment"><img data-src="images/posterior_odds_rain.png"></li>
</ul>
</section>
<section id="posterior-odds-schematic" class="slide level2">
<h2>Posterior odds schematic</h2>
<ul>
<li class="fragment"><img data-src="images/posterior_odds_eq.png"></li>
</ul>
</section>
<section id="try-it-out-4" class="slide level2">
<h2>Try it out</h2>
<ul>
<li class="fragment">Demo brings it all together<br>
</li>
<li class="fragment">See: <code>notebooks/probability distributions.ipynb</code><br>
→ “Posterior odds calculation”</li>
</ul>
</section>
<section id="conceptual-and-computational-challenges" class="slide level2">
<h2>Conceptual and computational challenges</h2>
<ul>
<li class="fragment">Priors affect inference — vague ≠ safe<br>
</li>
<li class="fragment">Marginal likelihoods are often hard to compute<br>
</li>
<li class="fragment">Complex models often need approximations</li>
</ul>
</section>
<section id="specifying-model-priors" class="slide level2">
<h2>Specifying model priors</h2>
<ul>
<li class="fragment">Model priors should reflect genuine uncertainty<br>
</li>
<li class="fragment">Two strategies:
<ul>
<li class="fragment">Subjective (expert-informed)<br>
</li>
<li class="fragment">Objective (e.g., unit-information)<br>
</li>
</ul></li>
<li class="fragment">Objective priors support reproducibility<br>
</li>
<li class="fragment">Subjective priors offer flexibility when justified</li>
</ul>
</section>
<section id="common-confusion-about-priors" class="slide level2">
<h2>Common confusion about priors</h2>
<ul>
<li class="fragment">Model priors ≠ parameter priors<br>
</li>
<li class="fragment">Be clear what you are talking about with “priors”</li>
</ul>
</section>
<section id="what-priors-influence-what" class="slide level2">
<h2>What priors influence what</h2>
<ul>
<li class="fragment">Model priors → Prior odds</li>
<li class="fragment"><em>Since prior odds are composed of model priors</em></li>
<li class="fragment"><br></li>
<li class="fragment">Parameter priors → Bayes factor</li>
<li class="fragment"><em>Parameter priors infuence how good each model predicts data</em></li>
<li class="fragment"><br></li>
<li class="fragment">Both model AND parameter priors → posterior odds</li>
<li class="fragment"><em>Since Posterior odds = prior odds </em> BF*</li>
</ul>
</section>
<section id="prior-sensitivity" class="slide level2">
<h2>Prior sensitivity</h2>
<ul>
<li class="fragment">Different priors can lead to different conclusions based on the posterior<br>
</li>
<li class="fragment">This is not a flaw — it’s a feature of honest uncertainty<br>
</li>
<li class="fragment">Sensitivity analysis tests how robust your conclusions are<br>
</li>
<li class="fragment">Try wider/narrower priors and compare results<br>
</li>
<li class="fragment">Some models are robust; others are fragile</li>
</ul>
</section>
<section id="belief-updating-for-models-vs.-parameters" class="slide level2">
<h2>Belief updating for models vs.&nbsp;parameters</h2>
<ul>
<li class="fragment"><p>Posterior probability of parameters (within a model, m):<br>
<span class="math display">\[
p(\theta \mid D, m) = \frac{\color{red}{p(D \mid \theta, m)} \cdot \color{blue}{p(\theta \mid m)}}{\color{green}{p(D \mid m)}}
\]</span></p></li>
<li class="fragment"><p>Posterior probability of models m (within a model space, M):<br>
<span class="math display">\[
p(m \mid D) = \frac{\color{green}{p(D \mid m)} \cdot \color{blue}{p(m)}}{\color{gray}{p(D \mid M)}}
\]</span></p></li>
<li class="fragment"><p><span style="color:gray;"> Note: First denominator is for a single model m, second sums over the full space of possible models, M {m1, m2…}. </span></p></li>
</ul>
</section>
<section id="computational-solutions" class="slide level2">
<h2>Computational solutions</h2>
<ul>
<li class="fragment">Marginal likelihoods are rarely available in closed form<br>
</li>
<li class="fragment">Approximate inference methods include:
<ul>
<li class="fragment">Variational Bayes<br>
</li>
<li class="fragment">Free energy minimisation<br>
</li>
<li class="fragment">MCMC (like we’ve used)</li>
</ul></li>
</ul>
<!-- #endregion -->
<!-- #region Misc lectures -->
<!-- #region Lecture#9 Savage Dickey -->
</section></section>
<section>
<section id="savage-dickkey-method-of-model-comparison" class="title-slide slide level1 center">
<h1>Savage-Dickkey method of model comparison</h1>
<ul>
<li class="fragment">In this method two models are compared:
<ul>
<li class="fragment"><strong>Null hypothesis (<span class="math inline">\(H_0\)</span>)</strong>: fixes parameter to a specific value, e.g., <span class="math inline">\(\phi = \phi_0\)</span></li>
<li class="fragment"><strong>Alternative hypothesis (<span class="math inline">\(H_1\)</span>)</strong>: parameter free to vary, e.g., <span class="math inline">\(\phi \ne \phi_0\)</span></li>
</ul></li>
<li class="fragment"><span class="math inline">\(H_0\)</span> is nested within <span class="math inline">\(H_1\)</span> (by constraining parameter).</li>
<li class="fragment">Classical null hypothesis usually sharp (point-null).</li>
</ul>
</section>
<section id="savagedickey-density-ratio" class="slide level2">
<h2>Savage–Dickey Density Ratio</h2>
<ul>
<li class="fragment">Defines Bayes factor for nested models: <span class="math inline">\(BF_{01} = \frac{p(D \mid H_0)}{p(D \mid H_1)} = \frac{p(\phi = \phi_0 \mid D, H_1)}{p(\phi = \phi_0 \mid H_1)}\)</span></li>
<li class="fragment">Simply the ratio of posterior to prior densities at the point of interest <span class="math inline">\(\phi_0\)</span> under the alternative hypothesis.</li>
</ul>
</section>
<section id="example-binomial-scenario" class="slide level2">
<h2>Example: Binomial Scenario</h2>
<ul>
<li class="fragment">Binomial scenario: <span class="math inline">\(\theta\)</span> parameter, observing 9 correct and 1 incorrect response.</li>
<li class="fragment">Null hypothesis (<span class="math inline">\(H_0\)</span>): <span class="math inline">\(\theta = 0.5\)</span></li>
<li class="fragment">Alternative hypothesis (<span class="math inline">\(H_1\)</span>): <span class="math inline">\(\theta\)</span> free to vary, prior <span class="math inline">\(\theta \sim Beta(1,1)\)</span></li>
<li class="fragment">Bayes factor is the ratio of posterior and prior densities at <span class="math inline">\(\theta=0.5\)</span></li>
</ul>
</section>
<section id="visual-interpretation-of-savage-dickey" class="slide level2">
<h2>Visual Interpretation of Savage-Dickey</h2>
<ul>
<li class="fragment"><img data-src="Fig7.1-example.jpg"></li>
<li class="fragment">Prior (uniform) and posterior distributions shown.</li>
<li class="fragment">Density ratio at <span class="math inline">\(\theta=0.5\)</span> gives Bayes factor.</li>
</ul>
</section>
<section id="mcmc-based-estimation-for-savage-dickey" class="slide level2">
<h2>MCMC-Based Estimation for Savage-Dickey</h2>
<ul>
<li class="fragment">When analytical solutions are difficult, use MCMC: -<img data-src="Fig7.2-MCMC-example.jpg"></li>
<li class="fragment">Posterior and prior estimated from MCMC samples.</li>
<li class="fragment">Heights of posterior and prior at the null point give Bayes factor.</li>
</ul>
</section>
<section id="advantages-of-savagedickey" class="slide level2">
<h2>Advantages of Savage–Dickey</h2>
<ul>
<li class="fragment">Direct interpretation as density ratio.</li>
<li class="fragment">Simplifies computation —no separate marginal likelihood calculation needed.</li>
<li class="fragment">Works well for nested models. <!-- #endregion    --></li>
</ul>
<!-- #region Lecture#10 Compare guassian means    -->
</section></section>
<section>
<section id="commpare-gaussian-means" class="title-slide slide level1 center">
<h1>Commpare Gaussian means</h1>
<ul>
<li class="fragment">Common task: test if two Gaussian means differ</li>
<li class="fragment">Example: does glucose improve detection performance</li>
<li class="fragment">Focus: test claim that glucose boost has larger effect in summer</li>
</ul>
</section>
<section id="data" class="slide level2">
<h2>Data</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Season</th>
<th>N</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Winter</td>
<td>41</td>
<td>0.11</td>
<td>0.15</td>
</tr>
<tr class="even">
<td>Summer</td>
<td>41</td>
<td>0.07</td>
<td>0.23</td>
</tr>
</tbody>
</table>
<ul>
<li class="fragment">Difference not significant<br>
</li>
<li class="fragment">t = 0.79, p = 0.44</li>
</ul>
</section>
<section id="p-values-and-the-null-hypothesis" class="slide level2">
<h2>p-values and the Null Hypothesis</h2>
<ul>
<li class="fragment">“From a null result, we cannot conclude that no difference exists…”</li>
<li class="fragment">p = 0.44 does not support H₀</li>
<li class="fragment">It just means data are not incompatible with H₀</li>
<li class="fragment">Need a Bayes factor to quantify support for H₀</li>
</ul>
</section>
<section id="bayes-factor-overview" class="slide level2">
<h2>Bayes Factor Overview</h2>
<ul>
<li class="fragment">Bayes factor compares posterior vs prior odds</li>
<li class="fragment">Quantifies evidence for or against H₀</li>
<li class="fragment">Unlike p-values, can support H₀</li>
</ul>
</section>
<section id="one-sample-comparison-model" class="slide level2">
<h2>One-Sample Comparison Model</h2>
<ul>
<li class="fragment">Test standardized difference scores (e.g., winter - summer)</li>
<li class="fragment">Assume:
<ul>
<li class="fragment">δ ~ Cauchy(0,1)</li>
<li class="fragment">xᵢ ~ Gaussian(μ, 1/σ²)</li>
<li class="fragment">μ = δσ</li>
</ul></li>
</ul>
</section>
<section id="one-sample-graphical-model" class="slide level2">
<h2>One-Sample Graphical Model</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/graphical_model_onesample.png"></p>
<figcaption>Fig 8.1</figcaption>
</figure>
</div></li>
<li class="fragment">Prior on δ: Cauchy(0,1)</li>
<li class="fragment">Prior on σ: Half-Cauchy</li>
<li class="fragment">Estimate posterior with MCMC</li>
</ul>
</section>
<section id="posterior-vs-prior" class="slide level2">
<h2>Posterior vs Prior</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/posterior_prior_onesample.png"></p>
<figcaption>Figure 8.2</figcaption>
</figure>
</div></li>
<li class="fragment">Posterior peaks near δ = 0</li>
<li class="fragment">Bayes Factor ≈ 5:1 in favor of H₀</li>
</ul>
</section>
<section id="order-restricted-model" class="slide level2">
<h2>Order-Restricted Model</h2>
<ul>
<li class="fragment">SMM predicts <strong>δ &lt; 0</strong></li>
<li class="fragment">Use order-restricted prior:
<ul>
<li class="fragment">δ ~ Cauchy(0,1) truncated to (-∞, 0)</li>
</ul></li>
</ul>
</section>
<section id="updated-bayes-factor" class="slide level2">
<h2>Updated Bayes Factor</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/posterior_prior_orderrestricted.png"></p>
<figcaption>Figure 8.4</figcaption>
</figure>
</div></li>
<li class="fragment">Stronger evidence for H₀: BF ≈ 10:1</li>
</ul>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">p-values can’t confirm H₀</li>
<li class="fragment">Bayes factors can</li>
<li class="fragment">“Evidence of absence” of support for SMM’s prediction</li>
</ul>
</section>
<section id="two-sample-comparison" class="slide level2">
<h2>Two-Sample Comparison</h2>
<ul>
<li class="fragment">Compare oxygenated vs plain water on memory</li>
<li class="fragment">Two independent groups</li>
</ul>
</section>
<section id="two-sample-model-structure" class="slide level2">
<h2>Two-Sample Model Structure</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/graphical_model_twosample.png"></p>
<figcaption>Figure 8.5</figcaption>
</figure>
</div></li>
<li class="fragment">Shared variance σ²</li>
<li class="fragment">δ = α / σ<br>
</li>
<li class="fragment">α = μₓ - μᵧ</li>
</ul>
</section>
<section id="large-effect-example" class="slide level2">
<h2>Large Effect Example</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Group</th>
<th>N</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Plain Water</td>
<td>20</td>
<td>68.35</td>
<td>6.38</td>
</tr>
<tr class="even">
<td>Oxygenated</td>
<td>20</td>
<td>76.65</td>
<td>4.06</td>
</tr>
<tr class="odd">
<td>t(38) = 4.47, p &lt; .01</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="two-sample-bayes-factor" class="slide level2">
<h2>Two-Sample Bayes Factor</h2>
<p>-<img data-src="figures/posterior_prior_twosample.png" alt="Figure 8.6"> - Posterior moves away from 0 - BF ≈ 447:1 in favor of H₁<br>
- Decisive evidence for oxygenated water effect <!-- #endregion   --></p>
<!-- #region Lecture#11 Compare binomial rates    -->
</section></section>
<section>
<section id="comparing-binomial-rates" class="title-slide slide level1 center">
<h1>Comparing binomial rates</h1>
<ul>
<li class="fragment">We will naturally compute binomial rates for different groups or conditions</li>
<li class="fragment">And ask which is larger?</li>
<li class="fragment">We thus need to compare binomial rates and test hypotheses about which is bigger etc.</li>
</ul>
</section>
<section id="bayesian-graphical-model" class="slide level2">
<h2>Bayesian graphical model</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/Fig9.1.png"></p>
<figcaption>Figure 9.1</figcaption>
</figure>
</div></li>
<li class="fragment">graphical model for comparing two proportions</li>
</ul>
</section>
<section id="bayesian-model" class="slide level2">
<h2>Bayesian model</h2>
<ul>
<li class="fragment">We model the observed counts using binomial likelihoods and assign uniform Beta priors: s1 ~ Binomial(theta1, n1)<br>
s2 ~ Binomial(theta2, n2)<br>
theta1 ~ Beta(1, 1)<br>
theta2 ~ Beta(1, 1)<br>
delta &lt;- theta1 - theta2</li>
<li class="fragment">theta1:</li>
<li class="fragment">theta2:</li>
<li class="fragment">delta = theta1 - theta2: difference in proportions</li>
<li class="fragment">We are interested in the posterior distribution of delta.</li>
</ul>
</section>
<section id="model-code" class="slide level2">
<h2>Model Code</h2>
<p>Here is the model used for posterior simulation: model { theta1 ~ dbeta(1,1) theta2 ~ dbeta(1,1) delta &lt;- theta1 - theta2 s1 ~ dbin(theta1, n1) s2 ~ dbin(theta2, n2) theta1prior ~ dbeta(1,1) theta2prior ~ dbeta(1,1) deltaprior &lt;- theta1prior - theta2prior } This allows us to compare the prior and posterior density of delta at zero.</p>
</section>
<section id="prior-and-posterior-distributions" class="slide level2">
<h2>Prior and Posterior Distributions</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/Fig9.2.png"></p>
<figcaption>Figure 9.2</figcaption>
</figure>
</div></li>
<li class="fragment">We estimate the posterior distribution for the rate difference delta = theta1 - theta2 using Bayesian inference.</li>
<li class="fragment">The left plot shows prior and posterior distributions for delta across its full range.</li>
<li class="fragment">The right plot zooms in near delta = 0.</li>
<li class="fragment">This is used in the Savage–Dickey density ratio to compute the Bayes factor.</li>
<li class="fragment">The Savage–Dickey method compares: BF_01 = prior density at delta = 0 / posterior density at delta = 0</li>
</ul>
</section>
<section id="interpreting-the-bayes-factor-1" class="slide level2">
<h2>Interpreting the Bayes Factor</h2>
<ul>
<li class="fragment">The posterior density at delta = 0 is about half the prior density.</li>
<li class="fragment">This gives a Bayes factor ≈ 2 in favor of the alternative hypothesis H1: delta ≠ 0.</li>
<li class="fragment">The 95% credible interval for delta is approximately [-0.09, 0.01], which does not include 0.</li>
</ul>
<p><strong>Interpretation</strong>: - There is only modest evidence that one rate is higher than another - The Bayes factor penalizes H1 for spreading prior mass over implausible values. <!-- #endregion    --></p>
<!-- #region Lecture#4 Inference with Gaussians   -->
</section></section>
<section>
<section id="inferences-with-gaussians" class="title-slide slide level1 center">
<h1>Inferences with Gaussians</h1>
<ul>
<li class="fragment">Due to central limit theorem, data and parameters are frequently Gaussian</li>
<li class="fragment">Gaussians have 2 parameters, a mean and a measure of their spread</li>
<li class="fragment">Spread can be expressed as a variance, std, or a precision (1/var)</li>
</ul>
</section>
<section id="graphical-model-for-gaussians" class="slide level2">
<h2>Graphical model for Gaussians</h2>
<ul>
<li class="fragment">Simple model for inferring Gaussian with unknown mean and std</li>
<li class="fragment"><img data-src="fig.4.1.jpg"></li>
</ul>
</section>
<section id="interactive-demo-of-gaussian" class="slide level2">
<h2>Interactive demo of Gaussian</h2>
<ul>
<li class="fragment">Jupyter notebook - interactive plotting of gaussian</li>
</ul>
</section>
<section id="sampling-model-for-inferring-gaussians" class="slide level2">
<h2>Sampling model for inferring Gaussians</h2>
<p>model { for (i in 1:n){ x[i]~dnorm(mu,lambda) } mu~dnorm(0,0.001) sigma~dunif(0,10) lambda&lt;-1/pow(sigma,2) }</p>
</section>
<section id="repeated-measures-of-iq" class="slide level2">
<h2>Repeated measures of IQ</h2>
<ul>
<li class="fragment">Imagine taking a cognitive test like IQ multiple times</li>
<li class="fragment">The mean is your IQ, and the spread models fluctuations in your performance. e.g.&nbsp;attention, fatigue, emotion, venus orbitting satturn</li>
<li class="fragment">We can model this as a Gaussian for each person</li>
</ul>
</section>
<section id="graphical-model-for-iq" class="slide level2">
<h2>Graphical model for IQ</h2>
<ul>
<li class="fragment"><img data-src="fig.4.3.jpg"></li>
<li class="fragment">What parameter is common to all subjects?</li>
<li class="fragment">No index on the std. This means it is fixed.</li>
<li class="fragment">Is this justified?</li>
<li class="fragment">How to change it?</li>
</ul>
</section>
<section id="sampling-code-for-iq" class="slide level2">
<h2>Sampling code for IQ</h2>
<p>model{ for (i in 1:n) { for (j in 1:m) { x[i,j]~dnorm(mu[i],lambda) } } sigma~dunif(0,100) lambda &lt;-1/pow(sigma,2) for (i in 1:n) { mu([i]~ dunif(0,300)) }</p>
<p>} <!-- #endregion                            --></p>
<!-- #endregion -->

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://tinyurl.com/bayeBinderRepo" target="_blank"> tinyurl.com/bayesBinderRepo </a>&nbsp;&nbsp;&nbsp; <a href="https://tinyurl.com/bayesGithub" target="_blank"> tinyurl.com/bayesGithub </a>&nbsp;&nbsp;</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="BayesianModels_slides_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="BayesianModels_slides_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>