<!DOCTYPE html>
<html lang="en"><head>
<script src="BayesianModels_slides_files/libs/clipboard/clipboard.min.js"></script>
<script src="BayesianModels_slides_files/libs/quarto-html/tabby.min.js"></script>
<script src="BayesianModels_slides_files/libs/quarto-html/popper.min.js"></script>
<script src="BayesianModels_slides_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="BayesianModels_slides_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="BayesianModels_slides_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="BayesianModels_slides_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.42">

  <meta name="author" content="Ollie Hulme">
  <title>Bayesian Models of Brains, Minds, &amp; Behaviors</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="BayesianModels_slides_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="BayesianModels_slides_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="BayesianModels_slides_files/libs/revealjs/dist/theme/quarto-2f366650f320edcfcf53d73c80250a32.css">
  <link rel="stylesheet" href="BayesianModels_styles.css">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Bayesian Models of Brains, Minds, &amp; Behaviors</h1>
  <p class="subtitle">Hvidovre Hospital ¬∑ Copenhagen ¬∑ May 2025</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Ollie Hulme 
</div>
</div>
</div>

</section>
<section class="slide level2">

<!-- #region Affiliation -->
</section>
<section>
<section id="affiliations" class="title-slide slide level1 center">
<h1>Affiliations</h1>
<ul>
<li class="fragment"><img data-src="images/DRCMR_regionH_KU_logo.png"><br>
<!-- #endregion --> <!-- #region Lecture#1 Preamble --> # Preamble</li>
<li class="fragment">Course materials<br>
</li>
<li class="fragment">Schedule<br>
</li>
<li class="fragment">Social üòä<br>
</li>
<li class="fragment">Aims &amp; expectations</li>
</ul>
</section>
<section id="interactive-exercises" class="slide level2">
<h2>Interactive exercises</h2>
<ul>
<li class="fragment">Run interactive python code via Binder üëá<br>
</li>
<li class="fragment">No installation needed ‚Äî just click and go üòå</li>
</ul>
</section>
<section id="github" class="slide level2">
<h2>GitHub</h2>
<ul>
<li class="fragment">All course materials + code on GitHub üëá<br>
</li>
<li class="fragment"><img src="/images/github.png" alt="GitHub screenshot" width="450px"></li>
</ul>
</section>
<section id="schedule" class="slide level2">
<h2>Schedule</h2>
<ul>
<li class="fragment"><a href="https://docs.google.com/presentation/d/10HT1QZe-NeLPbn6CZ8mrn3OlBRqWR4Sq8N3HI9Vq0c4/edit#slide=id.p">Schedule link</a></li>
<li class="fragment">Photograph the QR code to view schedule on your phone</li>
<li class="fragment"><img data-src="images/google_slides_qr.png"></li>
</ul>
</section>
<section id="whatsapp-social-group" class="slide level2">
<h2>WhatsApp social group</h2>
<ul>
<li class="fragment">Chat with each other üí¨</li>
<li class="fragment">Ask &amp; answer questions</li>
<li class="fragment">Stay in touch ‚ù§Ô∏è</li>
<li class="fragment">Friday bar? üòÑ</li>
</ul>
</section>
<section id="overarching-aim" class="slide level2">
<h2>Overarching aim</h2>
<ul>
<li class="fragment">Use the tools of probability theory to scientifically understand brains minds and behavior</li>
</ul>
</section>
<section id="specific-aims" class="slide level2">
<h2>Specific aims</h2>
<ul>
<li class="fragment">Focus on models of mind &amp; behavior</li>
<li class="fragment">Connect to neural data via vanilla methodsüß†</li>
<li class="fragment">Stop describing ü•± ‚Üí Start explaining üòå</li>
<li class="fragment">Hands-on ¬∑ Interactive ¬∑ Discursive</li>
<li class="fragment">Repetitive ¬∑ A lil‚Äô redundant (on purpose!)</li>
<li class="fragment">KISS: Keep It Simple, Stoopid üòú</li>
<li class="fragment">Intuitive ¬∑ Playful ¬∑ Fun ¬∑ Useful ‚ú®</li>
</ul>
</section>
<section id="our-expectations-of-you≈º" class="slide level2">
<h2>Our expectations of you≈º:</h2>
<ul>
<li class="fragment">Life is too to short to let things pass that you dont understand.</li>
<li class="fragment">Your life matters. Your understanding matters. Actively take part.</li>
<li class="fragment">You are entitled to understand this.</li>
<li class="fragment">Don‚Äôt listen to it, fight it. ü•ä</li>
<li class="fragment">Ask questions, interupt. üôã‚Äç‚ôÇÔ∏è</li>
<li class="fragment">Be sceptical. ü§®</li>
</ul>
</section>
<section id="do-say" class="slide level2">
<h2>Do say:</h2>
<ul>
<li class="fragment">‚ÄúI‚Äôm probably being stupid‚Ä¶‚Äù ü§™</li>
<li class="fragment">‚ÄúI might have missed this‚Ä¶‚Äù</li>
<li class="fragment">‚Äúi dont understand why‚Ä¶‚Äù</li>
<li class="fragment">‚Äúdo you have an intuition for why‚Ä¶‚Äù</li>
<li class="fragment">‚Äúi‚Äôm confused‚Ä¶‚Äù ü§î</li>
<li class="fragment">If shy, say it in the break, write in whatsApp</li>
</ul>
</section>
<section id="dont" class="slide level2">
<h2>Don‚Äôt:</h2>
<ul>
<li class="fragment">Nod and pretend you get something. üôÖ‚Äç‚ôÇÔ∏è</li>
<li class="fragment">Assume you are the only one confused. üòµ‚Äçüí´üòµ‚Äçüí´üòµ‚Äçüí´</li>
</ul>
<!-- #endregion Preamble-->
<!-- #region Lecture#2 Basics of Bayesian analysis-->
</section></section>
<section>
<section id="lecture2-basics-of-bayesian-analysis" class="title-slide slide level1 center">
<h1>Lecture#2: Basics of Bayesian analysis</h1>
<ul>
<li class="fragment">Principles of Bayesian inference</li>
<li class="fragment">Latent vs.&nbsp;observable variables</li>
<li class="fragment">Beliefs and evidence<br>
</li>
<li class="fragment">Estimation methods</li>
<li class="fragment">Why Bayes?</li>
</ul>
</section>
<section id="general-principles" class="slide level2">
<h2>General principles</h2>
<ul>
<li class="fragment">Current belief ‚Üí evidence ‚Üí new belief</li>
<li class="fragment">AKA:</li>
<li class="fragment">prior belief ‚Üí evidence ‚Üí posterior belief</li>
</ul>
</section>
<section id="a-cognitive-example" class="slide level2">
<h2>A cognitive example</h2>
<ul>
<li class="fragment">A subject performs a simple task (e.g.&nbsp;go/no-go)<br>
</li>
<li class="fragment">10 trials, equal difficulty<br>
</li>
<li class="fragment">We want to estimate cognitive ability, <span class="math inline">\(\theta\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(\theta\)</span> = rate of correct performance<br>
</li>
<li class="fragment"><span class="math inline">\(\theta\)</span> is <strong>latent</strong> ‚Äî its hidden, we can‚Äôt observe it directly<br>
</li>
<li class="fragment">We observe <em>only</em> the number of correct trials<br>
</li>
<li class="fragment">We are uncertain about <span class="math inline">\(\theta\)</span><br>
</li>
<li class="fragment">But we know <span class="math inline">\(0 &lt; \theta &lt; 1\)</span></li>
</ul>
</section>
<section id="hidden-variables" class="slide level2">
<h2>Hidden Variables</h2>
<ul>
<li class="fragment">Modeling hidden variables is <em>central</em> to cognitive modeling<br>
</li>
<li class="fragment">It‚Äôs how we <strong>explain</strong>, not just describe<br>
</li>
<li class="fragment">Descriptive: ‚Äúperformance = 8/10‚Äù<br>
</li>
<li class="fragment">Explanatory: ‚Äúwhat underlying ability led to that?‚Äù<br>
</li>
<li class="fragment">Psychology/neuroscience is always about <strong>hidden causes</strong></li>
</ul>
</section>
<section id="why-hidden-observed" class="slide level2">
<h2>Why Hidden ‚â† Observed?</h2>
<ul>
<li class="fragment">Number correct = <strong>observable</strong> variable<br>
</li>
<li class="fragment">Ability <span class="math inline">\(\theta\)</span> = <strong>hidden</strong> variable</li>
<li class="fragment">Same <span class="math inline">\(\theta\)</span> can produce different outcomes<br>
</li>
<li class="fragment">Theory lives in the <strong>hidden layer</strong></li>
<li class="fragment">Often our deeper questions and theories are about the hidden not the observed.</li>
<li class="fragment">e.g.&nbsp;<em>is there a difference in ability between the two groups?</em></li>
</ul>
</section>
<section id="prior-belief-over-theta" class="slide level2">
<h2>Prior Belief over <span class="math inline">\(\theta\)</span></h2>
<ul>
<li class="fragment">If agnostic: use a <strong>uniform prior</strong><br>
</li>
<li class="fragment">Equal probability to all <span class="math inline">\(\theta\)</span> values in (0,1)<br>
</li>
<li class="fragment">Maximum uncertainty</li>
</ul>
</section>
<section id="representing-beliefs-as-probabiity-distributions" class="slide level2">
<h2>Representing beliefs as probabiity distributions</h2>
<ul>
<li class="fragment">Probability distributions are a natural way to represent beliefs</li>
<li class="fragment">They express our what we believe (.e.g is theta high or low)</li>
<li class="fragment">‚Ä¶and our uncertainy (how spread out is the distribution)</li>
<li class="fragment"><em>interactive beta dist</em></li>
<li class="fragment">play with the sliders to represent different prior beliefs</li>
<li class="fragment"><em>e.g.&nbsp;certain low, certain high, uncertain low, uncertain high</em></li>
</ul>
</section>
<section id="update-prior-with-evidence-from-data" class="slide level2">
<h2>Update prior with evidence from data</h2>
<ul>
<li class="fragment">Data: D = 9/10 trials correct</li>
<li class="fragment"><span class="math inline">\(p(\theta) \rightarrow p(D|\theta) \rightarrow p(\theta|D)\)</span><br>
</li>
<li class="fragment">prior <span class="math inline">\(\rightarrow\)</span> likelihood <span class="math inline">\(\rightarrow\)</span> posterior</li>
</ul>
</section>
<section id="bayes-rule" class="slide level2">
<h2>Bayes rule</h2>
<ul>
<li class="fragment">Bayes rule expresses how to update priors into posterior according to data</li>
<li class="fragment"><span class="math inline">\(p(\theta | D) = \frac{p(D| \theta) \, p(\theta)}{p(D)}\)</span></li>
<li class="fragment"><span class="math inline">\(\text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{marginal likelihood}}\)</span></li>
</ul>
</section>
<section id="marginal-likelihood" class="slide level2">
<h2>Marginal likelihood</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(D)\)</span> this is the probability of the data.</li>
<li class="fragment">Its a single number that ensures the posterior sums to 1 and is therefore a probability distribution</li>
<li class="fragment">It doesn‚Äôt depend on <span class="math inline">\(\theta\)</span></li>
</ul>
</section>
<section id="alternative-expression-of-bayes" class="slide level2">
<h2>Alternative expression of Bayes</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(\theta | D) \propto p(D | \theta) \times p(\theta)\)</span></li>
<li class="fragment"><span class="math inline">\(\text{Posterior} \propto \text{Likelihood} \times \text{Prior}\)</span></li>
</ul>
</section>
<section id="bayesian-credibility-intervals-express-uncertainy" class="slide level2">
<h2>Bayesian credibility intervals express uncertainy</h2>
<ul>
<li class="fragment">The posterior can be interpreted as a probability distribution</li>
<li class="fragment">if 50% of distribution is above say 0.55, then there is <span class="math inline">\(p(0.5)&gt;\theta\)</span></li>
<li class="fragment">Bayesian credible intervals e.g.&nbsp;<span class="math inline">\(\text{BCI}_{95\%} [0.4,0.8]\)</span> meaans that 95% probability the true value is between those limits.</li>
<li class="fragment">This is what we want classical confidence intervals to be. <!-- - *insert drawing of how to make BCI* --></li>
</ul>
</section>
<section id="sequential-updating" class="slide level2">
<h2>Sequential updating</h2>
<ul>
<li class="fragment">suppose we collect more data</li>
<li class="fragment">5 more trials and 3 of these are correct</li>
<li class="fragment">we previously had 9 out of 10 correct</li>
<li class="fragment">we just add the data together so now its 12 / 15</li>
<li class="fragment">unlike frequentist statistics the order of data collection doesnt matter <!-- - *plot new posterior* --></li>
</ul>
</section>
<section id="sequential-updating-1" class="slide level2">
<h2>Sequential updating</h2>
<ul>
<li class="fragment">it makes no difference if‚Ä¶</li>
<li class="fragment">compute posterior for first data, then update again</li>
<li class="fragment">prior ‚Üí 9/10 ‚Üí posterior ‚Üí prior ‚Üí 3/ 5 ‚Üí posterior</li>
<li class="fragment">or just compute all in one go</li>
<li class="fragment">prior ‚Üí 12/15 ‚Üí posterior</li>
<li class="fragment">same final posterior. simples.</li>
</ul>
</section>
<section id="analytic-solution" class="slide level2">
<h2>Analytic solution</h2>
<ul>
<li class="fragment">For certain cases you can calculate posteriors ‚Äúanalytically‚Äù via equations</li>
<li class="fragment">e.g.&nbsp;in this example we represent our uniform prior as a beta distribution</li>
<li class="fragment"><span class="math inline">\(p(\theta) \sim beta(1,1)\)</span></li>
<li class="fragment">to update to the posterior is simple</li>
<li class="fragment"><span class="math inline">\(p(\theta|k,n) \sim beta(1+k,1+(n-k))\)</span></li>
<li class="fragment">where <span class="math inline">\(k\)</span> is correct and <span class="math inline">\(n\)</span> is total</li>
<li class="fragment">thus for every possible observation we know precisely how to update our posterior</li>
</ul>
</section>
<section id="interactive-beta-distribution" class="slide level2">
<h2>Interactive beta distribution</h2>
<!-- - *plot beta with sliders for k and n* -->
<ul>
<li class="fragment">play around with the data to see how it updates the uniform prior to any posterior</li>
<li class="fragment">what values of k and n, make the posterior most uncertain?</li>
<li class="fragment">which values make posterior most certain it theta is low? high? intermediate?</li>
</ul>
</section>
<section id="conjugate" class="slide level2">
<h2>Conjugate</h2>
<ul>
<li class="fragment">in nice cases like this, it is only possible because the prior and posterior belong to the same family of distributions</li>
<li class="fragment">they are ‚Äúconjugate‚Äù</li>
<li class="fragment">sadly, in most cases this is not possible üòû</li>
</ul>
</section>
<section id="so-what-do-we-do-in-other-cases" class="slide level2">
<h2>So what do we do in other cases?</h2>
<ul>
<li class="fragment">dont fear MCMC is here!</li>
<li class="fragment">Monte Carlo Markov Chain.<br>
</li>
<li class="fragment">Its a method for sampling from the posterior and it works in almost all cases.</li>
<li class="fragment">It‚Äôs a little bit magic.</li>
<li class="fragment">This course focuses on MCMC because it works in all cases and its simple to use.</li>
</ul>
</section>
<section id="comparing-analytic-vs.-numerical-sampling" class="slide level2">
<h2>Comparing analytic vs.&nbsp;numerical (sampling)</h2>
<!-- - *plot analytic posterior*
- *plot MCMC*
- *screencap from book?* -->
</section>
<section id="how-does-mcmc-sampling-work" class="slide level2">
<h2>How does MCMC sampling work?</h2>
<ul>
<li class="fragment">There are two paths forward.</li>
<li class="fragment">Red pill - you like to know the truth because truth is power. Therefore you can follow this video <em>insert link</em> on how it works</li>
<li class="fragment">Blue pill - you take for granted that it works and not knowing exactly how has no material impact on your modelling.</li>
</ul>
</section>
<section id="advantages-of-bayesian-modelling" class="slide level2">
<h2>Advantages of Bayesian modelling</h2>
<ul>
<li class="fragment">Flexibility - Bayesian models can be built to accomodate the complexity or simplicity of any data. They can deal with contamination, confounds, hierarchical data, missing data.</li>
<li class="fragment">Principled - Uncertainty is always fully represented, and accounted for, no useful information is discarded.</li>
<li class="fragment">Intuitive - Generally Bayes will align with your intuition, much better than other approaches. Sometimes you will need to update your intuition, but thats ok.</li>
<li class="fragment">Easy - It is easy, once you know how.</li>
</ul>
</section></section>
<section>
<section id="modelling-a-binary-process" class="title-slide slide level1 center">
<h1>Modelling a binary process</h1>
<ul>
<li class="fragment">Start simple ‚Üí binary processes</li>
<li class="fragment">Most/many tasks are binary processes üò≤</li>
</ul>
</section>
<section id="getting-started" class="slide level2">
<h2>Getting started</h2>
<ul>
<li class="fragment">Our first example was about inferring a rate for a binary process</li>
<li class="fragment">Binary process since either correct or incorrect</li>
<li class="fragment">Inferring a rate, since we were inferring the underlying ability as a probability <span class="math inline">\(\theta\)</span></li>
<li class="fragment">Lots of tasks can be represented as binary processes</li>
</ul>
</section>
<section id="cognitive-tasks-that-are-binary-processes" class="slide level2">
<h2>Cognitive tasks that are binary processes</h2>
<ul>
<li class="fragment">Task switching</li>
<li class="fragment">Go-no-go</li>
<li class="fragment">Stop signal</li>
<li class="fragment">‚Ä¶</li>
</ul>
</section>
<section id="from-binary-outcomes-to-rate" class="slide level2">
<h2>From binary outcomes to rate</h2>
<ul>
<li class="fragment">Binary outcomes typically k correct out of n</li>
<li class="fragment">Expressed as a rate = k / n</li>
<li class="fragment">This is the observed rate</li>
<li class="fragment">But what is the underlying hidden rate <span class="math inline">\(\theta\)</span></li>
<li class="fragment"><span class="math inline">\(k \sim \text{Binomial}(\theta, n)\)</span></li>
<li class="fragment">assuming no history dependent effects where what happens in past influences the present</li>
</ul>
</section>
<section id="from-binary-outcomes-to-rate-1" class="slide level2">
<h2>From binary outcomes to rate</h2>
<ul>
<li class="fragment">observing k out of n trials allows us to update our beliefs about <span class="math inline">\(\theta\)</span></li>
<li class="fragment">what we know, and what we dont know of variables is alwwys represented by probability distributions</li>
<li class="fragment"><span class="math inline">\(\theta \sim \text{Beta(1,1)}\)</span></li>
<li class="fragment"><span class="math inline">\(k \sim \text{Binomial(\theta,n)}\)</span></li>
<li class="fragment"><img data-src="fig2.1.jpg"></li>
</ul>
</section>
<section id="graphical" class="slide level2">
<h2>Graphical</h2>
<ul>
<li class="fragment">Graphical models represent our probabilisitc model</li>
<li class="fragment">nodes represent all the variables relevant to the problem</li>
<li class="fragment">graph structure represent dependencies - children depend on parents</li>
</ul>
</section>
<section id="graphical-notation" class="slide level2">
<h2>Graphical notation</h2>
<ul>
<li class="fragment">Circular - continuous, Square - discrete</li>
<li class="fragment">Shaded - observed, Non-shaded - hidden</li>
<li class="fragment">Single-boundary - stochastic, double-boundary - deterministic</li>
</ul>
</section>
<section id="sampling-via-jags" class="slide level2">
<h2>Sampling via JAGS</h2>
<ul>
<li class="fragment">JAGS code that would run this model</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource jags number-lines code-with-copy"><code class="sourceCode"><span id="cb1-1"><a href=""></a>model{ </span>
<span id="cb1-2"><a href=""></a>  theta ~ dbeta(1,1)</span>
<span id="cb1-3"><a href=""></a>  k ~ dbin(theta,n)</span>
<span id="cb1-4"><a href=""></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li class="fragment">We sample from the posterior to find the posterior distribution of <span class="math inline">\(\theta\)</span> for the data k = 5, n =10</li>
<li class="fragment"><em>[insert Fig.2.8]</em></li>
</ul>
</section>
<section id="r-hat-as-a-convergence-check" class="slide level2">
<h2>R-hat as a convergence check</h2>
<ul>
<li class="fragment"><p>Its important to check that the sampling has converged to the stationary distribution</p></li>
<li class="fragment"><p>One heuristic is the R-hat [ = ] -Rule of thumb: <span class="math inline">\(\hat{R}\)</span> should be 1-1.01 for convergence to be achieved</p>
<p>footer Learn more: Brooks &amp; Gelman (1998).</p></li>
</ul>
<!-- #endregion -->
<!-- #region Lecture#3 Parameter estimation       -->
</section></section>
<section>
<section id="lecture3-parameter-estimation" class="title-slide slide level1 center">
<h1>Lecture#3: Parameter estimation</h1>

</section>
<section id="posterior-from-the-last-example" class="slide level2">
<h2>Posterior from the last example</h2>
<ul>
<li class="fragment">We estimated the posterior distribution of <span class="math inline">\(\theta\)</span> given the data</li>
<li class="fragment">We sampled from this model</li>
<li class="fragment"><img data-src="fig2.1.jpg"></li>
<li class="fragment">and we got this distribution</li>
<li class="fragment"><img data-src="fig2.8.jpg"></li>
</ul>
</section>
<section id="conjugacy" class="slide level2">
<h2>Conjugacy</h2>
<ul>
<li class="fragment">Beta distributions has a natural interpretation</li>
<li class="fragment"><span class="math inline">\(\theta \sim text{Beta(\alpha, \beta)}\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> can be thought of as counts</li>
<li class="fragment"><span class="math inline">\(\alpha = 1 + text{successes} = 1 + k\)</span></li>
<li class="fragment">$= 1 + text{failures} = 1 + (n-k) $</li>
</ul>
</section>
<section id="why-1-count" class="slide level2">
<h2>Why 1 + count?</h2>
<ul>
<li class="fragment">Beta(1,1) is a uniform distribution so this is equivalent to 0 counts</li>
<li class="fragment">It is your belief distribution when you have no knowledge etc.<br>
</li>
<li class="fragment">Thus evidence accumulates from 1, via counting of successes and failures</li>
</ul>
</section>
<section id="conjugacy-1" class="slide level2">
<h2>Conjugacy</h2>
<ul>
<li class="fragment">Notice that the update from the prior to the posterior just comes from adding the counts to the beta distribution</li>
<li class="fragment">prior: Beta(1,1) ‚Üí posterior: Beta(1+k, 1+(n-k))</li>
<li class="fragment">prior: Beta(1,1) ‚Üí posterior: Beta(1+ successCount, 1+ failureCount)<br>
</li>
<li class="fragment">No sampling is needed because we have the equation</li>
<li class="fragment">When the prior and posterior have same type of distribution they are ‚Äúconjugate‚Äù</li>
</ul>
</section>
<section id="conjugacy-interactive-example" class="slide level2">
<h2>Conjugacy interactive example</h2>
<ul>
<li class="fragment">Binder example- play with the beta distribution</li>
<li class="fragment">No sampling needed you just change inputs to the beta distribution to compute posterior</li>
</ul>
</section>
<section id="difference-between-two-rates" class="slide level2">
<h2>Difference between two rates</h2>
<ul>
<li class="fragment">Suppose we have to processes, say two tasks producing:</li>
<li class="fragment">k1 successes out of n1 trials</li>
<li class="fragment">k2 successes out of n2 trials</li>
<li class="fragment">let‚Äôs assume they are generated by two different rates <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span></li>
<li class="fragment">we want to know the difference in the rates <span class="math inline">\(\delta = \theta_1 - \theta_2\)</span></li>
<li class="fragment"><em>e.g.&nbsp;the effect of a drug on cognitive performance</em></li>
<li class="fragment"><em>e.g.&nbsp;the effect of a age on task performance</em></li>
</ul>
</section>
<section id="graphical-model-for-inferring-differences-in-rates" class="slide level2">
<h2>Graphical model for inferring differences in rates</h2>
<ul>
<li class="fragment"><img data-src="fig3.3.jpg"></li>
</ul>
</section>
<section id="sampling-code-for-infferring-differences-in-rates" class="slide level2">
<h2>Sampling code for infferring differences in rates</h2>
<!-- ``` {.python code-line-numbers="1-2|6-7|8-9|10"}
import pyjags
import numpy as np
model = """
model{ 
  k1 ~ dbin(theta1,n1)
  k2 ~ dbin(theta2,n2)
  theta1 ~ dbeta(1,1)
  theta2 ~ dbeta(1,1)
  delta <-theta1-theta2
}
"""
data = {'N': 10, 'y': np.random.randn(10)}
model = pyjags.Model(model, data=data, chains=3)
samples = model.sample(1000, vars=['mu', 'sigma'])
print(samples)
``` -->
</section>
<section id="sampled-posterior-for-difference-in-rates" class="slide level2">
<h2>Sampled posterior for difference in rates</h2>
<ul>
<li class="fragment"><img data-src="fig3.4.jpg"></li>
</ul>
</section>
<section id="interpreting-distributions" class="slide level2">
<h2>Interpreting distributions</h2>
<ul>
<li class="fragment">Probability mass functions are for discrete variables which take finite number of values</li>
<li class="fragment">Probability density functiosn for continuous variables which take infinitely many values</li>
</ul>
</section>
<section id="probability-mass-functions" class="slide level2">
<h2>Probability mass functions</h2>
<ul>
<li class="fragment">insert left figure from box 3.2</li>
<li class="fragment">all sum to 1</li>
</ul>
</section>
<section id="probability-density-functions" class="slide level2">
<h2>Probability density functions</h2>
<ul>
<li class="fragment">insert right figure from box 3.2</li>
<li class="fragment">area sums to 1</li>
<li class="fragment">densities can exceed 1</li>
<li class="fragment">only areas can be interpreted as probabilities</li>
</ul>
</section>
<section id="inferring-a-common-rate" class="slide level2">
<h2>Inferring a common rate</h2>
<ul>
<li class="fragment">In some cases we want to infer a rate for 2 processes</li>
<li class="fragment">e.g.&nbsp;same subject, same task, two different sessions</li>
<li class="fragment">Here we would model with a single <span class="math inline">\(\theta\)</span></li>
<li class="fragment"><img data-src="fig3.5.jpg"></li>
</ul>
</section>
<section id="same-model-with-plate-notation" class="slide level2">
<h2>Same model with plate notation</h2>
<ul>
<li class="fragment"><img data-src="fig3.6.jpg"></li>
<li class="fragment">note only one theta, multiple processes</li>
</ul>
</section>
<section id="code-for-inferring-a-common-rate" class="slide level2">
<h2>Code for inferring a common rate</h2>
<ul>
<li class="fragment">model{ k1 ~ dbin(theta1,n1) k2 ~ dbin(theta2,n2) theta ~ dbeta(1,1) }</li>
</ul>
</section>
<section id="predicting-data" class="slide level2">
<h2>Predicting data</h2>
<ul>
<li class="fragment">We know 2 distributions already</li>
<li class="fragment"><ol type="1">
<li class="fragment">Prior distribution of parameter e.g.&nbsp;<span class="math inline">\(p(\theta)\)</span></li>
</ol></li>
<li class="fragment">This is our prior prediction for what the parameters will be, prior to the data</li>
<li class="fragment"><ol start="2" type="1">
<li class="fragment">Posterior distribution of parameter having observed the data e.g.&nbsp;<span class="math inline">\(p(\theta|data)\)</span></li>
</ol></li>
<li class="fragment">This is our posterior prediction for the what the parameters will be, having seen the data</li>
<li class="fragment">Remember todays posterior is tomorrows prior, so they can be a prediction for the next round of data, ad infinitum.</li>
</ul>
</section>
<section id="predicting-data-1" class="slide level2">
<h2>Predicting data</h2>
<ul>
<li class="fragment">Priors and posteriors are of parameters. what about actually predicting data?</li>
<li class="fragment"><ol start="3" type="1">
<li class="fragment">Prior predictive distribution <span class="math inline">\(p(data)\)</span></li>
</ol></li>
<li class="fragment">this is the prediciton of what the data will be based on the prior, prior to the data being seen.</li>
<li class="fragment"><ol start="4" type="1">
<li class="fragment">Posterior predictive distribution <span class="math inline">\(p(new_data|old_data)\)</span></li>
</ol></li>
<li class="fragment">this is the prediciton of what the data will be based on the posterior, after the data has been seen.</li>
</ul>
</section>
<section id="prior-and-posterior-prediction" class="slide level2">
<h2>Prior and posterior prediction</h2>
<p>model { k ~ dbin(theta,n) theta~dbeta(1,1) postpredk~dbin(theta,n) thetaprior~dbeta(1,1) priorpredk~dbin(thetapior,n) }</p>
</section>
<section id="samples-of-the-four-distributions" class="slide level2">
<h2>Samples of the four distributions</h2>
<ul>
<li class="fragment"><img data-src="fig3.9.jpg"></li>
<li class="fragment">Note the prior and posterior are in the space of the parameter</li>
<li class="fragment">And the prior and posterior predictive distributions are in space of the data k out of n trials.</li>
</ul>
</section>
<section id="comparing-data-to-the-posterior-predictive-distribution" class="slide level2">
<h2>Comparing data to the posterior predictive distribution</h2>
<ul>
<li class="fragment">If we estimate the model along with its predictive distributions</li>
<li class="fragment">We can see how this compares to the actual data</li>
<li class="fragment"><img data-src="fig3.11.jpg"></li>
<li class="fragment">Here we see it‚Äôs not good. The model has poor <em>descriptive adequacy</em></li>
<li class="fragment">Why is it not a good model?</li>
<li class="fragment">Its a common rate model, so it predicts the same rate, but the data clearly is better modelled with different rates.</li>
</ul>
</section>
<section id="inference-and-time" class="slide level2">
<h2>Inference and time</h2>
<ul>
<li class="fragment">prediction naturally suggests thinking about the future.</li>
<li class="fragment">predictions can also apply to the past‚Äîfor instance, when information is missing.</li>
<li class="fragment">data can help infer hidden cognitive variables, which in turn may predict past behavior where information is incomplete.</li>
<li class="fragment">similar to how historians reconstruct past events from limited evidence.</li>
<li class="fragment">or a court of law infers guilt based on available testimony and facts.</li>
<li class="fragment">we predict what might have been known in the past if more information had been available.</li>
<li class="fragment">Inference allows prediction both forward and backward in time.</li>
</ul>
</section>
<section id="todays-posterior-is-tomorrows-prior" class="slide level2">
<h2>Todays posterior is tomorrows prior</h2>
<ul>
<li class="fragment">Since the posterior distribution of parameters is continually updatable, then so is the posterior predictive distribution.</li>
<li class="fragment">As the posterior is updated, then so are its predictions for the data.</li>
<li class="fragment"><img data-src="box3.4.jpg"></li>
<li class="fragment">This can carry on forever.</li>
</ul>
<!-- #endregion -->
<!-- #region Lecture#4 Inference with Gaussians   -->
</section></section>
<section>
<section id="inferences-with-gaussians" class="title-slide slide level1 center">
<h1>Inferences with Gaussians</h1>
<ul>
<li class="fragment">Due to central limit theorem, data and parameters are frequently Gaussian</li>
<li class="fragment">Gaussians have 2 parameters, a mean and a measure of their spread</li>
<li class="fragment">Spread can be expressed as a variance, std, or a precision (1/var)</li>
</ul>
</section>
<section id="graphical-model-for-gaussians" class="slide level2">
<h2>Graphical model for Gaussians</h2>
<ul>
<li class="fragment">Simple model for inferring Gaussian with unknown mean and std</li>
<li class="fragment"><img data-src="fig.4.1.jpg"></li>
</ul>
</section>
<section id="interactive-demo-of-gaussian" class="slide level2">
<h2>Interactive demo of Gaussian</h2>
<ul>
<li class="fragment">Jupyter notebook - interactive plotting of gaussian</li>
</ul>
</section>
<section id="sampling-model-for-inferring-gaussians" class="slide level2">
<h2>Sampling model for inferring Gaussians</h2>
<p>model { for (i in 1:n){ x[i]~dnorm(mu,lambda) } mu~dnorm(0,0.001) sigma~dunif(0,10) lambda&lt;-1/pow(sigma,2) }</p>
</section>
<section id="repeated-measures-of-iq" class="slide level2">
<h2>Repeated measures of IQ</h2>
<ul>
<li class="fragment">Imagine taking a cognitive test like IQ multiple times</li>
<li class="fragment">The mean is your IQ, and the spread models fluctuations in your performance. e.g.&nbsp;attention, fatigue, emotion, venus orbitting satturn</li>
<li class="fragment">We can model this as a Gaussian for each person</li>
</ul>
</section>
<section id="graphical-model-for-iq" class="slide level2">
<h2>Graphical model for IQ</h2>
<ul>
<li class="fragment"><img data-src="fig.4.3.jpg"></li>
<li class="fragment">What parameter is common to all subjects?</li>
<li class="fragment">No index on the std. This means it is fixed.</li>
<li class="fragment">Is this justified?</li>
<li class="fragment">How to change it?</li>
</ul>
</section>
<section id="sampling-code-for-iq" class="slide level2">
<h2>Sampling code for IQ</h2>
<p>model{ for (i in 1:n) { for (j in 1:m) { x[i,j]~dnorm(mu[i],lambda) } } sigma~dunif(0,100) lambda &lt;-1/pow(sigma,2) for (i in 1:n) { mu([i]~ dunif(0,300)) }</p>
<p>}</p>
<!-- #endregion                            -->
<!-- #region Lecture#5 Latent Mixture models      -->
</section></section>
<section>
<section id="latent-mixture-models" class="title-slide slide level1 center">
<h1>Latent Mixture Models</h1>
<ul>
<li class="fragment">A latent mixture model is a model that assumes data are generated from a mixture of different hidden (latent) processes</li>
<li class="fragment">E.g. imagine a cogntive test where some people try their best, and others just guess</li>
<li class="fragment">the scores then are generated by two mechnamisms, ability and guessing</li>
<li class="fragment">how would we model this as a mixture of processes</li>
</ul>
</section>
<section id="latent-mixture-model-of-cognitive-test" class="slide level2">
<h2>Latent mixture model of cognitive test</h2>
<ul>
<li class="fragment">to model the ‚Äútryers‚Äù you would model their ability as a rate of correct performance, as before.</li>
<li class="fragment">to model the ‚Äúguessers‚Äù you just model it as chance performance</li>
</ul>
</section>
<section id="latent-mixture-graphical-model" class="slide level2">
<h2>Latent mixture graphical model</h2>
<ul>
<li class="fragment"><img data-src="fig.6.1.jpg"></li>
<li class="fragment">Here z determines whether the person is a tryer or a guesser. Thus z is a parameter that models the mixture of processes that explain the data.</li>
<li class="fragment">We know the probability of guessing is 0.5</li>
<li class="fragment">The posterior distribution of z gives us the estimate of how many guessers there were.</li>
</ul>
</section>
<section id="latent-mixture-code" class="slide level2">
<h2>Latent mixture code</h2>
<p>model{ for (i in 1:p) { z[i]~dbern(0.5) } psi &lt;-0.5 phi ~ dbeta(1,1)I(0.5,1) for (i in 1:p) { theta[i]&lt;- equals(z[i],0)<em>psi+equals(z[i],1)</em>phi k[i]~dbin(theta[i],n) } }</p>
<!-- #endregion                             -->
<!-- #region Lecture#6 Model selection            -->
</section></section>
<section>
<section id="model-selection" class="title-slide slide level1 center">
<h1>Model selection</h1>
<ul>
<li class="fragment">Model selection is perhaps the most persuasive reason to choose Bayesian methods.</li>
<li class="fragment">Bayesian models provide a simple and principled way to choose between models balancing between accuracy and complexity</li>
</ul>
</section>
<section id="why-model-comparison" class="slide level2">
<h2>Why model comparison?</h2>
<ul>
<li class="fragment">so far we have covered single models</li>
<li class="fragment">in psychology/neuroscience we want to compare different models</li>
<li class="fragment">is this theory or this theory better at explaining the data</li>
<li class="fragment">if someone says this is the theory. it begs the question, compared to what?</li>
<li class="fragment">we want to know how well a theory explains relative to another, or many others.</li>
<li class="fragment">we need to compare models if we are to move beyond descriptive data analysis.</li>
</ul>
</section>
<section id="ptolomy" class="slide level2">
<h2>Ptolomy</h2>
<ul>
<li class="fragment">We consider it a good principle to explain the phenomena by the simplest hypothesis, provided this doesnt contradict the data in an important way</li>
</ul>
</section>
<section id="occams-razon" class="slide level2">
<h2>Occam‚Äôs razon</h2>
<ul>
<li class="fragment">‚ÄúPlurarity must never be posited without necessity‚Äù</li>
<li class="fragment">This is nicknamed Occam‚Äôs razor, which cuts out needless complexity when it comes to theories or models or explanations</li>
</ul>
</section>
<section id="fristons-free-energy" class="slide level2">
<h2>Friston‚Äôs free energy</h2>
<ul>
<li class="fragment">‚ÄúMinimizing free energy means finding the right balance between accuracy and complexity‚Äù</li>
<li class="fragment">‚ÄúGood models should be as simple as possible, but not simpler than necessary to explain the data. Complexity must be minimized to avoid overfitting, yet sufficient to capture the underlying structure of the world.‚Äù</li>
</ul>
</section>
<section id="geoff-hinton" class="slide level2">
<h2>Geoff Hinton</h2>
<ul>
<li class="fragment">‚ÄúModel complexity must pay for itself‚Äù</li>
<li class="fragment">Geoff is referring to the fact that model complexity must be justified by its ability to explain the data.</li>
</ul>
</section>
<section id="bayesian-magic" class="slide level2">
<h2>Bayesian magic</h2>
<ul>
<li class="fragment">Lots of methods exist for trying to get this balance right</li>
<li class="fragment">Out of sample prediction, parameter counting algorithms, blah, blah.</li>
<li class="fragment">Bayes nails it by finding a universal principle by which to compare models that optimally balances accuracy and complexity.</li>
<li class="fragment">This is the magic sauce of Bayes.</li>
</ul>
</section>
<section id="marginal-likelihood-1" class="slide level2">
<h2>Marginal likelihood</h2>
<ul>
<li class="fragment">Back to Bayes equation we started with</li>
<li class="fragment"><img data-src="fig.1.1.jpg"></li>
<li class="fragment"><img data-src="fig.1.2.jpg"></li>
<li class="fragment">There is something missing which is that this is all conditional on a particular model. We could have a very different model that might have different parameters.</li>
</ul>
</section>
<section id="marginal-likelihood-conditional-on-model" class="slide level2">
<h2>Marginal likelihood conditional on model</h2>
<ul>
<li class="fragment">We can be explicit about how this is conditional on a specific model</li>
<li class="fragment"><img data-src="fig.7.1.jpg"></li>
<li class="fragment"><span class="math inline">\(p(D|M_1)\)</span> is a single number, the marginal likelihood, also known as the evidence.</li>
</ul>
</section>
<section id="marginal-likelihood-in-words" class="slide level2">
<h2>Marginal likelihood in words</h2>
<ul>
<li class="fragment">the probability of the data given model</li>
<li class="fragment">the probability of the data according to the models predictions</li>
<li class="fragment">average predictive performance of the model</li>
<li class="fragment">how unsuprised was the model when seeing the data</li>
<li class="fragment">the prior in the model is like betting on where good parameters lie. The marginal likelihood sums up how well those bets paid off in terms of predictive performance when the data arrived.</li>
<li class="fragment">a models predictions has probability mass of 1. it can spend its predicitons in different ways by having different priors. a bad prior wastes this on predictions that are not supported by the data, a good prior concrentrates its predicitons to where the data is well predicted.</li>
<li class="fragment">the better the predictions of the model, the greater the evidence for the model</li>
<li class="fragment">it‚Äôs like running every possible version of the model (with every parameter setting weighted by the prior), and asking: ‚Äúon average, how well does the model predict the data?‚Äù</li>
</ul>
</section>
<section id="marginal-likelihood-of-paranormal-octopi" class="slide level2">
<h2>Marginal likelihood of paranormal octopi</h2>
<ul>
<li class="fragment">Imagine two paranormal octupuses helping the KGB find a missing sub.</li>
<li class="fragment">Alice predicts the sub is in the nothern hemisphere</li>
<li class="fragment">Bob predicts its in nothern europe</li>
<li class="fragment">Data: the sub is found in the Baltic,</li>
<li class="fragment">the probability of the data according to Alice is reasonable. the evidence in favor of alice is reasonable</li>
<li class="fragment">the probability of the data according to Bob is high. The evidence in favor of Bob is high</li>
<li class="fragment">both were correct, but Bob was ‚Äúmore correct‚Äù because his prediciton was more specific</li>
</ul>
</section>
<section id="marginal-likelihood-2" class="slide level2">
<h2>Marginal likelihood</h2>
<ul>
<li class="fragment">The marginal likelihood is computed by averaging the likelihood of the data predicted across the models parameter space.<br>
</li>
<li class="fragment">Prior probabilities act as averaging weights.<br>
</li>
<li class="fragment">Based on the law of total probability:<br>
<span class="math inline">\(p(D \mid M_1) = \sum_{i=1}^{k} p(D \mid \xi_i, M_1) p(\xi_i \mid M_1)\)</span></li>
</ul>
</section>
<section id="example-calculation-of-marginal-likelihood" class="slide level2">
<h2>Example calculation of marginal likelihood</h2>
<ul>
<li class="fragment"><ul>
<li class="fragment">Consider a model <span class="math inline">\(M_x\)</span> with one parameter <span class="math inline">\(\xi\)</span>.<br>
</li>
</ul></li>
<li class="fragment"><span class="math inline">\(\xi\)</span> can take <strong>three values</strong>:
<ul>
<li class="fragment"><span class="math inline">\(\xi_1 = -1\)</span>, <span class="math inline">\(\xi_2 = 0\)</span>, <span class="math inline">\(\xi_3 = 1\)</span>.<br>
</li>
</ul></li>
<li class="fragment">Prior probabilities assigned:
<ul>
<li class="fragment"><span class="math inline">\(p(\xi_1) = 0.6\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(p(\xi_2) = 0.3\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(p(\xi_3) = 0.1\)</span></li>
</ul></li>
</ul>
</section>
<section id="likelihood-computation" class="slide level2">
<h2>Likelihood Computation</h2>
<ul>
<li class="fragment"><ul>
<li class="fragment">Likelihood values for observed data <span class="math inline">\(D\)</span>:<br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid \xi_1) = 0.001\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid \xi_2) = 0.002\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid \xi_3) = 0.003\)</span><br>
</li>
</ul></li>
<li class="fragment">Compute marginal likelihood:<br>
<span class="math inline">\(p(D \mid M_x) = p(\xi_1) p(D \mid \xi_1) + p(\xi_2) p(D \mid \xi_2) + p(\xi_3) p(D \mid \xi_3)\)</span> <span class="math inline">\(= 0.6 \times 0.001 + 0.3 \times 0.002 + 0.1 \times 0.003\)</span> <span class="math inline">\(= 0.0015\)</span></li>
</ul>
</section>
<section id="marginal-likelihood-and-complexity" class="slide level2">
<h2>Marginal likelihood and complexity</h2>
<ul>
<li class="fragment">To have high evidence, the model needs good predictive peformance</li>
<li class="fragment">It needs to focus its predictions where the data has a high likelihood</li>
<li class="fragment">Complex models with many parameters distribute their predictions widely</li>
<li class="fragment">Complex models thus make many predictions that may not turn out to have high likelihood</li>
<li class="fragment">Complex models thus are more likely to ‚Äúwaste their predictions‚Äù</li>
</ul>
</section>
<section id="model-complexity-is-not-the-same-as-the-number-of-parameters" class="slide level2">
<h2>Model complexity is not the same as the number of parameters</h2>
<ul>
<li class="fragment">Complexity is not just the number of parameters.</li>
<li class="fragment">A model with few parameters can still be complex if its predictions are highly uncertain due to vague priors.</li>
<li class="fragment">True complexity comes from how broadly the model spreads its predictions across its parameter space.</li>
<li class="fragment">A narrow prediction distribution suggests a simpler model.</li>
<li class="fragment">A wider distribution indicates greater complexity, as the model allows for more diverse possible outcomes.</li>
<li class="fragment">Models are complex to the degree to which they make broad range of predictions</li>
</ul>
</section>
<section id="model-complexity-and-vagueness-of-priors" class="slide level2">
<h2>Model complexity and vagueness of priors</h2>
<ul>
<li class="fragment">a model with prior <span class="math inline">\(\theta \sim \text{Uniform} (0,1)\)</span></li>
<li class="fragment">‚Ä¶is more complex than‚Ä¶</li>
<li class="fragment">a model with prior <span class="math inline">\(\theta \sim \text{Uniform} (0.5,1)\)</span></li>
</ul>
</section>
<section id="marginal-likelihood-as-a-unifying-maximandum" class="slide level2">
<h2>Marginal likelihood as a unifying maximandum</h2>
<ul>
<li class="fragment">Marginal likelihood is arguably the most important concept ever.</li>
<li class="fragment">It is at the heart of scientific inference, psychological and neural inference, and survival, and even existence of objects.</li>
<li class="fragment">Maximising it is hard, we often need to approximate it.</li>
<li class="fragment">Variational Bayes, Free-energy minimisation, Predictive coding, MCMC Sampling etc. are all ways to approximate it.</li>
<li class="fragment">Ultimately it is one quantity we care about.</li>
<li class="fragment">In our context we want to build models of brain, mind or behavior, that best explain these phenomena.</li>
<li class="fragment">This reduces to finding models that have the best average predictive performance.</li>
<li class="fragment">And this reduces to finding models with the highest marginal likelihood</li>
<li class="fragment">Thats it.</li>
<li class="fragment">Everything else is details.</li>
<li class="fragment">Ah but so and so maximises something different.</li>
<li class="fragment">Yes but at heart this is just a proxy for the marginal likelihood.</li>
</ul>
<!-- #endregion                                   -->
<!-- #region Lecture#7 Bayes factors              -->
</section></section>
<section>
<section id="the-bayes-factor" class="title-slide slide level1 center">
<h1>The Bayes factor</h1>
<ul>
<li class="fragment">Compared to what? This is a surprisingly powerful question to ask.</li>
<li class="fragment">This model is good - it has high model evidence.</li>
<li class="fragment">Ok great, but compared to what?</li>
<li class="fragment">We want relative evidence. We want to compare predictive performance for one model versus another.</li>
<li class="fragment">The Bayes factor gives us this by taking the ratio of the marginal likelihoods for two different models</li>
</ul>
</section>
<section id="bayes-factor-equation" class="slide level2">
<h2>Bayes factor equation</h2>
<ul>
<li class="fragment">Marginal likelihood measures a model‚Äôs absolute evidence by averaging its predictive performance over all parameter values.<br>
</li>
<li class="fragment">Model selection often focuses on relative evidence ‚Äîhow well one model explains the data compared to another.<br>
</li>
<li class="fragment">This is quantified using the Bayes factor:<br>
<span class="math inline">\(BF_{12} = \frac{p(D \mid M_1)}{p(D \mid M_2)}\)</span></li>
</ul>
</section>
<section id="interpretation-of-the-bayes-factor" class="slide level2">
<h2>Interpretation of the Bayes Factor</h2>
<ul>
<li class="fragment"><strong>BF &gt; 1</strong> ‚Üí Data favors <strong>M‚ÇÅ</strong> over <strong>M‚ÇÇ</strong>.<br>
</li>
<li class="fragment"><strong>BF &lt; 1</strong> ‚Üí Data favors <strong>M‚ÇÇ</strong> over <strong>M‚ÇÅ</strong>.<br>
</li>
<li class="fragment"><strong>Higher BF</strong> ‚Üí Stronger evidence for the favored model.<br>
</li>
<li class="fragment">If <strong>BF = 5</strong>, the data is <strong>5 times more likely</strong> under <strong>M‚ÇÅ</strong> than <strong>M‚ÇÇ</strong>.<br>
</li>
<li class="fragment">If <strong>BF = 1/5</strong>, the data is <strong>5 times more likely</strong> under <strong>M‚ÇÇ</strong> than <strong>M‚ÇÅ</strong>.</li>
</ul>
</section>
<section id="jeffreys-scale-for-bayes-factor" class="slide level2">
<h2>Jeffreys‚Äô Scale for Bayes Factor</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th><strong>Bayes Factor (BF‚ÇÅ‚ÇÇ)</strong></th>
<th><strong>Interpretation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>&gt;100</td>
<td>Extreme evidence for <strong>M‚ÇÅ</strong></td>
</tr>
<tr class="even">
<td>30‚Äì100</td>
<td>Very strong evidence for <strong>M‚ÇÅ</strong></td>
</tr>
<tr class="odd">
<td>10‚Äì30</td>
<td>Strong evidence for <strong>M‚ÇÅ</strong></td>
</tr>
<tr class="even">
<td>3‚Äì10</td>
<td>Moderate evidence for <strong>M‚ÇÅ</strong></td>
</tr>
<tr class="odd">
<td>1‚Äì3</td>
<td>Anecdotal evidence for <strong>M‚ÇÅ</strong></td>
</tr>
<tr class="even">
<td>1</td>
<td>No preference</td>
</tr>
<tr class="odd">
<td>1/3‚Äì1</td>
<td>Anecdotal evidence for <strong>M‚ÇÇ</strong></td>
</tr>
<tr class="even">
<td>1/10‚Äì1/3</td>
<td>Moderate evidence for <strong>M‚ÇÇ</strong></td>
</tr>
<tr class="odd">
<td>1/30‚Äì1/10</td>
<td>Strong evidence for <strong>M‚ÇÇ</strong></td>
</tr>
<tr class="even">
<td>1/100‚Äì1/30</td>
<td>Very strong evidence for <strong>M‚ÇÇ</strong></td>
</tr>
<tr class="odd">
<td>&lt;1/100</td>
<td>Extreme evidence for <strong>M‚ÇÇ</strong></td>
</tr>
</tbody>
</table>
</section>
<section id="example-binomial-model" class="slide level2">
<h2>Example: Binomial Model</h2>
<ul>
<li class="fragment"><p>Suppose we have 9 correct (k) out of 10 trials (n) and compare two models:<br>
</p></li>
<li class="fragment"><p>M‚ÇÅ: A non-guessing model with unknown success rate.<br>
<span class="math inline">\(p(\theta \mid M_1) \sim \text{Uniform}(0,1)\)</span></p></li>
<li class="fragment"><p>M‚ÇÇ: A guessing model with chance at 0.5 <span class="math inline">\(p(\theta \mid M_2) = 0.5\)</span></p></li>
<li class="fragment"><p>Compute <strong>marginal likelihoods</strong> for both models.</p></li>
</ul>
</section>
<section id="bayes-factor-for-the-example" class="slide level2">
<h2>Bayes Factor for the example</h2>
<ul>
<li class="fragment"><strong>Marginal likelihoods</strong>:<br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid M_1) = \left(\frac{10}{9}\right) \left(\frac{1}{2}\right)^{10}\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid M_2) = \left(\frac{1}{1+n}\right)\)</span><br>
</li>
<li class="fragment"><strong>Bayes factor calculation</strong>:<br>
<span class="math inline">\(BF_{12} = \frac{p(D \mid M_1)}{p(D \mid M_2)}\)</span><br>
</li>
<li class="fragment">$ BF_{12} = 0.107</li>
<li class="fragment">If <span class="math inline">\(BF_{12} &gt; 1\)</span>, M‚ÇÅ is preferred; if &lt; 1, M‚ÇÇ is preferred.</li>
</ul>
</section>
<section id="flipping-the-bf" class="slide level2">
<h2>Flipping the BF</h2>
<ul>
<li class="fragment">m1 is 0.107 less likely than m2</li>
<li class="fragment">hard to interpret</li>
<li class="fragment">thus when a BF is below it is often more convenient to take the reciprocal so that it is a value above 1.</li>
<li class="fragment">here it would be <span class="math inline">\(BF_{21}\)</span> which is 1/1.07 = 9.3</li>
<li class="fragment">M2 is 9.3 times more likely than M1</li>
<li class="fragment">the data is 9.3 times more likely under a model in which subjects are not guessing, vs.&nbsp;than in which they are guessing</li>
</ul>
</section>
<section id="bayes-vs.-fisher" class="slide level2">
<h2>Bayes vs.&nbsp;Fisher</h2>
<ul>
<li class="fragment">Bayesian methods are typically comparative, where two ore more models are compared</li>
<li class="fragment">Frequentist methods are not comparative, simply considering how unlikely data was generated by the null.</li>
<li class="fragment">Evidence is always comparative. ratio of p(data|Innocent) : p(data|Guilty)</li>
</ul>
</section>
<section id="cautions-and-critiques" class="slide level2">
<h2>Cautions and Critiques</h2>
<ul>
<li class="fragment">Bayes factors sensitive to prior specification.</li>
<li class="fragment">Must use careful prior selection and sensitivity analyses.</li>
<li class="fragment">Point null hypotheses controversial: can they ever be exactly true?
<ul>
<li class="fragment">Pragmatically, yes (experimental studies).</li>
<li class="fragment">Conceptually, more debated.</li>
</ul></li>
</ul>
<!-- #endregion                             -->
<!-- #region Lecture#8 Posterior                  -->
</section></section>
<section>
<section id="posterior-model-probabilities" class="title-slide slide level1 center">
<h1>Posterior model probabilities</h1>
<ul>
<li class="fragment">Bayes factors compare the predictive performance of two different theories</li>
<li class="fragment">But model plausibility also depends on prior beliefs in each model.</li>
<li class="fragment">To assess relative plausibility after seeing the data we combine:</li>
<li class="fragment">Predictive performance (likelihood) and Prior plausibility (prior probabilities)</li>
</ul>
</section>
<section id="posterior-odds" class="slide level2">
<h2>Posterior odds</h2>
<ul>
<li class="fragment">Relaitve plausibility of models after seeing the data is indicated by the posterior odds</li>
<li class="fragment">The posterior odds of two models is: <span class="math inline">\(\frac{p(M_1 \mid D)}{p(M_2 \mid D)} = \frac{p(D \mid M_1)}{p(D \mid M_2)} \cdot \frac{p(M_1)}{p(M_2)}\)</span></li>
<li class="fragment">Or in words: <strong>posterior odds = Bayes factor √ó prior odds</strong></li>
</ul>
</section>
<section id="bayes-factor-transforms-prior-odds-into-posterior-odds" class="slide level2">
<h2>Bayes factor transforms prior odds into posterior odds</h2>
<ul>
<li class="fragment">The Bayes factor transforms <strong>prior odds</strong> into <strong>posterior odds</strong>: <span class="math inline">\(\text{prior odds} \rightarrow \text{posterior odds}\)</span></li>
</ul>
</section>
<section id="advantages-of-the-bayes-factors" class="slide level2">
<h2>Advantages of the Bayes factors</h2>
<ul>
<li class="fragment">Bayesian hypothesis tests (e.g., Bayes factors) naturally implement Ockham‚Äôs razor, penalising complex models when comparing them.<br>
</li>
<li class="fragment">They quantify relative support for multiple models.<br>
</li>
<li class="fragment">Allow model-averaged predictions over parameters.</li>
<li class="fragment">Bayes factors can provide evidence in favor of the null hypothesis (unlike frequentist)</li>
<li class="fragment">Bayes factors can be forever updated as data arrive (no fixed sample size or plan required)</li>
<li class="fragment">Easy to understand. How many more times likely the data is for one model than another. Thats it. What was the p-value again?</li>
</ul>
</section>
<section id="extraordinary-claims-require-extraordinary-evidence" class="slide level2">
<h2>Extraordinary Claims Require Extraordinary Evidence</h2>
<ul>
<li class="fragment">Echoing Hume, Laplace, and Sagan: extraordinary claims require extraordinary evidence.</li>
<li class="fragment">This principle is baked into Bayesian methods through <strong>posterior odds</strong>:<br>
<span class="math inline">\(\text{Posterior odds} = \text{Bayes factor} \times \text{prior odds}\)</span></li>
<li class="fragment">The prior plausibility of a model matters.</li>
<li class="fragment">Even strong evidence may not overturn implausible claims.</li>
<li class="fragment">Bayesian methods make explicit the influence of prior beliefs. Thats a good thing.</li>
</ul>
</section>
<section id="problems-with-p-values" class="slide level2">
<h2>Problems with p-values</h2>
<ul>
<li class="fragment">p-values often misinterpreted</li>
<li class="fragment">ask your supervisor to define p-values. Most can‚Äôt.</li>
<li class="fragment">p-values do not provide evidence for the null.</li>
<li class="fragment">depend on experimenter‚Äôs intentions, and what they would do, if the data had turned out differently, as well as alternative hypotheses.</li>
<li class="fragment">Classical hypothesis testing is asymmetric:</li>
<li class="fragment">Null can be rejected, but never confirmed.</li>
</ul>
</section>
<section id="optional-stopping" class="slide level2">
<h2>Optional Stopping</h2>
<ul>
<li class="fragment">Bayesian methods allow data collection to stop at any time based on evidence.</li>
<li class="fragment">p-value methods require pre-specified stopping rules.</li>
<li class="fragment">Researchers can continue or stop based on interim Bayes factor values.</li>
<li class="fragment">Provides more flexibility and transparency.</li>
</ul>
</section>
<section id="challenges-for-bayesian-approach" class="slide level2">
<h2>Challenges for Bayesian Approach</h2>
<ul>
<li class="fragment">Conceptual: Sensitivity to <strong>prior distributions</strong>.</li>
<li class="fragment">Computational: Difficulty in calculating <strong>marginal likelihoods</strong>.</li>
<li class="fragment">Using vague priors can lead to low-precision predictions.</li>
<li class="fragment">The Ockham‚Äôs razor property penalizes overly vague models.</li>
<li class="fragment">Priors must reflect meaningful knowledge ‚Äî vague priors yield unhelpful results.</li>
<li class="fragment">Vague priors entail complex models</li>
</ul>
</section>
<section id="specifying-good-priors" class="slide level2">
<h2>Specifying Good Priors</h2>
<ul>
<li class="fragment">Researchers must encode prior knowledge into prior distributions.</li>
<li class="fragment">Options:
<ul>
<li class="fragment">Subjective specification: Based on domain expertise.</li>
<li class="fragment">Objective priors: E.g., unit-information priors, do not rely on specific prior knowledge.</li>
</ul></li>
<li class="fragment">Objective priors useful for:
<ul>
<li class="fragment">Wide applicability.</li>
<li class="fragment">Transparent baseline comparisons.</li>
<li class="fragment">Refinement with specific information when needed.</li>
</ul></li>
</ul>
</section>
<section id="confusion-about-priors" class="slide level2">
<h2>Confusion About Priors</h2>
<ul>
<li class="fragment">Common misconception: Bayes factor depends on the prior plausibility of the models.</li>
<li class="fragment">In truth:
<ul>
<li class="fragment">Bayes factor is unaffected by prior probabilities of the models.</li>
<li class="fragment">It does depend on the prior over model parameters (e.g., effect size).</li>
</ul></li>
<li class="fragment">Important distinction:
<ul>
<li class="fragment">Prior on models affects posterior probabilities but not bayes factor.</li>
<li class="fragment">Prior on parameters affects the Bayes factor itself and hence the posterior odds.</li>
</ul></li>
</ul>
</section>
<section id="prior-sensitivity" class="slide level2">
<h2>Prior Sensitivity</h2>
<ul>
<li class="fragment">When different parameter priors yield different conclusions, it shows scientific uncertainty.</li>
<li class="fragment">Strategies to handle prior sensitivity:
<ul>
<li class="fragment">Local/intrinsic/fractional/partial Bayes factors.</li>
<li class="fragment">Sensitivity analyses: Vary the priors on parameters and observe effects on conclusions.</li>
</ul></li>
<li class="fragment">Reminder: Some models are robust, others are fragile to prior assumptions.</li>
</ul>
</section>
<section id="computational-challenges" class="slide level2">
<h2>Computational Challenges</h2>
<ul>
<li class="fragment">Computing marginal likelihoods is hard for complex models.</li>
<li class="fragment">Approximate methods include:
<ul>
<li class="fragment">Candidate‚Äôs formula.</li>
<li class="fragment">Basic marginal likelihood identity: <span class="math inline">\(p(D \mid M_1) = \frac{p(D \mid \theta, M_1) p(\theta \mid M_1)}{p(\theta \mid D, M_1)}\)</span></li>
</ul></li>
<li class="fragment">MCMC-based methods:
<ul>
<li class="fragment">Sample from posterior, evaluate likelihoods.</li>
<li class="fragment">Use <strong>model indicator variables</strong> for model comparison.</li>
</ul></li>
</ul>
<!-- #endregion                           -->
<!-- #region Lecture#9 Savage Dickey              -->
</section></section>
<section>
<section id="savage-dickkey-method-of-model-comparison" class="title-slide slide level1 center">
<h1>Savage-Dickkey method of model comparison</h1>
<ul>
<li class="fragment">In this method two models are compared:
<ul>
<li class="fragment"><strong>Null hypothesis (<span class="math inline">\(H_0\)</span>)</strong>: fixes parameter to a specific value, e.g., <span class="math inline">\(\phi = \phi_0\)</span></li>
<li class="fragment"><strong>Alternative hypothesis (<span class="math inline">\(H_1\)</span>)</strong>: parameter free to vary, e.g., <span class="math inline">\(\phi \ne \phi_0\)</span></li>
</ul></li>
<li class="fragment"><span class="math inline">\(H_0\)</span> is nested within <span class="math inline">\(H_1\)</span> (by constraining parameter).</li>
<li class="fragment">Classical null hypothesis usually sharp (point-null).</li>
</ul>
</section>
<section id="savagedickey-density-ratio" class="slide level2">
<h2>Savage‚ÄìDickey Density Ratio</h2>
<ul>
<li class="fragment">Defines Bayes factor for nested models: <span class="math inline">\(BF_{01} = \frac{p(D \mid H_0)}{p(D \mid H_1)} = \frac{p(\phi = \phi_0 \mid D, H_1)}{p(\phi = \phi_0 \mid H_1)}\)</span></li>
<li class="fragment">Simply the ratio of posterior to prior densities at the point of interest <span class="math inline">\(\phi_0\)</span> under the alternative hypothesis.</li>
</ul>
</section>
<section id="example-binomial-scenario" class="slide level2">
<h2>Example: Binomial Scenario</h2>
<ul>
<li class="fragment">Binomial scenario: <span class="math inline">\(\theta\)</span> parameter, observing 9 correct and 1 incorrect response.</li>
<li class="fragment">Null hypothesis (<span class="math inline">\(H_0\)</span>): <span class="math inline">\(\theta = 0.5\)</span></li>
<li class="fragment">Alternative hypothesis (<span class="math inline">\(H_1\)</span>): <span class="math inline">\(\theta\)</span> free to vary, prior <span class="math inline">\(\theta \sim Beta(1,1)\)</span></li>
<li class="fragment">Bayes factor is the ratio of posterior and prior densities at <span class="math inline">\(\theta=0.5\)</span></li>
</ul>
</section>
<section id="visual-interpretation-of-savage-dickey" class="slide level2">
<h2>Visual Interpretation of Savage-Dickey</h2>
<ul>
<li class="fragment"><img data-src="Fig7.1-example.jpg"></li>
<li class="fragment">Prior (uniform) and posterior distributions shown.</li>
<li class="fragment">Density ratio at <span class="math inline">\(\theta=0.5\)</span> gives Bayes factor.</li>
</ul>
</section>
<section id="mcmc-based-estimation-for-savage-dickey" class="slide level2">
<h2>MCMC-Based Estimation for Savage-Dickey</h2>
<ul>
<li class="fragment">When analytical solutions are difficult, use MCMC: -<img data-src="Fig7.2-MCMC-example.jpg"></li>
<li class="fragment">Posterior and prior estimated from MCMC samples.</li>
<li class="fragment">Heights of posterior and prior at the null point give Bayes factor.</li>
</ul>
</section>
<section id="advantages-of-savagedickey" class="slide level2">
<h2>Advantages of Savage‚ÄìDickey</h2>
<ul>
<li class="fragment">Direct interpretation as density ratio.</li>
<li class="fragment">Simplifies computation ‚Äîno separate marginal likelihood calculation needed.</li>
<li class="fragment">Works well for nested models.</li>
</ul>
<!-- #endregion    -->
<!-- #region Lecture#10 Compare guassian means    -->
</section></section>
<section>
<section id="commpare-gaussian-means" class="title-slide slide level1 center">
<h1>Commpare Gaussian means</h1>
<ul>
<li class="fragment">Common task: test if two Gaussian means differ</li>
<li class="fragment">Example: does glucose improve detection performance</li>
<li class="fragment">Focus: test claim that glucose boost has larger effect in summer</li>
</ul>
</section>
<section id="data" class="slide level2">
<h2>Data</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Season</th>
<th>N</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Winter</td>
<td>41</td>
<td>0.11</td>
<td>0.15</td>
</tr>
<tr class="even">
<td>Summer</td>
<td>41</td>
<td>0.07</td>
<td>0.23</td>
</tr>
</tbody>
</table>
<ul>
<li class="fragment">Difference not significant<br>
</li>
<li class="fragment">t = 0.79, p = 0.44</li>
</ul>
</section>
<section id="p-values-and-the-null-hypothesis" class="slide level2">
<h2>p-values and the Null Hypothesis</h2>
<ul>
<li class="fragment">‚ÄúFrom a null result, we cannot conclude that no difference exists‚Ä¶‚Äù</li>
<li class="fragment">p = 0.44 does not support H‚ÇÄ</li>
<li class="fragment">It just means data are not incompatible with H‚ÇÄ</li>
<li class="fragment">Need a Bayes factor to quantify support for H‚ÇÄ</li>
</ul>
</section>
<section id="bayes-factor-overview" class="slide level2">
<h2>Bayes Factor Overview</h2>
<ul>
<li class="fragment">Bayes factor compares posterior vs prior odds</li>
<li class="fragment">Quantifies evidence for or against H‚ÇÄ</li>
<li class="fragment">Unlike p-values, can support H‚ÇÄ</li>
</ul>
</section>
<section id="one-sample-comparison-model" class="slide level2">
<h2>One-Sample Comparison Model</h2>
<ul>
<li class="fragment">Test standardized difference scores (e.g., winter - summer)</li>
<li class="fragment">Assume:
<ul>
<li class="fragment">Œ¥ ~ Cauchy(0,1)</li>
<li class="fragment">x·µ¢ ~ Gaussian(Œº, 1/œÉ¬≤)</li>
<li class="fragment">Œº = Œ¥œÉ</li>
</ul></li>
</ul>
</section>
<section id="one-sample-graphical-model" class="slide level2">
<h2>One-Sample Graphical Model</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/graphical_model_onesample.png"></p>
<figcaption>Fig 8.1</figcaption>
</figure>
</div></li>
<li class="fragment">Prior on Œ¥: Cauchy(0,1)</li>
<li class="fragment">Prior on œÉ: Half-Cauchy</li>
<li class="fragment">Estimate posterior with MCMC</li>
</ul>
</section>
<section id="posterior-vs-prior" class="slide level2">
<h2>Posterior vs Prior</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/posterior_prior_onesample.png"></p>
<figcaption>Figure 8.2</figcaption>
</figure>
</div></li>
<li class="fragment">Posterior peaks near Œ¥ = 0</li>
<li class="fragment">Bayes Factor ‚âà 5:1 in favor of H‚ÇÄ</li>
</ul>
</section>
<section id="order-restricted-model" class="slide level2">
<h2>Order-Restricted Model</h2>
<ul>
<li class="fragment">SMM predicts <strong>Œ¥ &lt; 0</strong></li>
<li class="fragment">Use order-restricted prior:
<ul>
<li class="fragment">Œ¥ ~ Cauchy(0,1) truncated to (-‚àû, 0)</li>
</ul></li>
</ul>
</section>
<section id="updated-bayes-factor" class="slide level2">
<h2>Updated Bayes Factor</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/posterior_prior_orderrestricted.png"></p>
<figcaption>Figure 8.4</figcaption>
</figure>
</div></li>
<li class="fragment">Stronger evidence for H‚ÇÄ: BF ‚âà 10:1</li>
</ul>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">p-values can‚Äôt confirm H‚ÇÄ</li>
<li class="fragment">Bayes factors can</li>
<li class="fragment">‚ÄúEvidence of absence‚Äù of support for SMM‚Äôs prediction</li>
</ul>
</section>
<section id="two-sample-comparison" class="slide level2">
<h2>Two-Sample Comparison</h2>
<ul>
<li class="fragment">Compare oxygenated vs plain water on memory</li>
<li class="fragment">Two independent groups</li>
</ul>
</section>
<section id="two-sample-model-structure" class="slide level2">
<h2>Two-Sample Model Structure</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/graphical_model_twosample.png"></p>
<figcaption>Figure 8.5</figcaption>
</figure>
</div></li>
<li class="fragment">Shared variance œÉ¬≤</li>
<li class="fragment">Œ¥ = Œ± / œÉ<br>
</li>
<li class="fragment">Œ± = Œº‚Çì - Œº·µß</li>
</ul>
</section>
<section id="large-effect-example" class="slide level2">
<h2>Large Effect Example</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Group</th>
<th>N</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Plain Water</td>
<td>20</td>
<td>68.35</td>
<td>6.38</td>
</tr>
<tr class="even">
<td>Oxygenated</td>
<td>20</td>
<td>76.65</td>
<td>4.06</td>
</tr>
<tr class="odd">
<td>t(38) = 4.47, p &lt; .01</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="two-sample-bayes-factor" class="slide level2">
<h2>Two-Sample Bayes Factor</h2>
<p>-<img data-src="figures/posterior_prior_twosample.png" alt="Figure 8.6"> - Posterior moves away from 0 - BF ‚âà 447:1 in favor of H‚ÇÅ<br>
- Decisive evidence for oxygenated water effect</p>
<!-- #endregion   -->
<!-- #region Lecture#11 Compare binomial rates    -->
</section></section>
<section>
<section id="comparing-binomial-rates" class="title-slide slide level1 center">
<h1>Comparing binomial rates</h1>
<ul>
<li class="fragment">We will naturally compute binomial rates for different groups or conditions</li>
<li class="fragment">And ask which is larger?</li>
<li class="fragment">We thus need to compare binomial rates and test hypotheses about which is bigger etc.</li>
</ul>
</section>
<section id="bayesian-graphical-model" class="slide level2">
<h2>Bayesian graphical model</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/Fig9.1.png"></p>
<figcaption>Figure 9.1</figcaption>
</figure>
</div></li>
<li class="fragment">graphical model for comparing two proportions</li>
</ul>
</section>
<section id="bayesian-model" class="slide level2">
<h2>Bayesian model</h2>
<ul>
<li class="fragment">We model the observed counts using binomial likelihoods and assign uniform Beta priors: s1 ~ Binomial(theta1, n1)<br>
s2 ~ Binomial(theta2, n2)<br>
theta1 ~ Beta(1, 1)<br>
theta2 ~ Beta(1, 1)<br>
delta &lt;- theta1 - theta2</li>
<li class="fragment">theta1:</li>
<li class="fragment">theta2:</li>
<li class="fragment">delta = theta1 - theta2: difference in proportions</li>
<li class="fragment">We are interested in the posterior distribution of delta.</li>
</ul>
</section>
<section id="model-code" class="slide level2">
<h2>Model Code</h2>
<p>Here is the model used for posterior simulation:</p>
<pre><code>model {
  theta1 ~ dbeta(1,1)
  theta2 ~ dbeta(1,1)
  delta &lt;- theta1 - theta2

  s1 ~ dbin(theta1, n1)
  s2 ~ dbin(theta2, n2)

  theta1prior ~ dbeta(1,1)
  theta2prior ~ dbeta(1,1)
  deltaprior &lt;- theta1prior - theta2prior
}</code></pre>
<p>This allows us to compare the prior and posterior density of delta at zero.</p>
</section>
<section id="prior-and-posterior-distributions" class="slide level2">
<h2>Prior and Posterior Distributions</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/Fig9.2.png"></p>
<figcaption>Figure 9.2</figcaption>
</figure>
</div></li>
<li class="fragment">We estimate the posterior distribution for the rate difference delta = theta1 - theta2 using Bayesian inference.</li>
<li class="fragment">The left plot shows prior and posterior distributions for delta across its full range.</li>
<li class="fragment">The right plot zooms in near delta = 0.</li>
<li class="fragment">This is used in the Savage‚ÄìDickey density ratio to compute the Bayes factor.</li>
<li class="fragment">The Savage‚ÄìDickey method compares: BF_01 = prior density at delta = 0 / posterior density at delta = 0</li>
</ul>
</section>
<section id="interpreting-the-bayes-factor" class="slide level2">
<h2>Interpreting the Bayes Factor</h2>
<ul>
<li class="fragment">The posterior density at delta = 0 is about half the prior density.</li>
<li class="fragment">This gives a Bayes factor ‚âà 2 in favor of the alternative hypothesis H1: delta ‚â† 0.</li>
<li class="fragment">The 95% credible interval for delta is approximately [-0.09, 0.01], which does not include 0.</li>
</ul>
<p><strong>Interpretation</strong>:</p>
<ul>
<li class="fragment">There is only modest evidence that one rate is higher than another</li>
<li class="fragment">The Bayes factor penalizes H1 for spreading prior mass over implausible values.</li>
</ul>
<!-- #endregion    -->
<!-- #region Lecture#12 Memory retention-->
<!-- #endregion    -->
<!-- #region Lecture#13 Psychophysical function fitting-->
<!-- #endregion    -->

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://mybinder.org/v2/gh/ollie-hulme/BayesianModels/main?urlpath=lab/tree/BayesianModels" target="_blank"> mybinder.org/v2/gh/ollie-hulme/BayesianModels </a>&nbsp;&nbsp;&nbsp; <a href="https://github.com/ollie-hulme/BayesianModels" target="_blank"> github.com/ollie-hulme/BayesianModels </a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="BayesianModels_slides_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="BayesianModels_slides_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>