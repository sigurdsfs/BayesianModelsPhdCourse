<!DOCTYPE html>
<html lang="en"><head>
<script src="BayesianModels_slides_files/libs/clipboard/clipboard.min.js"></script>
<script src="BayesianModels_slides_files/libs/quarto-html/tabby.min.js"></script>
<script src="BayesianModels_slides_files/libs/quarto-html/popper.min.js"></script>
<script src="BayesianModels_slides_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="BayesianModels_slides_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="BayesianModels_slides_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="BayesianModels_slides_files/libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.42">

  <title>bayesianmodels_slides</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="BayesianModels_slides_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="BayesianModels_slides_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="BayesianModels_slides_files/libs/revealjs/dist/theme/quarto-2f366650f320edcfcf53d73c80250a32.css">
  <link rel="stylesheet" href="BayesianModels_styles.css">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="BayesianModels_slides_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">


<section class="slide level2">

<!-- #region Title slide -->
</section>
<section class="slide level2">

<div style="display: flex; flex-direction: column; align-items: center; height: 100vh; padding-top: 60px; text-align: center;">
<div style="font-size:1.6em; font-weight:bold; margin-bottom:10px; margin-top:10px;">
<pre><code>Bayesian Models of Brains, Minds, &amp; Behaviors </code></pre>
</div>
<div style="font-size:1.1em; margin-bottom:15px;">
<pre><code>DRCMR · Copenhagen · May 2025 </code></pre>
</div>
<div style="font-size:0.9em; max-width: 80%; margin: 0 auto;">
<pre><code>Ollie Hulme · David Meder · Janine Bühler · Melissa Larsen
    Amin Kangavari · Simon Steinkamp · Naiara Demnitz</code></pre>
</div>
<p><img src="images/DRCMR_regionH_KU_logo.png" alt="DRCMR logo" style="height:80px; margin-top:15px;"></p>
</div>
</section>
<section class="slide level2">

<!-- #endregion -->
<!-- #region Lecture#1 Preamble -->
</section>
<section>
<section id="preamble" class="title-slide slide level1 center">
<h1>Preamble</h1>
<ul>
<li class="fragment">How the week will go</li>
<li class="fragment">Materials<br>
</li>
<li class="fragment">Schedule<br>
</li>
<li class="fragment">Social 😊<br>
</li>
<li class="fragment">Aims &amp; expectations</li>
</ul>
</section>
<section id="course-overview" class="slide level2">
<h2>Course overview</h2>
<ul>
<li class="fragment"><strong>Basic modelling</strong> → Get started</li>
<li class="fragment"><strong>Intermediate modelling</strong> → Go deeper</li>
<li class="fragment"><strong>Integrate neural data</strong> → Bridge brain &amp; behavior</li>
<li class="fragment"><strong>Group presentations</strong> → Show what you built</li>
<li class="fragment">↓</li>
<li class="fragment">lectures · interactive exercises · discussion · case studies · group work</li>
</ul>
</section>
<section id="group-work-presentations" class="slide level2">
<h2>Group work &amp; presentations</h2>
<ul>
<li class="fragment"><strong>Separate</strong> into small groups</li>
<li class="fragment"><strong>Supervision</strong> from multiple experts</li>
<li class="fragment"><strong>Pick question</strong> that requires cognitive modelling</li>
<li class="fragment"><strong>Design experiment</strong> that collects behavioral &amp; neural data</li>
<li class="fragment"><strong>Design models</strong>, sketch graphical diagrams, plan analyses to test hypotheses &amp; answer question</li>
<li class="fragment"><strong>Present projects</strong>, share your work, ~15 mins on fri</li>
<li class="fragment"><strong>Audience</strong> questions &amp; discussion</li>
</ul>
</section>
<section id="materials-interactive-exercises" class="slide level2">
<h2>Materials: Interactive exercises</h2>
<ul>
<li class="fragment">Run interactive python code via Binder<br>
</li>
<li class="fragment">No installation needed → just open in browser and go 😌<br>
</li>
<li class="fragment">Loading takes time → keep your tab open</li>
<li class="fragment"><div style="text-align: center;">
<p><img src="/images/binder.png" alt="binder screenshot" width="300px"></p>
</div></li>
<li class="fragment">link in footer 👇</li>
</ul>
</section>
<section id="materials-github" class="slide level2">
<h2>Materials: GitHub</h2>
<ul>
<li class="fragment">All materials + code on GitHub</li>
<li class="fragment"><div style="text-align: center;">
<p><img src="/images/github.png" alt="github screenshot" width="300px"></p>
</div></li>
<li class="fragment">link in footer 👇</li>
</ul>
</section>
<section id="materials-book" class="slide level2">
<h2>Materials: Book</h2>
<ul>
<li class="fragment"><div style="text-align: center;">
<img src="/images/book.jpeg" alt="book screenshot" width="300px">
</div></li>
<li class="fragment">We rely heavily on it especially on days 1-3</li>
</ul>
</section>
<section id="schedule" class="slide level2">
<h2>Schedule</h2>
<ul>
<li class="fragment">Up to date schedule &gt; See the github readme</li>
</ul>
</section>
<section id="social-whatsapp" class="slide level2">
<h2>Social: WhatsApp</h2>
<ul>
<li class="fragment">Chat with each other 💬</li>
<li class="fragment">Ask &amp; answer questions amongst yourselves 🗣️</li>
<li class="fragment">Stay in touch ❤️</li>
<li class="fragment">Friday bar! 😄</li>
<li class="fragment">Join group via 👇</li>
<li class="fragment"><img src="/images/whatsapp.png" width="300px"></li>
</ul>
</section>
<section id="overarching-aim" class="slide level2">
<h2>Overarching aim</h2>
<ul>
<li class="fragment"><strong>Use the tools of probability theory to scientifically understand brains, minds, and behavior</strong></li>
<li class="fragment"><strong>New perspective</strong>. Upgrade your scientific thinking</li>
<li class="fragment"><strong>Simplicity.</strong> Tools are parsimonious, flexible, intuitive, and powerful.</li>
<li class="fragment"><strong>Universality.</strong> Same principles apply to science, statistic, brains, minds, behaviors, evolution, even physics.</li>
</ul>
</section>
<section id="specific-aims" class="slide level2">
<h2>Specific aims</h2>
<ul>
<li class="fragment"><strong>Focus on mind &amp; behavior</strong> → Cognitive models</li>
<li class="fragment"><strong>Connect to neural data</strong> via simple methods 🧠</li>
<li class="fragment"><strong>Stop describing 🥱 → Start explaining 😌</strong></li>
<li class="fragment"><strong>Engage.</strong> Hands-on · Interactive · Discursive</li>
<li class="fragment"><strong>KISS.</strong> Keep It Simple, Stoopid 😜</li>
<li class="fragment"><strong>Vibes.</strong> Intuitive · Playful · Fun · Useful ✨</li>
</ul>
</section>
<section id="our-expectations-of-you" class="slide level2">
<h2>Our expectations of you:</h2>
<ul>
<li class="fragment"><strong>Life is short.</strong> Don’t things pass that you dont understand.</li>
<li class="fragment"><strong>Your life matters.</strong> Your understanding matters. Actively take part.</li>
<li class="fragment"><strong>You are entitled</strong> to understand this.</li>
<li class="fragment"><strong>Don’t listen</strong> to it, fight it. 🥊</li>
<li class="fragment"><strong>Disrupt.</strong> Ask questions, interrupt. 🙋‍♂️</li>
<li class="fragment"><strong>Be sceptical.</strong> The truth is out there. 🤨</li>
</ul>
</section>
<section id="we-love-to-hear" class="slide level2">
<h2>We love to hear:</h2>
<ul>
<li class="fragment"><em>“I’m probably being stupid…”</em> 🤪</li>
<li class="fragment"><em>“I might have missed this…”</em></li>
<li class="fragment"><em>“i dont understand why…”</em></li>
<li class="fragment"><em>“do you have an intuition for why…”</em></li>
<li class="fragment"><em>“i’m confused…”</em> 🤔</li>
<li class="fragment"><strong>If you are shy.</strong> Say it in the break / ask in whatsApp</li>
</ul>
</section>
<section id="dont-do-this-to-yourself" class="slide level2">
<h2>Don’t do this to yourself:</h2>
<ul>
<li class="fragment">Nod and pretend you get something 🙅‍♂️</li>
<li class="fragment">Assume you are the only one confused 😵‍💫😵</li>
<li class="fragment">Collapse under self doubt. <!-- #endregion--></li>
</ul>
<!-- #region Lecture#2 Basics of Bayesian analysis-->
</section></section>
<section>
<section id="basics-of-bayesian-analysis" class="title-slide slide level1 center">
<h1>Basics of Bayesian Analysis</h1>
<ul>
<li class="fragment">The spirit of Bayesian thinking</li>
<li class="fragment">What is Bayesian modeling?</li>
<li class="fragment">Principles of Bayesian inference</li>
<li class="fragment">Observable vs.&nbsp;latent variables</li>
<li class="fragment">Beliefs and evidence</li>
<li class="fragment">Estimation methods</li>
<li class="fragment">Why Bayesian methods?</li>
</ul>
</section>
<section id="the-spirit-of-bayesian-thinking" class="slide level2">
<h2>The spirit of Bayesian thinking</h2>
<ul>
<li class="fragment"><br></li>
<li class="fragment"><em>“Probability theory is nothing but common sense reduced to calculation.”</em> — Laplace (1814)<br>
</li>
<li class="fragment"><br></li>
<li class="fragment"><em>“The rules of probability are the rules of consistent reasoning.”</em> — Jaynes (2003)<br>
</li>
<li class="fragment"><br></li>
<li class="fragment"><em>“Bayesian methods are not a special brand of inference; they are the only logically consistent rules for inference that are known.”</em> — Jaynes (2003)</li>
</ul>
<div class="footer">
<p>Laplace (1814); Jaynes (2003)</p>
</div>
</section>
<section id="bayesianism-as-the-calculus-of-common-sense" class="slide level2">
<h2>Bayesianism as the calculus of common sense</h2>
<ul>
<li class="fragment"><br><br></li>
<li class="fragment"><em>Bayesianism is just probability theory applied to inference.</em> — Jaynes (2003)</li>
</ul>
<div class="footer">
<p>Jaynes (2003)</p>
</div>
</section>
<section id="probability-as-rational-consistency" class="slide level2">
<h2>Probability as rational consistency</h2>
<ul>
<li class="fragment">Bayesian modeling follows the rules of probability.</li>
<li class="fragment">Probability is logic.</li>
<li class="fragment">Logic is consistency.</li>
<li class="fragment">Consistency is rationality.</li>
<li class="fragment">And rationality is just thinking clearly.</li>
</ul>
</section>
<section id="so-what-is-bayesian-modeling-of-minds-brains-and-behavior" class="slide level2">
<h2>So what is “Bayesian Modeling of Minds, Brains, and Behavior”?</h2>
<ul>
<li class="fragment">This course is about <strong>thinking clearly about minds, brains, and behavior</strong>.</li>
<li class="fragment">It’s about testing theories rationally, using the evidence provided by data.</li>
<li class="fragment">Bayesian modeling offers a principled, rational way to update beliefs based on evidence.</li>
<li class="fragment">Ta-da!</li>
</ul>
</section>
<section id="bayesian-updating-in-a-nutshell" class="slide level2">
<h2>Bayesian Updating in a Nutshell</h2>
<ul>
<li class="fragment">Prior belief → Evidence → Posterior belief</li>
</ul>
</section>
<section id="example-of-a-cognitive-task" class="slide level2">
<h2>Example of a cognitive task</h2>
<ul>
<li class="fragment">e.g., go/no-go</li>
<li class="fragment"><img src="/images/gonogo.png" width="800px"></li>
</ul>
</section>
<section id="cognitive-task-example" class="slide level2">
<h2>Cognitive task example</h2>
<ul>
<li class="fragment">10 binary trials of equal difficulty</li>
<li class="fragment">Estimate ability <span class="math inline">\(\theta\)</span> from behavior</li>
<li class="fragment"><span class="math inline">\(\theta\)</span> is <em>latent</em>;</li>
<li class="fragment">data are <em>observed</em></li>
<li class="fragment">e.g., correct responses <span class="math inline">\(k = 8\)</span> out of <span class="math inline">\(n = 10\)</span></li>
</ul>
</section>
<section id="latent-vs.-observed" class="slide level2">
<h2>Latent vs.&nbsp;observed</h2>
<ul>
<li class="fragment"><img src="/images/latent_observable.png" width="1000px"></li>
</ul>
</section>
<section id="why-latent-variables" class="slide level2">
<h2>Why latent variables?</h2>
<ul>
<li class="fragment">We want to explain, not just describe</li>
<li class="fragment"><em>Descriptive</em>: e.g.&nbsp;“What did the subject score?”</li>
<li class="fragment"><em>Explanatory</em>: e.g.&nbsp;“What ability caused that score?”</li>
</ul>
</section>
<section id="why-science-cares-about-the-latent" class="slide level2">
<h2>Why Science Cares About the Latent</h2>
<ul>
<li class="fragment">Scientific questions are causal:
<ul>
<li class="fragment">Do parkinsons patients differ in <em>risk taking</em> on and off medication?</li>
<li class="fragment">Does serotonin change <em>empathy</em>?</li>
<li class="fragment">Does alpha waves cause <em>memory consolidation</em>?</li>
<li class="fragment">Does ozempic improve <em>cognitive flexibility</em>?</li>
</ul></li>
<li class="fragment">We observe data, but we infer about latent causes</li>
<li class="fragment">Bayesian modeling connects observables to latent processes</li>
</ul>
</section>
<section id="back-to-the-cogntive-task" class="slide level2">
<h2>Back to the cogntive task</h2>
<ul>
<li class="fragment"><strong>Observed</strong>: number correct (<span class="math inline">\(k/n\)</span>)</li>
<li class="fragment"><strong>Latent</strong>: ability (<span class="math inline">\(\theta\)</span>)</li>
<li class="fragment">Same <span class="math inline">\(\theta\)</span> can result in different <span class="math inline">\(k/n\)</span></li>
<li class="fragment">Different <span class="math inline">\(\theta\)</span> can result in the same <span class="math inline">\(k/n\)</span></li>
<li class="fragment">Bayesian inference accounts for this uncertainty</li>
</ul>
</section>
<section id="beliefs-as-distributions" class="slide level2">
<h2>Beliefs as Distributions</h2>
<ul>
<li class="fragment">Probability distributions encode <strong>beliefs</strong></li>
<li class="fragment">The <strong>center</strong> = most likely value</li>
<li class="fragment">The <strong>spread</strong> = uncertainty</li>
</ul>
</section>
<section id="interactive-demo" class="slide level2">
<h2>Interactive demo</h2>
<ul>
<li class="fragment">Explore distributions as beliefs:<br>
</li>
<li class="fragment">📂 <code>notebooks/probability_distributions.ipynb</code></li>
</ul>
</section>
<section id="bayes-rule" class="slide level2">
<h2>Bayes’ rule</h2>
<ul>
<li class="fragment"><br></li>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment">Posterior = <span class="math inline">\(\frac{\color{red}{\text{Likelihood}}  \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment">This tells us how prior beliefs are updated via the evidence provided by the data.</li>
</ul>
</section>
<section id="term-by-term" class="slide level2">
<h2>Term-by-term</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment"><strong><span class="math inline">\(\color{blue}{\text{Prior}}\)</span></strong> is what we believe about <span class="math inline">\(\theta\)</span> before seeing the data.</li>
<li class="fragment">It reflects our assumptions or prior knowledge.</li>
</ul>
</section>
<section id="term-by-term-1" class="slide level2">
<h2>Term-by-term</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment"><strong><span class="math inline">\(\color{red}{\text{Likelihood}}\)</span></strong> is how likely the observed data <span class="math inline">\(D\)</span> is, given a particular value of <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">It tells us how well different values of <span class="math inline">\(\theta\)</span> explain the data.</li>
<li class="fragment">It tells us how to update our beliefs in each value of <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">Those <span class="math inline">\(\theta\)</span> that explain the data well should be believed in more</li>
<li class="fragment">Those <span class="math inline">\(\theta\)</span> that explain the data poorly should be believed in less</li>
</ul>
</section>
<section id="term-by-term-2" class="slide level2">
<h2>Term-by-term</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment"><strong><span class="math inline">\(\color{green}{\text{Marginal likelihood}}\)</span></strong> is the total probability of the data under all possible values of <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">It represents how good the model is at predicting the data.</li>
<li class="fragment">In doing so it normalizes the result so the posterior is a valid probability distribution.</li>
<li class="fragment">Note that it is independent of <span class="math inline">\(\theta\)</span></li>
</ul>
</section>
<section id="term-by-term-3" class="slide level2">
<h2>Term-by-term</h2>
<ul>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\text{Posterior} = \frac{\color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}}{\color{green}{\text{Marginal likelihood}}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment"><strong>Posterior</strong> is what we believe about <span class="math inline">\(\theta\)</span> <em>after</em> seeing the data.</li>
<li class="fragment">It’s the prior that has been updated by the evidence provided by the data.</li>
</ul>
</section>
<section id="updating-beliefs-with-evidence" class="slide level2">
<h2>Updating Beliefs with Evidence</h2>
<ul>
<li class="fragment">We start with a <strong>prior</strong> belief <span class="math inline">\(p(\theta)\)</span></li>
<li class="fragment">Observe <strong>data</strong>: e.g.&nbsp;( k = 9 ) correct out of ( n = 10 )</li>
<li class="fragment">Bayes updates the belief:</li>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span></li>
</ul>
</section>
<section id="intuition-behind-belief-updating" class="slide level2">
<h2>Intuition behind belief updating</h2>
<ul>
<li class="fragment">The more likely the data is for a given <span class="math inline">\(\theta\)</span><br>
</li>
<li class="fragment">The more we believe in that value of <span class="math inline">\(\theta\)</span> after seeing the data.</li>
<li class="fragment">The values of theta that are better supported by the data, are more believed in after experiencing the data</li>
<li class="fragment">We have updated our beliefs according to the data</li>
</ul>
</section>
<section id="proportional-form" class="slide level2">
<h2>Proportional form</h2>
<ul>
<li class="fragment">Since <span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span></li>
<li class="fragment">The <strong><span class="math inline">\(\color{green}{\text{Marginal likelihood } p(D)}\)</span></strong> doesnt depend on <span class="math inline">\(\theta\)</span></li>
<li class="fragment">So we can rewrite as:</li>
<li class="fragment"><span class="math inline">\(p(\theta|D) \propto p(D|\theta) \cdot p(\theta)\)</span></li>
<li class="fragment"><span class="math inline">\(\text{Posterior} \propto \color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}\)</span></li>
</ul>
</section>
<section id="proportional-form-1" class="slide level2">
<h2>Proportional form</h2>
<ul>
<li class="fragment">Since:<br>
<span class="math inline">\(p(\theta \mid D) = \frac{\color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}}{\color{green}{p(D)}}\)</span></li>
<li class="fragment"><br></li>
<li class="fragment">The <strong><span class="math inline">\(\color{green}{\text{Marginal likelihood}}\)</span></strong> <span class="math inline">\(\color{green}{p(D)}\)</span> doesn’t depend on <span class="math inline">\(\theta\)</span></li>
<li class="fragment"><br></li>
<li class="fragment">So we can rewrite as:<br>
</li>
<li class="fragment"><span class="math inline">\(p(\theta \mid D) \propto \color{red}{p(D \mid \theta)} \cdot \color{blue}{p(\theta)}\)</span></li>
<li class="fragment"><span class="math inline">\(\text{Posterior} \propto \color{red}{\text{Likelihood}} \cdot \color{blue}{\text{Prior}}\)</span></li>
<li class="fragment">“<em>Posterior is proportional to likelihood times prior</em>””</li>
</ul>
</section>
<section id="computing-posteriors" class="slide level2">
<h2>Computing posteriors</h2>
<ul>
<li class="fragment">Here is a notebook to step through updating priors according to data</li>
<li class="fragment">Go to “Multiplying prior and likelihood” 📂 <code>notebooks/probability distributions.ipynb</code></li>
<li class="fragment">See how beliefs update</li>
<li class="fragment">Explore summaries: MAP, mean, BCI</li>
</ul>
</section>
<section id="special-case" class="slide level2">
<h2>Special case</h2>
<ul>
<li class="fragment">Prior: <span class="math inline">\(p(\theta) \sim \text{Beta}(1,1)\)</span></li>
<li class="fragment">Posterior: <span class="math inline">\(p(\theta \mid D) \sim \text{Beta}(1 + k, 1 + n - k)\)</span></li>
<li class="fragment"><span class="math inline">\(k\)</span> = correct, <span class="math inline">\(n\)</span> = total trials</li>
<li class="fragment">Simple, tractable update rule for binary outcomes</li>
</ul>
</section>
<section id="try-it-yourself" class="slide level2">
<h2>Try it yourself</h2>
<ul>
<li class="fragment">Go to “Computing posterior: the special case of the beta distribution”</li>
<li class="fragment">📂 <code>notebooks/probability distributions.ipynb</code></li>
</ul>
</section>
<section id="sequential-updating" class="slide level2">
<h2>Sequential updating</h2>
<ul>
<li class="fragment">Bayesian inference is <strong>consistent</strong> across steps</li>
<li class="fragment">One-step:
<ul>
<li class="fragment">Prior → Combined data → Posterior</li>
</ul></li>
<li class="fragment">Two-step:
<ul>
<li class="fragment">Prior → Data1 → Intermediate Posterior → Data2 → Final Posterior</li>
</ul></li>
<li class="fragment">✅ Final result is <strong>identical</strong></li>
</ul>
</section>
<section id="why-sequential-updating-matters" class="slide level2">
<h2>Why Sequential Updating Matters</h2>
<ul>
<li class="fragment">Matches real-world data collection</li>
<li class="fragment">Enables learning as data rolls in</li>
<li class="fragment">Supports early stopping, or extending data collection</li>
<li class="fragment">Efficient: time, money, resources</li>
<li class="fragment">Ethical: minimises animals, humans, unnecessary treatment etc.</li>
</ul>
</section>
<section id="why-frequentist-methods-dont-allow-this" class="slide level2">
<h2>Why Frequentist Methods Don’t Allow This</h2>
<ul>
<li class="fragment">Frequentist inference assumes a fixed sample size and plan</li>
<li class="fragment">Stopping early or collecting more data <strong>invalidates p-values</strong></li>
<li class="fragment">Counterfactual policies also <strong>invalidates p-values</strong></li>
<li class="fragment">Frequentist methods lack a way to incorporate prior knowledge</li>
</ul>
</section>
<section id="conjugate-priors" class="slide level2">
<h2>Conjugate Priors</h2>
<ul>
<li class="fragment">If Prior and posterior from the same distribution family → <em>conjugate</em><br>
</li>
<li class="fragment"><br></li>
<li class="fragment">For example:</li>
<li class="fragment"><span class="math inline">\(p(\theta): \text{Beta}(\alpha, \beta)\)</span></li>
<li class="fragment"><span class="math inline">\(p(\theta|data): \text{Beta}(\alpha + k, \beta + n - k)\)</span></li>
<li class="fragment"><br></li>
<li class="fragment">The posterior can be computed by plugging data directly into an equation</li>
<li class="fragment">Conjugacy allows for <em>analytic updates</em></li>
</ul>
</section>
<section id="when-conjugacy-fails-use" class="slide level2">
<h2>When Conjugacy Fails: Use</h2>
<ul>
<li class="fragment">Conjugacy is relatively rare in real world cases</li>
<li class="fragment">In cases where conjugacy is not available sampling solutions are possible</li>
<li class="fragment">Commonly MCMC - Markov Chain Monte Carlo</li>
<li class="fragment">Works even when no closed-form solution</li>
<li class="fragment">Approximates the posterior via sampling</li>
</ul>
</section>
<section id="analytic-vs.-sampling" class="slide level2">
<h2>Analytic vs.&nbsp;Sampling</h2>
<ul>
<li class="fragment"><strong>Analytic</strong>: Exact, when conjugate</li>
<li class="fragment"><strong>MCMC</strong>: Approximate, more flexible</li>
</ul>
</section>
<section id="mcmc-in-practice" class="slide level2">
<h2>MCMC in Practice</h2>
<ul>
<li class="fragment"><strong>Red pill</strong>: Learn MCMC internals 🤓</li>
<li class="fragment">Intuitive guide: https://chat.openai.com/share/887b4e2d-2689-4e2d-b9ae-246b71d5782e</li>
<li class="fragment">For an intuitive explanation of Metropolis-Hastings:https://chat.openai.com/share/55cb6a00-1718-4327-8d51-3902a2064a11</li>
<li class="fragment"><br></li>
<li class="fragment"><strong>Blue pill</strong>: Trust the method and use it, (but know how to spot problems) 😄</li>
<li class="fragment">We will demonstrate and visualise MCMC</li>
</ul>
</section>
<section id="mcmc-demo-try-it-yourself" class="slide level2">
<h2>MCMC demo: Try it yourself</h2>
<ul>
<li class="fragment">Go to “MCMC demo”</li>
<li class="fragment">📂 <code>notebooks/probability distributions.ipynb</code></li>
</ul>
</section>
<section id="why-bayesian-methods" class="slide level2">
<h2>Why Bayesian Methods?</h2>
<ul>
<li class="fragment"><strong>Principled reasoning</strong>: Probabilistic logic = consistent thinking<br>
</li>
<li class="fragment"><strong>Uncertainty-aware</strong>: Fully models uncertainty<br>
</li>
<li class="fragment"><strong>Latent variables</strong>: Model hidden causes, not just outcomes<br>
</li>
<li class="fragment"><strong>Transparent updating</strong>: Prior → Evidence → Posterior</li>
<li class="fragment"><strong>Sequential updating</strong>: Updating is the same whether data is sequential or all-in-one-go.<br>
</li>
<li class="fragment"><strong>Richer explanations</strong>: From describing data to explaining via theory<br>
</li>
<li class="fragment"><strong>Flexible models</strong>: Hierarchical, generative, extensible</li>
<li class="fragment"><strong>Simple</strong>: Same principle always. Learn it once. Apply it forever.</li>
</ul>
<!-- #endregion-->
<!-- #region Lecture#3 Modelling a binary process-->
</section>
<section id="modeling-a-binary-process" class="slide level2">
<h2>Modeling a Binary Process</h2>
<ul>
<li class="fragment">Start simple: focus on binary outcomes.</li>
<li class="fragment">Each trial is a <em>Success/Failure</em>, <em>Correct/Incorrect</em>, <em>Yes/No</em>.</li>
<li class="fragment">Examples: Coin flips, true/false questions, detection tasks, motor responses.</li>
<li class="fragment">Binary processes are foundational for modeling cognition.</li>
</ul>
</section>
<section id="getting-started" class="slide level2">
<h2>Getting Started</h2>
<ul>
<li class="fragment">Let’s infer <strong>ability</strong> in a go/no-go task.</li>
<li class="fragment">We estimate a <strong>rate</strong> — as a hidden probability <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">We represent our uncertainty as a <strong>distribution</strong> over likely values of <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">Many cognitive tasks can be represented in this way.</li>
</ul>
</section>
<section id="binary-tasks-in-cognitive-science" class="slide level2">
<h2>Binary Tasks in Cognitive Science</h2>
<ul>
<li class="fragment">Go/no-go, stop-signal, 2AFC, task switching<br>
</li>
<li class="fragment">Recognition memory, Stroop, Flanker, oddball detection<br>
</li>
<li class="fragment">Visual search, discrimination tasks</li>
</ul>
</section>
<section id="from-binary-outcomes-to-hidden-rate" class="slide level2">
<h2>From Binary Outcomes to Hidden Rate</h2>
<ul>
<li class="fragment">Observe <span class="math inline">\(k\)</span> successes in <span class="math inline">\(n\)</span> trials → rate <span class="math inline">\(\frac{k}{n}\)</span></li>
<li class="fragment">But we want the <em>underlying</em> rate <span class="math inline">\(\theta\)</span></li>
<li class="fragment">Model: <span class="math inline">\(k \sim \text{Binomial}(\theta, n)\)</span><br>
</li>
<li class="fragment"><span style="color: lightgrey;"><span class="math inline">\(p(k \mid \theta, n) = \binom{n}{k} \theta^k (1 - \theta)^{n - k}\)</span></span></li>
<li class="fragment">Assumes no history effects (i.i.d. trials)</li>
</ul>
</section>
<section id="try-it-yourself-1" class="slide level2">
<h2>Try it yourself</h2>
<ul>
<li class="fragment">go to “Visualising the binomial distribution”</li>
<li class="fragment">📂 <code>notebooks/probability distributions.ipynb</code></li>
</ul>
</section>
<section id="graphical-models" class="slide level2">
<h2>Graphical models</h2>
<ul>
<li class="fragment">Graphical models are used to represent our probabilisitc model</li>
<li class="fragment">Nodes represent all the variables relevant to the problem</li>
<li class="fragment">Graph structure represent dependencies - children depend on parents</li>
<li class="fragment"><img data-src="images/graphical%20model.png" style="width:30.0%"></li>
</ul>
</section>
<section id="graphical-notation" class="slide level2">
<h2>Graphical notation</h2>
<ul>
<li class="fragment"><em>Circular</em> - continuous, <em>Square</em> - discrete</li>
<li class="fragment"><em>Shaded</em> - observed, <em>Non-shaded</em> - hidden</li>
<li class="fragment"><em>Single-boundary</em> - stochastic, <em>Double-boundary</em> - deterministic</li>
</ul>
</section>
<section id="graphical-notation-1" class="slide level2">
<h2>Graphical notation</h2>
<ul>
<li class="fragment"><img data-src="images/graphical%20lexicon.jpeg" style="width:70.0%"></li>
</ul>
</section>
<section id="tester" class="slide level2">
<h2>Tester</h2>
<ul>
<li class="fragment"><img data-src="images/graphical%20model%20test.jpeg" style="width:30.0%"></li>
<li class="fragment">Upper is what?</li>
<li class="fragment"><strong>continuous, stochastic, and observed</strong></li>
<li class="fragment">Lower is what?</li>
<li class="fragment"><strong>discrete, deterministic, and unobserved</strong></li>
</ul>
</section>
<section id="sampling-via-jags" class="slide level2">
<h2>Sampling via JAGS</h2>
<div class="sourceCode" id="cb4" data-code-line-numbers="0|1|2|3|4|5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href=""></a>model {              <span class="co"># opens the model</span></span>
<span id="cb4-2"><a href=""></a>  theta <span class="op">~</span> dbeta(<span class="dv">1</span>,<span class="dv">1</span>) <span class="co"># the prior: theta is distributed as a beta(1,1) </span></span>
<span id="cb4-3"><a href=""></a>  k <span class="op">~</span> dbin(theta,n)  <span class="co"># the likelihood: k is distrbuted binomially </span></span>
<span id="cb4-4"><a href=""></a>                     <span class="co"># ...with the theta and n as parameters</span></span>
<span id="cb4-5"><a href=""></a>}                    <span class="co"># this closes the model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="try-it-yourself-2" class="slide level2">
<h2>Try it yourself</h2>
<ul>
<li class="fragment">go to “Using JAGS to sample from the posterior for a binomial rate process”</li>
<li class="fragment">📂 <code>notebooks/probability distributions.ipynb</code></li>
</ul>
</section>
<section id="r-hat-as-a-convergence-check" class="slide level2">
<h2>R-hat as a convergence check</h2>
<ul>
<li class="fragment">It’s important to check that the sampling has converged to the stationary distribution.</li>
<li class="fragment">One heuristic is the R-hat statistic:<br>
<span class="math inline">\(\hat{R} = \frac{\text{var}(\text{within-chain})}{\text{var}(\text{across-chain})}\)</span></li>
<li class="fragment">Rule of thumb: <span class="math inline">\(\hat{R}\)</span> should be between 1.00 and 1.01 for convergence.</li>
<li class="fragment">You can check this for your chains using tools like ArviZ or Stan’s diagnostics.</li>
</ul>
<!-- #endregion -->
<!-- #region Lecture#3 Parameter estimation-->
</section></section>
<section>
<section id="lecture3-parameter-estimation" class="title-slide slide level1 center">
<h1>Lecture#3: Parameter estimation</h1>

</section>
<section id="posterior-from-the-last-example" class="slide level2">
<h2>Posterior from the last example</h2>
<ul>
<li class="fragment">We estimated the posterior distribution of <span class="math inline">\(\theta\)</span> given the data</li>
<li class="fragment">We sampled from this model</li>
<li class="fragment"><img data-src="fig2.1.jpg"></li>
<li class="fragment">and we got this distribution</li>
<li class="fragment"><img data-src="fig2.8.jpg"></li>
</ul>
</section>
<section id="conjugacy" class="slide level2">
<h2>Conjugacy</h2>
<ul>
<li class="fragment">Beta distributions has a natural interpretation</li>
<li class="fragment"><span class="math inline">\(\theta \sim text{Beta(\alpha, \beta)}\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> can be thought of as counts</li>
<li class="fragment"><span class="math inline">\(\alpha = 1 + text{successes} = 1 + k\)</span></li>
<li class="fragment">$= 1 + text{failures} = 1 + (n-k) $</li>
</ul>
</section>
<section id="why-1-count" class="slide level2">
<h2>Why 1 + count?</h2>
<ul>
<li class="fragment">Beta(1,1) is a uniform distribution so this is equivalent to 0 counts</li>
<li class="fragment">It is your belief distribution when you have no knowledge etc.<br>
</li>
<li class="fragment">Thus evidence accumulates from 1, via counting of successes and failures</li>
</ul>
</section>
<section id="conjugacy-1" class="slide level2">
<h2>Conjugacy</h2>
<ul>
<li class="fragment">Notice that the update from the prior to the posterior just comes from adding the counts to the beta distribution</li>
<li class="fragment">prior: Beta(1,1) → posterior: Beta(1+k, 1+(n-k))</li>
<li class="fragment">prior: Beta(1,1) → posterior: Beta(1+ successCount, 1+ failureCount)<br>
</li>
<li class="fragment">No sampling is needed because we have the equation</li>
<li class="fragment">When the prior and posterior have same type of distribution they are “conjugate”</li>
</ul>
</section>
<section id="conjugacy-interactive-example" class="slide level2">
<h2>Conjugacy interactive example</h2>
<ul>
<li class="fragment">Binder example- play with the beta distribution</li>
<li class="fragment">No sampling needed you just change inputs to the beta distribution to compute posterior</li>
</ul>
</section>
<section id="difference-between-two-rates" class="slide level2">
<h2>Difference between two rates</h2>
<ul>
<li class="fragment">Suppose we have to processes, say two tasks producing:</li>
<li class="fragment">k1 successes out of n1 trials</li>
<li class="fragment">k2 successes out of n2 trials</li>
<li class="fragment">let’s assume they are generated by two different rates <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span></li>
<li class="fragment">we want to know the difference in the rates <span class="math inline">\(\delta = \theta_1 - \theta_2\)</span></li>
<li class="fragment"><em>e.g.&nbsp;the effect of a drug on cognitive performance</em></li>
<li class="fragment"><em>e.g.&nbsp;the effect of a age on task performance</em></li>
</ul>
</section>
<section id="graphical-model-for-inferring-differences-in-rates" class="slide level2">
<h2>Graphical model for inferring differences in rates</h2>
<ul>
<li class="fragment"><img data-src="fig3.3.jpg"></li>
</ul>
</section>
<section id="sampling-code-for-infferring-differences-in-rates" class="slide level2">
<h2>Sampling code for infferring differences in rates</h2>
<!-- ``` {.python code-line-numbers="1-2|6-7|8-9|10"}
import pyjags
import numpy as np
model = """
model{ 
  k1 ~ dbin(theta1,n1)
  k2 ~ dbin(theta2,n2)
  theta1 ~ dbeta(1,1)
  theta2 ~ dbeta(1,1)
  delta <-theta1-theta2
}
"""
data = {'N': 10, 'y': np.random.randn(10)}
model = pyjags.Model(model, data=data, chains=3)
samples = model.sample(1000, vars=['mu', 'sigma'])
print(samples)
``` -->
</section>
<section id="sampled-posterior-for-difference-in-rates" class="slide level2">
<h2>Sampled posterior for difference in rates</h2>
<ul>
<li class="fragment"><img data-src="fig3.4.jpg"></li>
</ul>
</section>
<section id="interpreting-distributions" class="slide level2">
<h2>Interpreting distributions</h2>
<ul>
<li class="fragment">Probability mass functions are for discrete variables which take finite number of values</li>
<li class="fragment">Probability density functiosn for continuous variables which take infinitely many values</li>
</ul>
</section>
<section id="probability-mass-functions" class="slide level2">
<h2>Probability mass functions</h2>
<ul>
<li class="fragment">insert left figure from box 3.2</li>
<li class="fragment">all sum to 1</li>
</ul>
</section>
<section id="probability-density-functions" class="slide level2">
<h2>Probability density functions</h2>
<ul>
<li class="fragment">insert right figure from box 3.2</li>
<li class="fragment">area sums to 1</li>
<li class="fragment">densities can exceed 1</li>
<li class="fragment">only areas can be interpreted as probabilities</li>
</ul>
</section>
<section id="inferring-a-common-rate" class="slide level2">
<h2>Inferring a common rate</h2>
<ul>
<li class="fragment">In some cases we want to infer a rate for 2 processes</li>
<li class="fragment">e.g.&nbsp;same subject, same task, two different sessions</li>
<li class="fragment">Here we would model with a single <span class="math inline">\(\theta\)</span></li>
<li class="fragment"><img data-src="fig3.5.jpg"></li>
</ul>
</section>
<section id="same-model-with-plate-notation" class="slide level2">
<h2>Same model with plate notation</h2>
<ul>
<li class="fragment"><img data-src="fig3.6.jpg"></li>
<li class="fragment">note only one theta, multiple processes</li>
</ul>
</section>
<section id="code-for-inferring-a-common-rate" class="slide level2">
<h2>Code for inferring a common rate</h2>
<ul>
<li class="fragment">model{ k1 ~ dbin(theta1,n1) k2 ~ dbin(theta2,n2) theta ~ dbeta(1,1) }</li>
</ul>
</section>
<section id="predicting-data" class="slide level2">
<h2>Predicting data</h2>
<ul>
<li class="fragment">We know 2 distributions already</li>
<li class="fragment"><ol type="1">
<li class="fragment">Prior distribution of parameter e.g.&nbsp;<span class="math inline">\(p(\theta)\)</span></li>
</ol></li>
<li class="fragment">This is our prior prediction for what the parameters will be, prior to the data</li>
<li class="fragment"><ol start="2" type="1">
<li class="fragment">Posterior distribution of parameter having observed the data e.g.&nbsp;<span class="math inline">\(p(\theta|data)\)</span></li>
</ol></li>
<li class="fragment">This is our posterior prediction for the what the parameters will be, having seen the data</li>
<li class="fragment">Remember todays posterior is tomorrows prior, so they can be a prediction for the next round of data, ad infinitum.</li>
</ul>
</section>
<section id="predicting-data-1" class="slide level2">
<h2>Predicting data</h2>
<ul>
<li class="fragment">Priors and posteriors are of parameters. what about actually predicting data?</li>
<li class="fragment"><ol start="3" type="1">
<li class="fragment">Prior predictive distribution <span class="math inline">\(p(data)\)</span></li>
</ol></li>
<li class="fragment">this is the prediciton of what the data will be based on the prior, prior to the data being seen.</li>
<li class="fragment"><ol start="4" type="1">
<li class="fragment">Posterior predictive distribution <span class="math inline">\(p(new_data|old_data)\)</span></li>
</ol></li>
<li class="fragment">this is the prediciton of what the data will be based on the posterior, after the data has been seen.</li>
</ul>
</section>
<section id="prior-and-posterior-prediction" class="slide level2">
<h2>Prior and posterior prediction</h2>
<p>model { k ~ dbin(theta,n) theta~dbeta(1,1) postpredk~dbin(theta,n) thetaprior~dbeta(1,1) priorpredk~dbin(thetapior,n) }</p>
</section>
<section id="samples-of-the-four-distributions" class="slide level2">
<h2>Samples of the four distributions</h2>
<ul>
<li class="fragment"><img data-src="fig3.9.jpg"></li>
<li class="fragment">Note the prior and posterior are in the space of the parameter</li>
<li class="fragment">And the prior and posterior predictive distributions are in space of the data k out of n trials.</li>
</ul>
</section>
<section id="comparing-data-to-the-posterior-predictive-distribution" class="slide level2">
<h2>Comparing data to the posterior predictive distribution</h2>
<ul>
<li class="fragment">If we estimate the model along with its predictive distributions</li>
<li class="fragment">We can see how this compares to the actual data</li>
<li class="fragment"><img data-src="fig3.11.jpg"></li>
<li class="fragment">Here we see it’s not good. The model has poor <em>descriptive adequacy</em></li>
<li class="fragment">Why is it not a good model?</li>
<li class="fragment">Its a common rate model, so it predicts the same rate, but the data clearly is better modelled with different rates.</li>
</ul>
</section>
<section id="inference-and-time" class="slide level2">
<h2>Inference and time</h2>
<ul>
<li class="fragment">prediction naturally suggests thinking about the future.</li>
<li class="fragment">predictions can also apply to the past—for instance, when information is missing.</li>
<li class="fragment">data can help infer hidden cognitive variables, which in turn may predict past behavior where information is incomplete.</li>
<li class="fragment">similar to how historians reconstruct past events from limited evidence.</li>
<li class="fragment">or a court of law infers guilt based on available testimony and facts.</li>
<li class="fragment">we predict what might have been known in the past if more information had been available.</li>
<li class="fragment">Inference allows prediction both forward and backward in time.</li>
</ul>
</section>
<section id="todays-posterior-is-tomorrows-prior" class="slide level2">
<h2>Todays posterior is tomorrows prior</h2>
<ul>
<li class="fragment">Since the posterior distribution of parameters is continually updatable, then so is the posterior predictive distribution.</li>
<li class="fragment">As the posterior is updated, then so are its predictions for the data.</li>
<li class="fragment"><img data-src="box3.4.jpg"></li>
<li class="fragment">This can carry on forever. <!-- #endregion --></li>
</ul>
<!-- #region Lecture#4 Inference with Gaussians   -->
</section></section>
<section>
<section id="inferences-with-gaussians" class="title-slide slide level1 center">
<h1>Inferences with Gaussians</h1>
<ul>
<li class="fragment">Due to central limit theorem, data and parameters are frequently Gaussian</li>
<li class="fragment">Gaussians have 2 parameters, a mean and a measure of their spread</li>
<li class="fragment">Spread can be expressed as a variance, std, or a precision (1/var)</li>
</ul>
</section>
<section id="graphical-model-for-gaussians" class="slide level2">
<h2>Graphical model for Gaussians</h2>
<ul>
<li class="fragment">Simple model for inferring Gaussian with unknown mean and std</li>
<li class="fragment"><img data-src="fig.4.1.jpg"></li>
</ul>
</section>
<section id="interactive-demo-of-gaussian" class="slide level2">
<h2>Interactive demo of Gaussian</h2>
<ul>
<li class="fragment">Jupyter notebook - interactive plotting of gaussian</li>
</ul>
</section>
<section id="sampling-model-for-inferring-gaussians" class="slide level2">
<h2>Sampling model for inferring Gaussians</h2>
<p>model { for (i in 1:n){ x[i]~dnorm(mu,lambda) } mu~dnorm(0,0.001) sigma~dunif(0,10) lambda&lt;-1/pow(sigma,2) }</p>
</section>
<section id="repeated-measures-of-iq" class="slide level2">
<h2>Repeated measures of IQ</h2>
<ul>
<li class="fragment">Imagine taking a cognitive test like IQ multiple times</li>
<li class="fragment">The mean is your IQ, and the spread models fluctuations in your performance. e.g.&nbsp;attention, fatigue, emotion, venus orbitting satturn</li>
<li class="fragment">We can model this as a Gaussian for each person</li>
</ul>
</section>
<section id="graphical-model-for-iq" class="slide level2">
<h2>Graphical model for IQ</h2>
<ul>
<li class="fragment"><img data-src="fig.4.3.jpg"></li>
<li class="fragment">What parameter is common to all subjects?</li>
<li class="fragment">No index on the std. This means it is fixed.</li>
<li class="fragment">Is this justified?</li>
<li class="fragment">How to change it?</li>
</ul>
</section>
<section id="sampling-code-for-iq" class="slide level2">
<h2>Sampling code for IQ</h2>
<p>model{ for (i in 1:n) { for (j in 1:m) { x[i,j]~dnorm(mu[i],lambda) } } sigma~dunif(0,100) lambda &lt;-1/pow(sigma,2) for (i in 1:n) { mu([i]~ dunif(0,300)) }</p>
<p>} <!-- #endregion                            --></p>
<!-- #region Lecture#5 Latent Mixture models      -->
</section></section>
<section>
<section id="latent-mixture-models" class="title-slide slide level1 center">
<h1>Latent Mixture Models</h1>
<ul>
<li class="fragment">A latent mixture model is a model that assumes data are generated from a mixture of different hidden (latent) processes</li>
<li class="fragment">E.g. imagine a cogntive test where some people try their best, and others just guess</li>
<li class="fragment">the scores then are generated by two mechnamisms, ability and guessing</li>
<li class="fragment">how would we model this as a mixture of processes</li>
</ul>
</section>
<section id="latent-mixture-model-of-cognitive-test" class="slide level2">
<h2>Latent mixture model of cognitive test</h2>
<ul>
<li class="fragment">to model the “tryers” you would model their ability as a rate of correct performance, as before.</li>
<li class="fragment">to model the “guessers” you just model it as chance performance</li>
</ul>
</section>
<section id="latent-mixture-graphical-model" class="slide level2">
<h2>Latent mixture graphical model</h2>
<ul>
<li class="fragment"><img data-src="fig.6.1.jpg"></li>
<li class="fragment">Here z determines whether the person is a tryer or a guesser. Thus z is a parameter that models the mixture of processes that explain the data.</li>
<li class="fragment">We know the probability of guessing is 0.5</li>
<li class="fragment">The posterior distribution of z gives us the estimate of how many guessers there were.</li>
</ul>
</section>
<section id="latent-mixture-code" class="slide level2">
<h2>Latent mixture code</h2>
<p>model{ for (i in 1:p) { z[i]~dbern(0.5) } psi &lt;-0.5 phi ~ dbeta(1,1)I(0.5,1) for (i in 1:p) { theta[i]&lt;- equals(z[i],0)<em>psi+equals(z[i],1)</em>phi k[i]~dbin(theta[i],n) } } <!-- #endregion                             --></p>
<!-- #region Lecture#6 Model selection            -->
</section></section>
<section>
<section id="model-selection" class="title-slide slide level1 center">
<h1>Model selection</h1>
<ul>
<li class="fragment">Model selection is perhaps the most persuasive reason to choose Bayesian methods.</li>
<li class="fragment">Bayesian models provide a simple and principled way to choose between models balancing between accuracy and complexity</li>
</ul>
</section>
<section id="why-model-comparison" class="slide level2">
<h2>Why model comparison?</h2>
<ul>
<li class="fragment">so far we have covered single models</li>
<li class="fragment">in psychology/neuroscience we want to compare different models</li>
<li class="fragment">is this theory or this theory better at explaining the data</li>
<li class="fragment">if someone says this is the theory. it begs the question, compared to what?</li>
<li class="fragment">we want to know how well a theory explains relative to another, or many others.</li>
<li class="fragment">we need to compare models if we are to move beyond descriptive data analysis.</li>
</ul>
</section>
<section id="ptolomy" class="slide level2">
<h2>Ptolomy</h2>
<ul>
<li class="fragment">We consider it a good principle to explain the phenomena by the simplest hypothesis, provided this doesnt contradict the data in an important way</li>
</ul>
</section>
<section id="occams-razon" class="slide level2">
<h2>Occam’s razon</h2>
<ul>
<li class="fragment">“Plurarity must never be posited without necessity”</li>
<li class="fragment">This is nicknamed Occam’s razor, which cuts out needless complexity when it comes to theories or models or explanations</li>
</ul>
</section>
<section id="fristons-free-energy" class="slide level2">
<h2>Friston’s free energy</h2>
<ul>
<li class="fragment">“Minimizing free energy means finding the right balance between accuracy and complexity”</li>
<li class="fragment">“Good models should be as simple as possible, but not simpler than necessary to explain the data. Complexity must be minimized to avoid overfitting, yet sufficient to capture the underlying structure of the world.”</li>
</ul>
</section>
<section id="geoff-hinton" class="slide level2">
<h2>Geoff Hinton</h2>
<ul>
<li class="fragment">“Model complexity must pay for itself”</li>
<li class="fragment">Geoff is referring to the fact that model complexity must be justified by its ability to explain the data.</li>
</ul>
</section>
<section id="bayesian-magic" class="slide level2">
<h2>Bayesian magic</h2>
<ul>
<li class="fragment">Lots of methods exist for trying to get this balance right</li>
<li class="fragment">Out of sample prediction, parameter counting algorithms, blah, blah.</li>
<li class="fragment">Bayes nails it by finding a universal principle by which to compare models that optimally balances accuracy and complexity.</li>
<li class="fragment">This is the magic sauce of Bayes.</li>
</ul>
</section>
<section id="marginal-likelihood" class="slide level2">
<h2>Marginal likelihood</h2>
<ul>
<li class="fragment">Back to Bayes equation we started with</li>
<li class="fragment"><img data-src="fig.1.1.jpg"></li>
<li class="fragment"><img data-src="fig.1.2.jpg"></li>
<li class="fragment">There is something missing which is that this is all conditional on a particular model. We could have a very different model that might have different parameters.</li>
</ul>
</section>
<section id="marginal-likelihood-conditional-on-model" class="slide level2">
<h2>Marginal likelihood conditional on model</h2>
<ul>
<li class="fragment">We can be explicit about how this is conditional on a specific model</li>
<li class="fragment"><img data-src="fig.7.1.jpg"></li>
<li class="fragment"><span class="math inline">\(p(D|M_1)\)</span> is a single number, the marginal likelihood, also known as the evidence.</li>
</ul>
</section>
<section id="marginal-likelihood-in-words" class="slide level2">
<h2>Marginal likelihood in words</h2>
<ul>
<li class="fragment">the probability of the data given model</li>
<li class="fragment">the probability of the data according to the models predictions</li>
<li class="fragment">average predictive performance of the model</li>
<li class="fragment">how unsuprised was the model when seeing the data</li>
<li class="fragment">the prior in the model is like betting on where good parameters lie. The marginal likelihood sums up how well those bets paid off in terms of predictive performance when the data arrived.</li>
<li class="fragment">a models predictions has probability mass of 1. it can spend its predicitons in different ways by having different priors. a bad prior wastes this on predictions that are not supported by the data, a good prior concrentrates its predicitons to where the data is well predicted.</li>
<li class="fragment">the better the predictions of the model, the greater the evidence for the model</li>
<li class="fragment">it’s like running every possible version of the model (with every parameter setting weighted by the prior), and asking: “on average, how well does the model predict the data?”</li>
</ul>
</section>
<section id="marginal-likelihood-of-paranormal-octopi" class="slide level2">
<h2>Marginal likelihood of paranormal octopi</h2>
<ul>
<li class="fragment">Imagine two paranormal octupuses helping the KGB find a missing sub.</li>
<li class="fragment">Alice predicts the sub is in the nothern hemisphere</li>
<li class="fragment">Bob predicts its in nothern europe</li>
<li class="fragment">Data: the sub is found in the Baltic,</li>
<li class="fragment">the probability of the data according to Alice is reasonable. the evidence in favor of alice is reasonable</li>
<li class="fragment">the probability of the data according to Bob is high. The evidence in favor of Bob is high</li>
<li class="fragment">both were correct, but Bob was “more correct” because his prediciton was more specific</li>
</ul>
</section>
<section id="marginal-likelihood-1" class="slide level2">
<h2>Marginal likelihood</h2>
<ul>
<li class="fragment">The marginal likelihood is computed by averaging the likelihood of the data predicted across the models parameter space.<br>
</li>
<li class="fragment">Prior probabilities act as averaging weights.<br>
</li>
<li class="fragment">Based on the law of total probability:<br>
<span class="math inline">\(p(D \mid M_1) = \sum_{i=1}^{k} p(D \mid \xi_i, M_1) p(\xi_i \mid M_1)\)</span></li>
</ul>
</section>
<section id="example-calculation-of-marginal-likelihood" class="slide level2">
<h2>Example calculation of marginal likelihood</h2>
<ul>
<li class="fragment"><ul>
<li class="fragment">Consider a model <span class="math inline">\(M_x\)</span> with one parameter <span class="math inline">\(\xi\)</span>.<br>
</li>
</ul></li>
<li class="fragment"><span class="math inline">\(\xi\)</span> can take <strong>three values</strong>:
<ul>
<li class="fragment"><span class="math inline">\(\xi_1 = -1\)</span>, <span class="math inline">\(\xi_2 = 0\)</span>, <span class="math inline">\(\xi_3 = 1\)</span>.<br>
</li>
</ul></li>
<li class="fragment">Prior probabilities assigned:
<ul>
<li class="fragment"><span class="math inline">\(p(\xi_1) = 0.6\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(p(\xi_2) = 0.3\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(p(\xi_3) = 0.1\)</span></li>
</ul></li>
</ul>
</section>
<section id="likelihood-computation" class="slide level2">
<h2>Likelihood Computation</h2>
<ul>
<li class="fragment"><ul>
<li class="fragment">Likelihood values for observed data <span class="math inline">\(D\)</span>:<br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid \xi_1) = 0.001\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid \xi_2) = 0.002\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid \xi_3) = 0.003\)</span><br>
</li>
</ul></li>
<li class="fragment">Compute marginal likelihood:<br>
<span class="math inline">\(p(D \mid M_x) = p(\xi_1) p(D \mid \xi_1) + p(\xi_2) p(D \mid \xi_2) + p(\xi_3) p(D \mid \xi_3)\)</span> <span class="math inline">\(= 0.6 \times 0.001 + 0.3 \times 0.002 + 0.1 \times 0.003\)</span> <span class="math inline">\(= 0.0015\)</span></li>
</ul>
</section>
<section id="marginal-likelihood-and-complexity" class="slide level2">
<h2>Marginal likelihood and complexity</h2>
<ul>
<li class="fragment">To have high evidence, the model needs good predictive peformance</li>
<li class="fragment">It needs to focus its predictions where the data has a high likelihood</li>
<li class="fragment">Complex models with many parameters distribute their predictions widely</li>
<li class="fragment">Complex models thus make many predictions that may not turn out to have high likelihood</li>
<li class="fragment">Complex models thus are more likely to “waste their predictions”</li>
</ul>
</section>
<section id="model-complexity-is-not-the-same-as-the-number-of-parameters" class="slide level2">
<h2>Model complexity is not the same as the number of parameters</h2>
<ul>
<li class="fragment">Complexity is not just the number of parameters.</li>
<li class="fragment">A model with few parameters can still be complex if its predictions are highly uncertain due to vague priors.</li>
<li class="fragment">True complexity comes from how broadly the model spreads its predictions across its parameter space.</li>
<li class="fragment">A narrow prediction distribution suggests a simpler model.</li>
<li class="fragment">A wider distribution indicates greater complexity, as the model allows for more diverse possible outcomes.</li>
<li class="fragment">Models are complex to the degree to which they make broad range of predictions</li>
</ul>
</section>
<section id="model-complexity-and-vagueness-of-priors" class="slide level2">
<h2>Model complexity and vagueness of priors</h2>
<ul>
<li class="fragment">a model with prior <span class="math inline">\(\theta \sim \text{Uniform} (0,1)\)</span></li>
<li class="fragment">…is more complex than…</li>
<li class="fragment">a model with prior <span class="math inline">\(\theta \sim \text{Uniform} (0.5,1)\)</span></li>
</ul>
</section>
<section id="marginal-likelihood-as-a-unifying-maximandum" class="slide level2">
<h2>Marginal likelihood as a unifying maximandum</h2>
<ul>
<li class="fragment">Marginal likelihood is arguably the most important concept ever.</li>
<li class="fragment">It is at the heart of scientific inference, psychological and neural inference, and survival, and even existence of objects.</li>
<li class="fragment">Maximising it is hard, we often need to approximate it.</li>
<li class="fragment">Variational Bayes, Free-energy minimisation, Predictive coding, MCMC Sampling etc. are all ways to approximate it.</li>
<li class="fragment">Ultimately it is one quantity we care about.</li>
<li class="fragment">In our context we want to build models of brain, mind or behavior, that best explain these phenomena.</li>
<li class="fragment">This reduces to finding models that have the best average predictive performance.</li>
<li class="fragment">And this reduces to finding models with the highest marginal likelihood</li>
<li class="fragment">Thats it.</li>
<li class="fragment">Everything else is details.</li>
<li class="fragment">Ah but so and so maximises something different.</li>
<li class="fragment">Yes but at heart this is just a proxy for the marginal likelihood. <!-- #endregion                                   --></li>
</ul>
<!-- #region Lecture#7 Bayes factors              -->
</section></section>
<section>
<section id="the-bayes-factor" class="title-slide slide level1 center">
<h1>The Bayes factor</h1>
<ul>
<li class="fragment">Compared to what? This is a surprisingly powerful question to ask.</li>
<li class="fragment">This model is good - it has high model evidence.</li>
<li class="fragment">Ok great, but compared to what?</li>
<li class="fragment">We want relative evidence. We want to compare predictive performance for one model versus another.</li>
<li class="fragment">The Bayes factor gives us this by taking the ratio of the marginal likelihoods for two different models</li>
</ul>
</section>
<section id="bayes-factor-equation" class="slide level2">
<h2>Bayes factor equation</h2>
<ul>
<li class="fragment">Marginal likelihood measures a model’s absolute evidence by averaging its predictive performance over all parameter values.<br>
</li>
<li class="fragment">Model selection often focuses on relative evidence —how well one model explains the data compared to another.<br>
</li>
<li class="fragment">This is quantified using the Bayes factor:<br>
<span class="math inline">\(BF_{12} = \frac{p(D \mid M_1)}{p(D \mid M_2)}\)</span></li>
</ul>
</section>
<section id="interpretation-of-the-bayes-factor" class="slide level2">
<h2>Interpretation of the Bayes Factor</h2>
<ul>
<li class="fragment"><strong>BF &gt; 1</strong> → Data favors <strong>M₁</strong> over <strong>M₂</strong>.<br>
</li>
<li class="fragment"><strong>BF &lt; 1</strong> → Data favors <strong>M₂</strong> over <strong>M₁</strong>.<br>
</li>
<li class="fragment"><strong>Higher BF</strong> → Stronger evidence for the favored model.<br>
</li>
<li class="fragment">If <strong>BF = 5</strong>, the data is <strong>5 times more likely</strong> under <strong>M₁</strong> than <strong>M₂</strong>.<br>
</li>
<li class="fragment">If <strong>BF = 1/5</strong>, the data is <strong>5 times more likely</strong> under <strong>M₂</strong> than <strong>M₁</strong>.</li>
</ul>
</section>
<section id="jeffreys-scale-for-bayes-factor" class="slide level2">
<h2>Jeffreys’ Scale for Bayes Factor</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th><strong>Bayes Factor (BF₁₂)</strong></th>
<th><strong>Interpretation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>&gt;100</td>
<td>Extreme evidence for <strong>M₁</strong></td>
</tr>
<tr class="even">
<td>30–100</td>
<td>Very strong evidence for <strong>M₁</strong></td>
</tr>
<tr class="odd">
<td>10–30</td>
<td>Strong evidence for <strong>M₁</strong></td>
</tr>
<tr class="even">
<td>3–10</td>
<td>Moderate evidence for <strong>M₁</strong></td>
</tr>
<tr class="odd">
<td>1–3</td>
<td>Anecdotal evidence for <strong>M₁</strong></td>
</tr>
<tr class="even">
<td>1</td>
<td>No preference</td>
</tr>
<tr class="odd">
<td>1/3–1</td>
<td>Anecdotal evidence for <strong>M₂</strong></td>
</tr>
<tr class="even">
<td>1/10–1/3</td>
<td>Moderate evidence for <strong>M₂</strong></td>
</tr>
<tr class="odd">
<td>1/30–1/10</td>
<td>Strong evidence for <strong>M₂</strong></td>
</tr>
<tr class="even">
<td>1/100–1/30</td>
<td>Very strong evidence for <strong>M₂</strong></td>
</tr>
<tr class="odd">
<td>&lt;1/100</td>
<td>Extreme evidence for <strong>M₂</strong></td>
</tr>
</tbody>
</table>
</section>
<section id="example-binomial-model" class="slide level2">
<h2>Example: Binomial Model</h2>
<ul>
<li class="fragment">Suppose we have 9 correct (k) out of 10 trials (n) and compare two models:<br>
</li>
<li class="fragment">M₁: A non-guessing model with unknown success rate.<br>
<span class="math inline">\(p(\theta \mid M_1) \sim \text{Uniform}(0,1)\)</span><br>
</li>
<li class="fragment">M₂: A guessing model with chance at 0.5 <span class="math inline">\(p(\theta \mid M_2) = 0.5\)</span></li>
<li class="fragment">Compute <strong>marginal likelihoods</strong> for both models.</li>
</ul>
</section>
<section id="bayes-factor-for-the-example" class="slide level2">
<h2>Bayes Factor for the example</h2>
<ul>
<li class="fragment"><strong>Marginal likelihoods</strong>:<br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid M_1) = \left(\frac{10}{9}\right) \left(\frac{1}{2}\right)^{10}\)</span><br>
</li>
<li class="fragment"><span class="math inline">\(p(D \mid M_2) = \left(\frac{1}{1+n}\right)\)</span><br>
</li>
<li class="fragment"><strong>Bayes factor calculation</strong>:<br>
<span class="math inline">\(BF_{12} = \frac{p(D \mid M_1)}{p(D \mid M_2)}\)</span><br>
</li>
<li class="fragment">$ BF_{12} = 0.107</li>
<li class="fragment">If <span class="math inline">\(BF_{12} &gt; 1\)</span>, M₁ is preferred; if &lt; 1, M₂ is preferred.</li>
</ul>
</section>
<section id="flipping-the-bf" class="slide level2">
<h2>Flipping the BF</h2>
<ul>
<li class="fragment">m1 is 0.107 less likely than m2</li>
<li class="fragment">hard to interpret</li>
<li class="fragment">thus when a BF is below it is often more convenient to take the reciprocal so that it is a value above 1.</li>
<li class="fragment">here it would be <span class="math inline">\(BF_{21}\)</span> which is 1/1.07 = 9.3</li>
<li class="fragment">M2 is 9.3 times more likely than M1</li>
<li class="fragment">the data is 9.3 times more likely under a model in which subjects are not guessing, vs.&nbsp;than in which they are guessing</li>
</ul>
</section>
<section id="bayes-vs.-fisher" class="slide level2">
<h2>Bayes vs.&nbsp;Fisher</h2>
<ul>
<li class="fragment">Bayesian methods are typically comparative, where two ore more models are compared</li>
<li class="fragment">Frequentist methods are not comparative, simply considering how unlikely data was generated by the null.</li>
<li class="fragment">Evidence is always comparative. ratio of p(data|Innocent) : p(data|Guilty)</li>
</ul>
</section>
<section id="cautions-and-critiques" class="slide level2">
<h2>Cautions and Critiques</h2>
<ul>
<li class="fragment">Bayes factors sensitive to prior specification.</li>
<li class="fragment">Must use careful prior selection and sensitivity analyses.</li>
<li class="fragment">Point null hypotheses controversial: can they ever be exactly true?
<ul>
<li class="fragment">Pragmatically, yes (experimental studies).</li>
<li class="fragment">Conceptually, more debated. <!-- #endregion                             --></li>
</ul></li>
</ul>
<!-- #region Lecture#8 Posterior                  -->
</section></section>
<section>
<section id="posterior-model-probabilities" class="title-slide slide level1 center">
<h1>Posterior model probabilities</h1>
<ul>
<li class="fragment">Bayes factors compare the predictive performance of two different theories</li>
<li class="fragment">But model plausibility also depends on prior beliefs in each model.</li>
<li class="fragment">To assess relative plausibility after seeing the data we combine:</li>
<li class="fragment">Predictive performance (likelihood) and Prior plausibility (prior probabilities)</li>
</ul>
</section>
<section id="posterior-odds" class="slide level2">
<h2>Posterior odds</h2>
<ul>
<li class="fragment">Relaitve plausibility of models after seeing the data is indicated by the posterior odds</li>
<li class="fragment">The posterior odds of two models is: <span class="math inline">\(\frac{p(M_1 \mid D)}{p(M_2 \mid D)} = \frac{p(D \mid M_1)}{p(D \mid M_2)} \cdot \frac{p(M_1)}{p(M_2)}\)</span></li>
<li class="fragment">Or in words: <strong>posterior odds = Bayes factor × prior odds</strong></li>
</ul>
</section>
<section id="bayes-factor-transforms-prior-odds-into-posterior-odds" class="slide level2">
<h2>Bayes factor transforms prior odds into posterior odds</h2>
<ul>
<li class="fragment">The Bayes factor transforms <strong>prior odds</strong> into <strong>posterior odds</strong>: <span class="math inline">\(\text{prior odds} \rightarrow \text{posterior odds}\)</span></li>
</ul>
</section>
<section id="advantages-of-the-bayes-factors" class="slide level2">
<h2>Advantages of the Bayes factors</h2>
<ul>
<li class="fragment">Bayesian hypothesis tests (e.g., Bayes factors) naturally implement Ockham’s razor, penalising complex models when comparing them.<br>
</li>
<li class="fragment">They quantify relative support for multiple models.<br>
</li>
<li class="fragment">Allow model-averaged predictions over parameters.</li>
<li class="fragment">Bayes factors can provide evidence in favor of the null hypothesis (unlike frequentist)</li>
<li class="fragment">Bayes factors can be forever updated as data arrive (no fixed sample size or plan required)</li>
<li class="fragment">Easy to understand. How many more times likely the data is for one model than another. Thats it. What was the p-value again?</li>
</ul>
</section>
<section id="extraordinary-claims-require-extraordinary-evidence" class="slide level2">
<h2>Extraordinary Claims Require Extraordinary Evidence</h2>
<ul>
<li class="fragment">Echoing Hume, Laplace, and Sagan: extraordinary claims require extraordinary evidence.</li>
<li class="fragment">This principle is baked into Bayesian methods through <strong>posterior odds</strong>:<br>
<span class="math inline">\(\text{Posterior odds} = \text{Bayes factor} \times \text{prior odds}\)</span></li>
<li class="fragment">The prior plausibility of a model matters.</li>
<li class="fragment">Even strong evidence may not overturn implausible claims.</li>
<li class="fragment">Bayesian methods make explicit the influence of prior beliefs. Thats a good thing.</li>
</ul>
</section>
<section id="problems-with-p-values" class="slide level2">
<h2>Problems with p-values</h2>
<ul>
<li class="fragment">p-values often misinterpreted</li>
<li class="fragment">ask your supervisor to define p-values. Most can’t.</li>
<li class="fragment">p-values do not provide evidence for the null.</li>
<li class="fragment">depend on experimenter’s intentions, and what they would do, if the data had turned out differently, as well as alternative hypotheses.</li>
<li class="fragment">Classical hypothesis testing is asymmetric:</li>
<li class="fragment">Null can be rejected, but never confirmed.</li>
</ul>
</section>
<section id="optional-stopping" class="slide level2">
<h2>Optional Stopping</h2>
<ul>
<li class="fragment">Bayesian methods allow data collection to stop at any time based on evidence.</li>
<li class="fragment">p-value methods require pre-specified stopping rules.</li>
<li class="fragment">Researchers can continue or stop based on interim Bayes factor values.</li>
<li class="fragment">Provides more flexibility and transparency.</li>
</ul>
</section>
<section id="challenges-for-bayesian-approach" class="slide level2">
<h2>Challenges for Bayesian Approach</h2>
<ul>
<li class="fragment">Conceptual: Sensitivity to <strong>prior distributions</strong>.</li>
<li class="fragment">Computational: Difficulty in calculating <strong>marginal likelihoods</strong>.</li>
<li class="fragment">Using vague priors can lead to low-precision predictions.</li>
<li class="fragment">The Ockham’s razor property penalizes overly vague models.</li>
<li class="fragment">Priors must reflect meaningful knowledge — vague priors yield unhelpful results.</li>
<li class="fragment">Vague priors entail complex models</li>
</ul>
</section>
<section id="specifying-good-priors" class="slide level2">
<h2>Specifying Good Priors</h2>
<ul>
<li class="fragment">Researchers must encode prior knowledge into prior distributions.</li>
<li class="fragment">Options:
<ul>
<li class="fragment">Subjective specification: Based on domain expertise.</li>
<li class="fragment">Objective priors: E.g., unit-information priors, do not rely on specific prior knowledge.</li>
</ul></li>
<li class="fragment">Objective priors useful for:
<ul>
<li class="fragment">Wide applicability.</li>
<li class="fragment">Transparent baseline comparisons.</li>
<li class="fragment">Refinement with specific information when needed.</li>
</ul></li>
</ul>
</section>
<section id="confusion-about-priors" class="slide level2">
<h2>Confusion About Priors</h2>
<ul>
<li class="fragment">Common misconception: Bayes factor depends on the prior plausibility of the models.</li>
<li class="fragment">In truth:
<ul>
<li class="fragment">Bayes factor is unaffected by prior probabilities of the models.</li>
<li class="fragment">It does depend on the prior over model parameters (e.g., effect size).</li>
</ul></li>
<li class="fragment">Important distinction:
<ul>
<li class="fragment">Prior on models affects posterior probabilities but not bayes factor.</li>
<li class="fragment">Prior on parameters affects the Bayes factor itself and hence the posterior odds.</li>
</ul></li>
</ul>
</section>
<section id="prior-sensitivity" class="slide level2">
<h2>Prior Sensitivity</h2>
<ul>
<li class="fragment">When different parameter priors yield different conclusions, it shows scientific uncertainty.</li>
<li class="fragment">Strategies to handle prior sensitivity:
<ul>
<li class="fragment">Local/intrinsic/fractional/partial Bayes factors.</li>
<li class="fragment">Sensitivity analyses: Vary the priors on parameters and observe effects on conclusions.</li>
</ul></li>
<li class="fragment">Reminder: Some models are robust, others are fragile to prior assumptions.</li>
</ul>
</section>
<section id="computational-challenges" class="slide level2">
<h2>Computational Challenges</h2>
<ul>
<li class="fragment">Computing marginal likelihoods is hard for complex models.</li>
<li class="fragment">Approximate methods include:
<ul>
<li class="fragment">Candidate’s formula.</li>
<li class="fragment">Basic marginal likelihood identity: <span class="math inline">\(p(D \mid M_1) = \frac{p(D \mid \theta, M_1) p(\theta \mid M_1)}{p(\theta \mid D, M_1)}\)</span></li>
</ul></li>
<li class="fragment">MCMC-based methods:
<ul>
<li class="fragment">Sample from posterior, evaluate likelihoods.</li>
<li class="fragment">Use <strong>model indicator variables</strong> for model comparison. <!-- #endregion --></li>
</ul></li>
</ul>
<!-- #region Lecture#9 Savage Dickey -->
</section></section>
<section>
<section id="savage-dickkey-method-of-model-comparison" class="title-slide slide level1 center">
<h1>Savage-Dickkey method of model comparison</h1>
<ul>
<li class="fragment">In this method two models are compared:
<ul>
<li class="fragment"><strong>Null hypothesis (<span class="math inline">\(H_0\)</span>)</strong>: fixes parameter to a specific value, e.g., <span class="math inline">\(\phi = \phi_0\)</span></li>
<li class="fragment"><strong>Alternative hypothesis (<span class="math inline">\(H_1\)</span>)</strong>: parameter free to vary, e.g., <span class="math inline">\(\phi \ne \phi_0\)</span></li>
</ul></li>
<li class="fragment"><span class="math inline">\(H_0\)</span> is nested within <span class="math inline">\(H_1\)</span> (by constraining parameter).</li>
<li class="fragment">Classical null hypothesis usually sharp (point-null).</li>
</ul>
</section>
<section id="savagedickey-density-ratio" class="slide level2">
<h2>Savage–Dickey Density Ratio</h2>
<ul>
<li class="fragment">Defines Bayes factor for nested models: <span class="math inline">\(BF_{01} = \frac{p(D \mid H_0)}{p(D \mid H_1)} = \frac{p(\phi = \phi_0 \mid D, H_1)}{p(\phi = \phi_0 \mid H_1)}\)</span></li>
<li class="fragment">Simply the ratio of posterior to prior densities at the point of interest <span class="math inline">\(\phi_0\)</span> under the alternative hypothesis.</li>
</ul>
</section>
<section id="example-binomial-scenario" class="slide level2">
<h2>Example: Binomial Scenario</h2>
<ul>
<li class="fragment">Binomial scenario: <span class="math inline">\(\theta\)</span> parameter, observing 9 correct and 1 incorrect response.</li>
<li class="fragment">Null hypothesis (<span class="math inline">\(H_0\)</span>): <span class="math inline">\(\theta = 0.5\)</span></li>
<li class="fragment">Alternative hypothesis (<span class="math inline">\(H_1\)</span>): <span class="math inline">\(\theta\)</span> free to vary, prior <span class="math inline">\(\theta \sim Beta(1,1)\)</span></li>
<li class="fragment">Bayes factor is the ratio of posterior and prior densities at <span class="math inline">\(\theta=0.5\)</span></li>
</ul>
</section>
<section id="visual-interpretation-of-savage-dickey" class="slide level2">
<h2>Visual Interpretation of Savage-Dickey</h2>
<ul>
<li class="fragment"><img data-src="Fig7.1-example.jpg"></li>
<li class="fragment">Prior (uniform) and posterior distributions shown.</li>
<li class="fragment">Density ratio at <span class="math inline">\(\theta=0.5\)</span> gives Bayes factor.</li>
</ul>
</section>
<section id="mcmc-based-estimation-for-savage-dickey" class="slide level2">
<h2>MCMC-Based Estimation for Savage-Dickey</h2>
<ul>
<li class="fragment">When analytical solutions are difficult, use MCMC: -<img data-src="Fig7.2-MCMC-example.jpg"></li>
<li class="fragment">Posterior and prior estimated from MCMC samples.</li>
<li class="fragment">Heights of posterior and prior at the null point give Bayes factor.</li>
</ul>
</section>
<section id="advantages-of-savagedickey" class="slide level2">
<h2>Advantages of Savage–Dickey</h2>
<ul>
<li class="fragment">Direct interpretation as density ratio.</li>
<li class="fragment">Simplifies computation —no separate marginal likelihood calculation needed.</li>
<li class="fragment">Works well for nested models. <!-- #endregion    --></li>
</ul>
<!-- #region Lecture#10 Compare guassian means    -->
</section></section>
<section>
<section id="commpare-gaussian-means" class="title-slide slide level1 center">
<h1>Commpare Gaussian means</h1>
<ul>
<li class="fragment">Common task: test if two Gaussian means differ</li>
<li class="fragment">Example: does glucose improve detection performance</li>
<li class="fragment">Focus: test claim that glucose boost has larger effect in summer</li>
</ul>
</section>
<section id="data" class="slide level2">
<h2>Data</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Season</th>
<th>N</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Winter</td>
<td>41</td>
<td>0.11</td>
<td>0.15</td>
</tr>
<tr class="even">
<td>Summer</td>
<td>41</td>
<td>0.07</td>
<td>0.23</td>
</tr>
</tbody>
</table>
<ul>
<li class="fragment">Difference not significant<br>
</li>
<li class="fragment">t = 0.79, p = 0.44</li>
</ul>
</section>
<section id="p-values-and-the-null-hypothesis" class="slide level2">
<h2>p-values and the Null Hypothesis</h2>
<ul>
<li class="fragment">“From a null result, we cannot conclude that no difference exists…”</li>
<li class="fragment">p = 0.44 does not support H₀</li>
<li class="fragment">It just means data are not incompatible with H₀</li>
<li class="fragment">Need a Bayes factor to quantify support for H₀</li>
</ul>
</section>
<section id="bayes-factor-overview" class="slide level2">
<h2>Bayes Factor Overview</h2>
<ul>
<li class="fragment">Bayes factor compares posterior vs prior odds</li>
<li class="fragment">Quantifies evidence for or against H₀</li>
<li class="fragment">Unlike p-values, can support H₀</li>
</ul>
</section>
<section id="one-sample-comparison-model" class="slide level2">
<h2>One-Sample Comparison Model</h2>
<ul>
<li class="fragment">Test standardized difference scores (e.g., winter - summer)</li>
<li class="fragment">Assume:
<ul>
<li class="fragment">δ ~ Cauchy(0,1)</li>
<li class="fragment">xᵢ ~ Gaussian(μ, 1/σ²)</li>
<li class="fragment">μ = δσ</li>
</ul></li>
</ul>
</section>
<section id="one-sample-graphical-model" class="slide level2">
<h2>One-Sample Graphical Model</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/graphical_model_onesample.png"></p>
<figcaption>Fig 8.1</figcaption>
</figure>
</div></li>
<li class="fragment">Prior on δ: Cauchy(0,1)</li>
<li class="fragment">Prior on σ: Half-Cauchy</li>
<li class="fragment">Estimate posterior with MCMC</li>
</ul>
</section>
<section id="posterior-vs-prior" class="slide level2">
<h2>Posterior vs Prior</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/posterior_prior_onesample.png"></p>
<figcaption>Figure 8.2</figcaption>
</figure>
</div></li>
<li class="fragment">Posterior peaks near δ = 0</li>
<li class="fragment">Bayes Factor ≈ 5:1 in favor of H₀</li>
</ul>
</section>
<section id="order-restricted-model" class="slide level2">
<h2>Order-Restricted Model</h2>
<ul>
<li class="fragment">SMM predicts <strong>δ &lt; 0</strong></li>
<li class="fragment">Use order-restricted prior:
<ul>
<li class="fragment">δ ~ Cauchy(0,1) truncated to (-∞, 0)</li>
</ul></li>
</ul>
</section>
<section id="updated-bayes-factor" class="slide level2">
<h2>Updated Bayes Factor</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/posterior_prior_orderrestricted.png"></p>
<figcaption>Figure 8.4</figcaption>
</figure>
</div></li>
<li class="fragment">Stronger evidence for H₀: BF ≈ 10:1</li>
</ul>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">p-values can’t confirm H₀</li>
<li class="fragment">Bayes factors can</li>
<li class="fragment">“Evidence of absence” of support for SMM’s prediction</li>
</ul>
</section>
<section id="two-sample-comparison" class="slide level2">
<h2>Two-Sample Comparison</h2>
<ul>
<li class="fragment">Compare oxygenated vs plain water on memory</li>
<li class="fragment">Two independent groups</li>
</ul>
</section>
<section id="two-sample-model-structure" class="slide level2">
<h2>Two-Sample Model Structure</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/graphical_model_twosample.png"></p>
<figcaption>Figure 8.5</figcaption>
</figure>
</div></li>
<li class="fragment">Shared variance σ²</li>
<li class="fragment">δ = α / σ<br>
</li>
<li class="fragment">α = μₓ - μᵧ</li>
</ul>
</section>
<section id="large-effect-example" class="slide level2">
<h2>Large Effect Example</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Group</th>
<th>N</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Plain Water</td>
<td>20</td>
<td>68.35</td>
<td>6.38</td>
</tr>
<tr class="even">
<td>Oxygenated</td>
<td>20</td>
<td>76.65</td>
<td>4.06</td>
</tr>
<tr class="odd">
<td>t(38) = 4.47, p &lt; .01</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="two-sample-bayes-factor" class="slide level2">
<h2>Two-Sample Bayes Factor</h2>
<p>-<img data-src="figures/posterior_prior_twosample.png" alt="Figure 8.6"> - Posterior moves away from 0 - BF ≈ 447:1 in favor of H₁<br>
- Decisive evidence for oxygenated water effect <!-- #endregion   --></p>
<!-- #region Lecture#11 Compare binomial rates    -->
</section></section>
<section>
<section id="comparing-binomial-rates" class="title-slide slide level1 center">
<h1>Comparing binomial rates</h1>
<ul>
<li class="fragment">We will naturally compute binomial rates for different groups or conditions</li>
<li class="fragment">And ask which is larger?</li>
<li class="fragment">We thus need to compare binomial rates and test hypotheses about which is bigger etc.</li>
</ul>
</section>
<section id="bayesian-graphical-model" class="slide level2">
<h2>Bayesian graphical model</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/Fig9.1.png"></p>
<figcaption>Figure 9.1</figcaption>
</figure>
</div></li>
<li class="fragment">graphical model for comparing two proportions</li>
</ul>
</section>
<section id="bayesian-model" class="slide level2">
<h2>Bayesian model</h2>
<ul>
<li class="fragment">We model the observed counts using binomial likelihoods and assign uniform Beta priors: s1 ~ Binomial(theta1, n1)<br>
s2 ~ Binomial(theta2, n2)<br>
theta1 ~ Beta(1, 1)<br>
theta2 ~ Beta(1, 1)<br>
delta &lt;- theta1 - theta2</li>
<li class="fragment">theta1:</li>
<li class="fragment">theta2:</li>
<li class="fragment">delta = theta1 - theta2: difference in proportions</li>
<li class="fragment">We are interested in the posterior distribution of delta.</li>
</ul>
</section>
<section id="model-code" class="slide level2">
<h2>Model Code</h2>
<p>Here is the model used for posterior simulation: model { theta1 ~ dbeta(1,1) theta2 ~ dbeta(1,1) delta &lt;- theta1 - theta2 s1 ~ dbin(theta1, n1) s2 ~ dbin(theta2, n2) theta1prior ~ dbeta(1,1) theta2prior ~ dbeta(1,1) deltaprior &lt;- theta1prior - theta2prior } This allows us to compare the prior and posterior density of delta at zero.</p>
</section>
<section id="prior-and-posterior-distributions" class="slide level2">
<h2>Prior and Posterior Distributions</h2>
<ul>
<li class="fragment"><div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figures/Fig9.2.png"></p>
<figcaption>Figure 9.2</figcaption>
</figure>
</div></li>
<li class="fragment">We estimate the posterior distribution for the rate difference delta = theta1 - theta2 using Bayesian inference.</li>
<li class="fragment">The left plot shows prior and posterior distributions for delta across its full range.</li>
<li class="fragment">The right plot zooms in near delta = 0.</li>
<li class="fragment">This is used in the Savage–Dickey density ratio to compute the Bayes factor.</li>
<li class="fragment">The Savage–Dickey method compares: BF_01 = prior density at delta = 0 / posterior density at delta = 0</li>
</ul>
</section>
<section id="interpreting-the-bayes-factor" class="slide level2">
<h2>Interpreting the Bayes Factor</h2>
<ul>
<li class="fragment">The posterior density at delta = 0 is about half the prior density.</li>
<li class="fragment">This gives a Bayes factor ≈ 2 in favor of the alternative hypothesis H1: delta ≠ 0.</li>
<li class="fragment">The 95% credible interval for delta is approximately [-0.09, 0.01], which does not include 0.</li>
</ul>
<p><strong>Interpretation</strong>: - There is only modest evidence that one rate is higher than another - The Bayes factor penalizes H1 for spreading prior mass over implausible values. <!-- #endregion    --></p>
<!-- #region Lecture#12 Memory retention-->
<!-- #endregion    -->
<!-- #region Lecture#13 Psychophysical function fitting-->
<!-- #endregion    -->

</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://tinyurl.com/bayeBinderRepo" target="_blank"> tinyurl.com/bayesBinderRepo </a>&nbsp;&nbsp;&nbsp; <a href="https://tinyurl.com/bayesGithub" target="_blank"> tinyurl.com/bayesGithub </a>&nbsp;&nbsp; <a href="https://tinyurl.com/bayesSchedule" target="_blank"> tinyurl.com/bayesSchedule </a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="BayesianModels_slides_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="BayesianModels_slides_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="BayesianModels_slides_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>